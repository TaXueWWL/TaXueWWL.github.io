<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>朝·闻·道</title>
  <subtitle>SnoWalker&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wuwenliang.net/"/>
  <updated>2020-02-28T14:43:36.085Z</updated>
  <id>http://wuwenliang.net/</id>
  
  <author>
    <name>SnoWalker</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>跟我学Spring之自定义bean容器提升代码可读性</title>
    <link href="http://wuwenliang.net/2020/02/28/%E8%B7%9F%E6%88%91%E5%AD%A6Spring%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89bean%E5%AE%B9%E5%99%A8%E6%8F%90%E5%8D%87%E4%BB%A3%E7%A0%81%E5%8F%AF%E8%AF%BB%E6%80%A7/"/>
    <id>http://wuwenliang.net/2020/02/28/跟我学Spring之自定义bean容器提升代码可读性/</id>
    <published>2020-02-28T11:41:21.000Z</published>
    <updated>2020-02-28T14:43:36.085Z</updated>
    
    <content type="html"><![CDATA[<p>开发中经常有这样的场景：</p>
<p><strong>根据某个类型标识走不同的业务逻辑</strong>，通常我们会使用if(type.equals(xxxxx)) 或者 switch语句来进行逻辑处理。</p>
<p>这样做当然是没什么问题的。</p>
<p>当业务逻辑变得越来越复杂，类型标识增多之后，难免会出现if判断增加，或者switch case分支变多，这样的代码往往会过于冗长，代码重复性较大，或者说逼格不够高。</p>
<p>本文介绍一种基于自定义Bean容器的开发方式，消除代码中的判断分支，提升代码可读性。</p>
<a id="more"></a>
<p>我们通过一个demo来看如何实现这种编码方式。</p>
<h2 id="定义接口"><a href="#定义接口" class="headerlink" title="定义接口"></a>定义接口</h2><p>首先定义一个接口，主要有两个方法：</p>
<pre><code>public interface AbstractService&lt;T&gt; {

    /**
    * 返回serviceName
    * 作为bean选择标识
    * @return
    */
    String serviceName();

    /**
    * 具体的service方法
    * @param parm
    * @return
    */
    T execute(Object parm);
}
</code></pre><p>实现类需要实现serviceName，返回具体的类型，注意不同的bean实现类该返回值不能重复</p>
<p>execute方法为业务方法，这里只是做个示范，实际开发中可以是任意的通用业务方法。</p>
<h2 id="实现接口"><a href="#实现接口" class="headerlink" title="实现接口"></a>实现接口</h2><p>接着编写实现类，实现接口</p>
<blockquote>
<p>ServiceAImpl标记类型为 ServiceA</p>
</blockquote>
<pre><code>@Component
public class ServiceAImpl implements AbstractService&lt;DemoA&gt; {

    @Override
    public String serviceName() {
        return &quot;ServiceA&quot;;
    }

    @Override
    public DemoA execute(Object parm) {
        System.out.println(&quot;ServiceAImpl execute&quot;);
        return new DemoA().setName(&quot;DemoA&quot;);
    }
}
</code></pre><blockquote>
<p>ServiceBImpl标记类型为 ServiceB</p>
</blockquote>
<pre><code>@Component
public class ServiceBImpl implements AbstractService&lt;DemoB&gt; {

    @Override
    public String serviceName() {
        return &quot;ServiceB&quot;;
    }

    @Override
    public DemoB execute(Object parm) {
        System.out.println(&quot;ServiceBImpl execute&quot;);
        return new DemoB().setName(&quot;DemoB&quot;);
    }
}
</code></pre><h2 id="编写自定义Bean上下文"><a href="#编写自定义Bean上下文" class="headerlink" title="编写自定义Bean上下文"></a>编写自定义Bean上下文</h2><p>这里是重头戏，我们需要编写一个Bean上下文，并注入AbstractService集合。</p>
<pre><code>@Component
public class ServiceContext {

    // IService容器，key=serviceName，velue=实例
    private static Map&lt;String, AbstractService&gt; SERVICE_CONTEXT;

    @Autowired
    List&lt;AbstractService&gt; services;

    @PostConstruct
    void init() {
        SERVICE_CONTEXT = new ConcurrentHashMap&lt;&gt; ();
        if (services == null) {
            return;
        }
        // 将IService所有的实现类注册到serviceContext
        for(AbstractService service : services) {
            SERVICE_CONTEXT.put(service.serviceName(), service);
        }
        System.out.println(JSON.toJSONString(SERVICE_CONTEXT));
    }

    /**
    * 根据serviceName获取实例
    * @param serviceName
    * @return
    */
    public AbstractService getServiceImpl(String serviceName) {
        return SERVICE_CONTEXT.get(serviceName);
    }
}
</code></pre><p>其实注释已经很清楚了，首先定义一个Map，key为String，代表我们上文中接口返回的serviceName。</p>
<p>value为接口实现类bean实例。</p>
<p>接着通过@Autowired注入AbstractService集合，这里是一个List。当Spring容器初始化完成，会将AbstractService的实现类都加载到List中。</p>
<p>在@PostConstruct标记的初始化方法中，遍历 <strong>List＜AbstractService＞</strong>，并依次加载到我们初始化好的Map中。key=AbstractService.serviceName()的返回值，value为AbstractService实例。</p>
<p>定义一个getServiceImpl(String serviceName)提供给业务使用，能够让我们通过具体的serviceName标识获取到Bean实例。这也是为何serviceName不能重复的原因。</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>到此主要的逻辑编写就完成了，我们编写一个测试类测试一下具体如何使用。</p>
<pre><code>public static void main(String[] args) {
    ConfigurableApplicationContext applicationContext = SpringApplication.run(DemoApplication.class, args);
    // 获取bean Context
    ServiceContext serviceContext = applicationContext.getBean(&quot;serviceContext&quot;, ServiceContext.class);
    // 根据serviceName获取具体的接口实现类
    AbstractService serviceA = serviceContext.getServiceImpl(&quot;ServiceA&quot;);
    AbstractService serviceB = serviceContext.getServiceImpl(&quot;ServiceB&quot;);
    // 调用service方法
    serviceA.execute(null);
    serviceB.execute(null);
}
</code></pre><p>这里从Spring上下文中获取到ServiceContext，并通过具体的serviceName获取到对应的Bean实例，并调用实例的execute方法。执行结果如下：</p>
<pre><code>ServiceAImpl execute
ServiceBImpl execute
</code></pre><p>可能这还不算很直观，我们模拟一个业务场景。</p>
<p>业务需要先判断serviceName，再根据具体的值选择不同的执行逻辑。</p>
<p>正常情况下，我们会这样编写业务代码：</p>
<pre><code>if (&quot;ServiceA&quot;.equals(serviceName)) {
    serviceA.execute()
    return;
}

if (&quot;ServiceB&quot;.equals(serviceName)) {
    serviceB.execute()
    return;
}

...
</code></pre><p>如果有一百个serviceName，那么这里就要有100个if分支，switch也同理。</p>
<p>但是采取本文中的编码方式则只需要这么写：</p>
<pre><code>...省略获取serviceContext过程，最简单的方法是通过@Autowired/@Resource注入...
AbstractService service = serviceContext.getServiceImpl(serviceName);
service.execute()
</code></pre><p>这样我们就只需要在新增serviceName类型后，开发一个对应的实现类即可。</p>
<p>如果是传统的编码方式，则除了新增service实现，还需要修改if/switch判断逻辑，不够灵活且容易出错。</p>
<p>这里其实就是开放封闭原则的体现。传统的方式对修改和扩展都是开放的，而这种方式则是对扩展开发，对修改封闭的。尤其适用于复杂业务场景的开发。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>简单讲一下原理。</p>
<p>Spring框架支持对集合类型进行依赖注入，对于集合类型依赖注入与查找起作用的ApplicationContext实现类为 <strong>ListableBeanFactory</strong>。</p>
<p>我们看下源码是如何实现该特性的：</p>
<p>具体的逻辑在 <strong>org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency</strong> 这个方法中</p>
<p>打开该方法，重点关注下面这行</p>
<pre><code>Object arg = beanFactory.resolveDependency(currDesc, beanName, autowiredBeans, typeConverter);
</code></pre><p>进入resolveDependency方法，看到下面这一行，跳入doResolveDependency方法</p>
<pre><code>result = doResolveDependency(descriptor, requestingBeanName, 
autowiredBeanNames, typeConverter);
</code></pre><p>重点关注下面的逻辑</p>
<pre><code>Object multipleBeans = resolveMultipleBeans(descriptor, beanName, autowiredBeanNames, typeConverter);
if (multipleBeans != null) {
    return multipleBeans;
}
</code></pre><p>此处的resolveMultipleBeans方法逻辑为，如果解析到了多个匹配条件的Bean，就直接返回解析结果。</p>
<p>那具体的解析结果又是什么呢？我们进入resolveMultipleBeans方法</p>
<pre><code>private Object resolveMultipleBeans(DependencyDescriptor descriptor, @Nullable String beanName,
            @Nullable Set&lt;String&gt; autowiredBeanNames, @Nullable TypeConverter typeConverter) {

    Class&lt;?&gt; type = descriptor.getDependencyType();
    // 数组类型
    if (type.isArray()) {
        Class&lt;?&gt; componentType = type.getComponentType();
        ResolvableType resolvableType = descriptor.getResolvableType();
        Class&lt;?&gt; resolvedArrayType = resolvableType.resolve();
        if (resolvedArrayType != null &amp;&amp; resolvedArrayType != type) {
            type = resolvedArrayType;
            componentType = resolvableType.getComponentType().resolve();
        }
        if (componentType == null) {
            return null;
        }
        Map&lt;String, Object&gt; matchingBeans = findAutowireCandidates(beanName, componentType,
                new MultiElementDescriptor(descriptor));
        if (matchingBeans.isEmpty()) {
            return null;
        }
        if (autowiredBeanNames != null) {
            autowiredBeanNames.addAll(matchingBeans.keySet());
        }
        TypeConverter converter = (typeConverter != null ? typeConverter : getTypeConverter());
        Object result = converter.convertIfNecessary(matchingBeans.values(), type);
        if (getDependencyComparator() != null &amp;&amp; result instanceof Object[]) {
            Arrays.sort((Object[]) result, adaptDependencyComparator(matchingBeans));
        }
        return result;
    }
    // 集合类型，如List set
    else if (Collection.class.isAssignableFrom(type) &amp;&amp; type.isInterface()) {
        Class&lt;?&gt; elementType = descriptor.getResolvableType().asCollection().resolveGeneric();
        if (elementType == null) {
            return null;
        }
        Map&lt;String, Object&gt; matchingBeans = findAutowireCandidates(beanName, elementType,
                new MultiElementDescriptor(descriptor));
        if (matchingBeans.isEmpty()) {
            return null;
        }
        if (autowiredBeanNames != null) {
            autowiredBeanNames.addAll(matchingBeans.keySet());
        }
        TypeConverter converter = (typeConverter != null ? typeConverter : getTypeConverter());
        Object result = converter.convertIfNecessary(matchingBeans.values(), type);
        if (getDependencyComparator() != null &amp;&amp; result instanceof List) {
            ((List&lt;?&gt;) result).sort(adaptDependencyComparator(matchingBeans));
        }
        return result;
    }
    // Map类型
    else if (Map.class == type) {
        ResolvableType mapType = descriptor.getResolvableType().asMap();
        Class&lt;?&gt; keyType = mapType.resolveGeneric(0);
        if (String.class != keyType) {
            return null;
        }
        Class&lt;?&gt; valueType = mapType.resolveGeneric(1);
        if (valueType == null) {
            return null;
        }
        Map&lt;String, Object&gt; matchingBeans = findAutowireCandidates(beanName, valueType,
                new MultiElementDescriptor(descriptor));
        if (matchingBeans.isEmpty()) {
            return null;
        }
        if (autowiredBeanNames != null) {
            autowiredBeanNames.addAll(matchingBeans.keySet());
        }
        return matchingBeans;
    }
    else {
        return null;
    }
}
</code></pre><p>这里便是@Autowired注入集合类型的核心。</p>
<ul>
<li>首先判断注入类型，如果是数组、Collection、Map等类型，则注入元素数据，即查找与元素类型相同的Bean，并注入到集合中。</li>
<li><p>这里重点强调下Map类型，我们能够看出，Map的 key 为Bean的 name，value 为 与定义的元素类型相同的Bean。</p>
<pre><code>// Map的key
Class&lt;?&gt; keyType = mapType.resolveGeneric(0);
if (String.class != keyType) {
    return null;
}
// Map的value
Class&lt;?&gt; valueType = mapType.resolveGeneric(1);
if (valueType == null) {
    return null;
}
</code></pre></li>
</ul>
<p>也就是说，如果业务上不依赖外部的type，那么我们可以直接注入一个Map集合，比如：</p>
<pre><code>@Autowired
private Map&lt;String, BeanInterface&gt; map;
</code></pre><p>这样就能够将接口BeanInterface的实现都注入到Map中，key的值为具体Bean的name，value为Bean实例。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文中，我们通过案例与源码，全方位呈现了Spring对集合类型的注入方式。总结一下：</p>
<ol>
<li><p><strong>Spring在注入集合类的同时，会将集合泛型类的实例填入集合中，作为集合的初始值。</strong></p>
</li>
<li><p>对于list、set填入的是注入类型Spring管理的实例，对于map，Spring会将service的名字作为key，对象作为value封装进入Map。</p>
</li>
<li><p>对于List类型，可以通过@Order指定加入List的顺序。只需要在实现类中加入@Order(value) 注解即可 ，值越小越先被初始化越先被放入List</p>
</li>
</ol>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开发中经常有这样的场景：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;根据某个类型标识走不同的业务逻辑&lt;/strong&gt;，通常我们会使用if(type.equals(xxxxx)) 或者 switch语句来进行逻辑处理。&lt;/p&gt;
&lt;p&gt;这样做当然是没什么问题的。&lt;/p&gt;
&lt;p&gt;当业务逻辑变得越来越复杂，类型标识增多之后，难免会出现if判断增加，或者switch case分支变多，这样的代码往往会过于冗长，代码重复性较大，或者说逼格不够高。&lt;/p&gt;
&lt;p&gt;本文介绍一种基于自定义Bean容器的开发方式，消除代码中的判断分支，提升代码可读性。&lt;/p&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>跟我学RocketMQ之拉模式消费的两种方式</title>
    <link href="http://wuwenliang.net/2020/02/20/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ%E4%B9%8B%E6%8B%89%E6%A8%A1%E5%BC%8F%E6%B6%88%E8%B4%B9%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F/"/>
    <id>http://wuwenliang.net/2020/02/20/跟我学RocketMQ之拉模式消费的两种方式/</id>
    <published>2020-02-20T09:02:18.000Z</published>
    <updated>2020-02-20T10:19:04.076Z</updated>
    
    <content type="html"><![CDATA[<p>今天我们聊聊RocketMQ基于拉模式的两种消费方式。</p>
<p>对于消费而言，RocketMQ提供了推拉两种方式，我们常用的是基于长轮询的DefaultPushConsumer，它具有实时性好，易开发等特点。</p>
<p>但同时由于是长轮询，因此在大量消息消费的场景下，可能导致broker端CPU负载较大等情况，因此我们会在这种情况下选择使用拉模式的</p>
<p>PullConsumer或者MQPullConsumerScheduleService+PullTaskCallback这两种方式进行更为灵活的消费。</p>
<a id="more"></a>
<h2 id="PullConsumer消费示例代码"><a href="#PullConsumer消费示例代码" class="headerlink" title="PullConsumer消费示例代码"></a>PullConsumer消费示例代码</h2><p>首先我们看下基于PullConsumer方式如何进行消费。这里引用官方的样例代码进行说明：</p>
<pre><code>public class PullConsumer {
</code></pre><p>我们定义了一个Map，key为指定的队列，value为这个队列拉取数据的最后位置，保存每个对列的拉取进度（offset），这里只是用作示例。实际开发中，这里需要基于持久化到Redis或者MySQL等外部存储设施中。</p>
<pre><code>//Map&lt;key, value&gt;  key为指定的队列，value为这个队列拉取数据的最后位置
private static final Map&lt;MessageQueue, Long&gt; offseTable = new HashMap&lt;MessageQueue, Long&gt;();

public static void main(String[] args) throws MQClientException {
</code></pre><p>首先需要定义消费者组，实例化一个DefaultMQPullConsumer消费者对象，标记消费者组。</p>
<p>为消费者设置NameServer地址，保证消费者客户端能够从NameServer获取到broker地址，从而执行消息消费流程。</p>
<pre><code>String group_name = &quot;test_pull_consumer_name&quot;;
DefaultMQPullConsumer consumer = new DefaultMQPullConsumer(group_name);
consumer.setNamesrvAddr(Const.NAMESRV_ADDR_MASTER_SLAVE);
consumer.start();
System.err.println(&quot;consumer start&quot;);
</code></pre><p>通过consumer.fetchSubscribeMessageQueues(TOPIC)方法获取指定TOPIC下的所有队列，默认有4个。</p>
<pre><code>//    从TopicTest这个主题去获取所有的队列（默认会有4个队列）
Set&lt;MessageQueue&gt; mqs = consumer.fetchSubscribeMessageQueues(&quot;test_pull_topic&quot;);
</code></pre><p>对获取到MessageQueue集合进行遍历，拉取数据并执行具体的消费过程。</p>
<pre><code>//    遍历每一个队列，进行拉取数据
for (MessageQueue mq : mqs) {
    System.out.println(&quot;Consume from the queue: &quot; + mq);
</code></pre><p>通过while(true) 不间断地从队列中拉取数据。默认情况下，每次拉取32条，这里需要显式地传入拉取开始offset，通过getMessageQueueOffset(mq)方法获取，从我们持久化的设施中得到对应MessageQueue的拉取进度(可以认为是消费进度)。</p>
<p>拉取结束后，在持久化设施(这里是一个Map)保存下次拉取的开始offset，也就是本次拉取结束的下一个offset。（通过pullResult.getNextBeginOffset()获取）</p>
<pre><code>while (true) {
    try {
        //    从queue中获取数据，从什么位置开始拉取数据 单次对多拉取32条记录
        PullResult pullResult = consumer.pullBlockIfNotFound(mq, null, getMessageQueueOffset(mq), 32);
        System.out.println(pullResult);
        System.out.println(pullResult.getPullStatus());
        System.out.println();
        putMessageQueueOffset(mq, pullResult.getNextBeginOffset());
</code></pre><p>从pullResult拉取结果中获取拉取状态，如果是FOUND则表明消息拉取成功；获取消息列表，并循环进行消费。其余均认为未拉取到消息，不做处理。</p>
<pre><code>                switch (pullResult.getPullStatus()) {
                    case FOUND:
                        List&lt;MessageExt&gt; list = pullResult.getMsgFoundList();
                        for(MessageExt msg : list){
                            System.out.println(new String(msg.getBody()));
                        }
                        break;
                    case NO_MATCHED_MSG:
                        break;
                    case NO_NEW_MSG:
                        System.out.println(&quot;没有新的数据啦...&quot;);
                        break SINGLE_MQ;
                    case OFFSET_ILLEGAL:
                        break;
                    default:
                        break;
                }
            }
            catch (Exception e) {
                e.printStackTrace();
            }
        }
    }
    consumer.shutdown();
}
</code></pre><p> 拉取结束之后，手动显式调用该方法，刷新对应队列MessageQueue的拉取进度；</p>
<pre><code>private static void putMessageQueueOffset(MessageQueue mq, long offset) {
    offseTable.put(mq, offset);
}
</code></pre><p>获取对应MessageQueue的消息消费进度Offset</p>
<pre><code>    private static long getMessageQueueOffset(MessageQueue mq) {
        Long offset = offseTable.get(mq);
        if (offset != null)
            return offset;
        return 0;
    }

}
</code></pre><h3 id="小结PullConsumer消费方式"><a href="#小结PullConsumer消费方式" class="headerlink" title="小结PullConsumer消费方式"></a>小结PullConsumer消费方式</h3><p>从上述代码样例中可以看出，PullConsumer方式需要我们显式地存储消费进度，并且在消费过程中要根据情况进行消费进度的更新与存储。</p>
<p>如果开发者稍有不慎，忘记保存offset，则每次都会从第一条进行拉取，这样很容易造成消息重复。如果是生产环境，则后果不忍想象。</p>
<p>另外，我们还需要通过额外的存储手段对offset进行保存，并且尽量保证该设施的稳定可靠，否则还是会引起重复消费的问题。</p>
<p>基于此，我建议使用MQPullConsumerScheduleService+PullTaskCallback这种消费方式，那它具体如何使用呢？</p>
<h2 id="MQPullConsumerScheduleService-PullTaskCallback消费方式"><a href="#MQPullConsumerScheduleService-PullTaskCallback消费方式" class="headerlink" title="MQPullConsumerScheduleService+PullTaskCallback消费方式"></a>MQPullConsumerScheduleService+PullTaskCallback消费方式</h2><p>基于上述分析的PullConsumer使用一些不便之处，我这里建议使用MQPullConsumerScheduleService+PullTaskCallback方式进行消费。我们还是按照习惯方式，直接上代码。</p>
<p> <strong>step1:</strong> 声明并实例化一个MQPullConsumerScheduleService对象，通过构造方法传递消费者组；           </p>
<pre><code>String group_name = &quot;test_pull_consumer_name&quot;;

final MQPullConsumerScheduleService scheduleService = new MQPullConsumerScheduleService(group_name);
</code></pre><p>为消费者设置NameServer地址，以便能够获取broker地址，开启消费过程。</p>
<pre><code>scheduleService.getDefaultMQPullConsumer().setNamesrvAddr(Const.NAMESRV_ADDR_MASTER_SLAVE);
</code></pre><p>设置消费方式为集群模式；</p>
<pre><code>scheduleService.setMessageModel(MessageModel.CLUSTERING);
</code></pre><p><strong>step2：</strong> 调用registerPullTaskCallback(topic, pullTaskCallback) ,将开发者实现的PullTaskCallback消息拉取实现类注册给MQPullConsumerScheduleService。并绑定到指定的topic下；</p>
<pre><code>scheduleService.registerPullTaskCallback(&quot;test_pull_topic&quot;, new PullTaskCallback() {
</code></pre><p> <strong>step3:</strong> 开发者需要实现PullTaskCallback的doPullTask消息拉取回调方法，这里使用匿名内部类的方式。如果是Spring项目，我们可以定义一个Bean实现PullTaskCallback接口，并将该Bean的引用设置到一个实例化好的MQPullConsumerScheduleService对象中。</p>
<pre><code>@Override
public void doPullTask(MessageQueue mq, PullTaskContext context) {
</code></pre><p>通过 PullTaskContext上下文获取到消息拉取实例对象MQPullConsumer；</p>
<pre><code>MQPullConsumer consumer = context.getPullConsumer();
System.err.println(&quot;-------------- queueId: &quot; + mq.getQueueId()  + &quot;-------------&quot;);
try {
</code></pre><p>获取当前的消费进度，即：从哪儿开始消费，如果offset小于0则指定从0开始。</p>
<pre><code>// 获取从哪里拉取
long offset = consumer.fetchConsumeOffset(mq, false);
if (offset &lt; 0)
    offset = 0;
</code></pre><p>从对应的offset拉取指定数量的消息，默认32条，返回结果为PullResult。</p>
<p>通过pullResult.getPullStatus()判断拉取结果，如果为FOUND，则开始消费流程；其他状态不做处理。</p>
<pre><code>PullResult pullResult = consumer.pull(mq, &quot;*&quot;, offset, 32);
switch (pullResult.getPullStatus()) {
    case FOUND:
        List&lt;MessageExt&gt; list = pullResult.getMsgFoundList();
        for(MessageExt msg : list){
            //消费数据...
            ystem.out.println(new String(msg.getBody()));
        }
        break;
    case NO_MATCHED_MSG:
        break;
    case NO_NEW_MSG:
    case OFFSET_ILLEGAL:
        break;
    default:
        break;
}
</code></pre><p><strong>step4：</strong> 重点来了，这里通过调用updateConsumeOffset，更新消费进度，将下次消费开始时的offset更新到broker。并不需要客户端本地保存消费进度。</p>
<p>设置下次拉取时间，定时进行拉取调度。</p>
<pre><code>    consumer.updateConsumeOffset(mq, pullResult.getNextBeginOffset());
    // 设置再过3000ms后重新拉取
    context.setPullNextDelayTimeMillis(3000);

}
...省略catch块...
</code></pre><p>启动拉取过程。 </p>
<pre><code>        scheduleService.start();
    }
}
</code></pre><h3 id="小结MQPullConsumerScheduleService消费方式"><a href="#小结MQPullConsumerScheduleService消费方式" class="headerlink" title="小结MQPullConsumerScheduleService消费方式"></a>小结MQPullConsumerScheduleService消费方式</h3><p>从代码中我们能够清晰的看出，MQPullConsumerScheduleService的优势：</p>
<ol>
<li>MQPullConsumerScheduleService基于定时任务，消费端能够灵活控制拉取频率</li>
<li>MQPullConsumerScheduleService支持提交消费进度到broker，不需要消费端进行保存</li>
<li>MQPullConsumerScheduleService本身基于PullConsumer，定制化程度高，使用起来不易出错</li>
</ol>
<p>可以说，MQPullConsumerScheduleService既保留了PullConsumer的优势，还对其进行了一定程序的增强；通过直接提交消费offset到broker，降低了客户端的开发量，较少了消费重复的风险。</p>
<p>因此笔者提倡在实际开发中，使用MQPullConsumerScheduleService进行拉模式的消息消费。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>这里做个小预告，在后续的文章中，笔者将对MQPullConsumerScheduleService消费方式源码实现进行解析，请拭目以待。</p>
<p>为了方便读者和笔者后续开发，这里贴出两种方式的完整源码实现，以略去重读文章的繁琐：</p>
<h3 id="PullConsumer样例"><a href="#PullConsumer样例" class="headerlink" title="PullConsumer样例"></a>PullConsumer样例</h3><pre><code>public class PullConsumer {
    //Map&lt;key, value&gt;  key为指定的队列，value为这个队列拉取数据的最后位置
    private static final Map&lt;MessageQueue, Long&gt; offseTable = new HashMap&lt;MessageQueue, Long&gt;();

    public static void main(String[] args) throws MQClientException {

        String group_name = &quot;test_pull_consumer_name&quot;;
        DefaultMQPullConsumer consumer = new DefaultMQPullConsumer(group_name);
        consumer.setNamesrvAddr(Const.NAMESRV_ADDR_MASTER_SLAVE);
        consumer.start();
        System.err.println(&quot;consumer start&quot;);
        //    从TopicTest这个主题去获取所有的队列（默认会有4个队列）
        Set&lt;MessageQueue&gt; mqs = consumer.fetchSubscribeMessageQueues(&quot;test_pull_topic&quot;);
        //    遍历每一个队列，进行拉取数据
        for (MessageQueue mq : mqs) {
            System.out.println(&quot;Consume from the queue: &quot; + mq);

            SINGLE_MQ: while (true) {
                try {
                    //    从queue中获取数据，从什么位置开始拉取数据 单次对多拉取32条记录
                    PullResult pullResult = consumer.pullBlockIfNotFound(mq, null, getMessageQueueOffset(mq), 32);
                    System.out.println(pullResult);
                    System.out.println(pullResult.getPullStatus());
                    System.out.println();
                    putMessageQueueOffset(mq, pullResult.getNextBeginOffset());
                    switch (pullResult.getPullStatus()) {
                        case FOUND:
                            List&lt;MessageExt&gt; list = pullResult.getMsgFoundList();
                            for(MessageExt msg : list){
                                System.out.println(new String(msg.getBody()));
                            }
                            break;
                        case NO_MATCHED_MSG:
                            break;
                        case NO_NEW_MSG:
                            System.out.println(&quot;没有新的数据啦...&quot;);
                            break SINGLE_MQ;
                        case OFFSET_ILLEGAL:
                            break;
                        default:
                            break;
                    }
                }
                catch (Exception e) {
                    e.printStackTrace();
                }
            }
        }
        consumer.shutdown();
    }


    private static void putMessageQueueOffset(MessageQueue mq, long offset) {
        offseTable.put(mq, offset);
    }


    private static long getMessageQueueOffset(MessageQueue mq) {
        Long offset = offseTable.get(mq);
        if (offset != null)
            return offset;
        return 0;
    }

}
</code></pre><h3 id="MQPullConsumerScheduleService代码样例"><a href="#MQPullConsumerScheduleService代码样例" class="headerlink" title="MQPullConsumerScheduleService代码样例"></a>MQPullConsumerScheduleService代码样例</h3><pre><code>public class PullScheduleService {

    public static void main(String[] args) throws MQClientException {

        String group_name = &quot;test_pull_consumer_name&quot;;

        final MQPullConsumerScheduleService scheduleService = new MQPullConsumerScheduleService(group_name);

        scheduleService.getDefaultMQPullConsumer().setNamesrvAddr(Const.NAMESRV_ADDR_MASTER_SLAVE);

        scheduleService.setMessageModel(MessageModel.CLUSTERING);

        scheduleService.registerPullTaskCallback(&quot;test_pull_topic&quot;, new PullTaskCallback() {

            @Override
            public void doPullTask(MessageQueue mq, PullTaskContext context) {
                MQPullConsumer consumer = context.getPullConsumer();
                System.err.println(&quot;-------------- queueId: &quot; + mq.getQueueId()  + &quot;-------------&quot;);
                try {
                    // 从哪里拉取
                    long offset = consumer.fetchConsumeOffset(mq, false);
                    if (offset &lt; 0)
                        offset = 0;

                    PullResult pullResult = consumer.pull(mq, &quot;*&quot;, offset, 32);
                    switch (pullResult.getPullStatus()) {
                    case FOUND:
                        List&lt;MessageExt&gt; list = pullResult.getMsgFoundList();
                        for(MessageExt msg : list){
                            //消费数据
                            System.out.println(new String(msg.getBody()));
                        }
                        break;
                    case NO_MATCHED_MSG:
                        break;
                    case NO_NEW_MSG:
                    case OFFSET_ILLEGAL:
                        break;
                    default:
                        break;
                    }
                    consumer.updateConsumeOffset(mq, pullResult.getNextBeginOffset());
                    // 设置再过3000ms后重新拉取
                    context.setPullNextDelayTimeMillis(3000);

                }
                catch (Exception e) {
                    e.printStackTrace();
                }
            }
        });

        scheduleService.start();
    }
}
</code></pre><p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天我们聊聊RocketMQ基于拉模式的两种消费方式。&lt;/p&gt;
&lt;p&gt;对于消费而言，RocketMQ提供了推拉两种方式，我们常用的是基于长轮询的DefaultPushConsumer，它具有实时性好，易开发等特点。&lt;/p&gt;
&lt;p&gt;但同时由于是长轮询，因此在大量消息消费的场景下，可能导致broker端CPU负载较大等情况，因此我们会在这种情况下选择使用拉模式的&lt;/p&gt;
&lt;p&gt;PullConsumer或者MQPullConsumerScheduleService+PullTaskCallback这两种方式进行更为灵活的消费。&lt;/p&gt;
    
    </summary>
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/categories/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/tags/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
  </entry>
  
  <entry>
    <title>跟我学RocketMQ之消息持久化原理与Mmap</title>
    <link href="http://wuwenliang.net/2020/02/11/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ%E4%B9%8B%E6%B6%88%E6%81%AF%E6%8C%81%E4%B9%85%E5%8C%96%E5%8E%9F%E7%90%86%E4%B8%8EMmap/"/>
    <id>http://wuwenliang.net/2020/02/11/跟我学RocketMQ之消息持久化原理与Mmap/</id>
    <published>2020-02-11T15:35:37.000Z</published>
    <updated>2020-02-12T14:43:38.032Z</updated>
    
    <content type="html"><![CDATA[<p>大家好，跟我学RocketMQ系列并没有结束。随着笔者对RocketMQ的学习与感悟不断深入，我们的旅程也在继续。</p>
<p>本文我将带领读者朋友们一睹RocketMQ实现高性能消息存储的原理，以及它背后的核心Mmap的风采。</p>
<h2 id="RocketMQ消息持久化-消息不丢失-原理"><a href="#RocketMQ消息持久化-消息不丢失-原理" class="headerlink" title="RocketMQ消息持久化(消息不丢失)原理"></a>RocketMQ消息持久化(消息不丢失)原理</h2><p>在之前的文章中我们已经得知，broker通过调用以下代码实现消息持久化</p>
<pre><code>putMessageResult = this.brokerController.getMessageStore().putMessage(msgInner);
</code></pre><p>我们先不深入putMessage方法的内部实现，单说它背后的原理：<strong>CommitLog的顺序写入机制</strong>。</p>
<p>CommitLog顺序写，其实就是一个WAL，即write ahead log。</p>
<a id="more"></a>
<blockquote>
<p>Write Ahead Log（WAL），即：预写式日志。是关系数据库系统中用于提供原子性和持久性（ACID属性中的两个）的一系列技术。在使用WAL的系统中，所有的修改在提交之前都要先写入log文件中，log文件中通常包括redo和undo信息，通过日志记录描述好数据的改变后(redo和undo)，再写入缓存，等缓存区写满后，最后再往持久层修改数据。</p>
</blockquote>
<p>其实不单单数据库会使用，RocketMQ的commitLog也是WAL的一种，通过对文件的顺序追加写入，提高了文件的写入性能。</p>
<p>我们在RocketMQ的broker端文件系统中，能够看到如下的文件</p>
<pre><code>$HOME/store/consumequeue/{topic}/{queueId}/{fileName}
</code></pre><p>由于我们在broker上设置的每个Topic下都会存在一些MessageQueue，这里的{topic}指代的就是具体的Topic，而{queueId}指代的就是该Topic下某个MessageQueue。</p>
<p>fileName就是MessageQueue中消息在CommitLog中的偏移量，通过这个offset偏移量保证消息读取阶段能够定位到消息的物理位置。</p>
<blockquote>
<p>这个offset可以理解为对CommitLog文件中一个消息的引用。</p>
</blockquote>
<p>除了offset，在ConsumeQueue中还存储了消息的其他属性如，消息长度、消息tag等。单条数据大小为20字节，单个ConsumeQueue文件能够保存30万条数据，每个文件大约占用5.7MB。</p>
<p>也就是说Topic下每个MessageQueue对应了Broker上多个ConsumeQueue文件，这些ConsumeQueue文件保存了该MessageQueue的所有消息在CommitLog文件中的物理位置，即offset偏移量。</p>
<blockquote>
<p>事实上，ConsumeQueue的作用类似索引文件。</p>
</blockquote>
<p>它能够区分不同Topic下的不同MessageQueue的消息，同时能够为消费消息起到一定的<strong>缓冲作用</strong>（当只有ReputMessageService异步服务线程通过doDispatch异步生成了ConsumeQueue队列的元素后，Consumer端才能进行消费）。</p>
<p>这样，只要消息写入成功，并刷盘至CommitLog文件后，<strong>消息就不会丢失</strong>，即使ConsumeQueue中的数据丢失，也可以通过CommitLog来恢复。</p>
<h3 id="如何保证消息写入CommitLog文件性能接近内存写入性能？"><a href="#如何保证消息写入CommitLog文件性能接近内存写入性能？" class="headerlink" title="如何保证消息写入CommitLog文件性能接近内存写入性能？"></a>如何保证消息写入CommitLog文件性能接近内存写入性能？</h3><p>我们都知道的一点是，文件随机写入磁盘的性能是远低于随机写内存的性能。两者性能差距很大。</p>
<p>举个例子：</p>
<pre><code>我们对机械硬盘在随机写入情况下进行性能测试。
测试显示在数据块为512字节时平均写入速度仅为0.083MB/s，
当数据块大小为4KB时，平均写入速度仅为0.576MB/s

当对同样的机械硬盘顺序写情况下进行测试，
测试显示平均写入速度能达到79.0MB/s。
</code></pre><p>可以看到两者相差两个数量级。</p>
<p>说一个结论，顺序写文件的性能约等于随机写内存的性能。这也是RocketMQ为何选择对commitLog进行顺序写的原因。</p>
<p>提升写入性能的核心方法为：</p>
<ul>
<li>基于操作系统的PageCache</li>
<li>顺序写机制</li>
</ul>
<p>他们两者共同提升了commitLog写入性能。</p>
<p>这里重点说一下PageCache。</p>
<h3 id="RocketMQ对PageCache的使用（Mmap）"><a href="#RocketMQ对PageCache的使用（Mmap）" class="headerlink" title="RocketMQ对PageCache的使用（Mmap）"></a>RocketMQ对PageCache的使用（Mmap）</h3><p>PageCache是操作系统对文件的缓存，用于加速对文件的读写。</p>
<blockquote>
<p>一般来说，程序对文件进行顺序读写的速度几乎接近于内存的读写访问，这里的主要原因就是在于OS使用PageCache机制对读写访问操作进行了性能优化，将一部分的内存用作PageCache。</p>
</blockquote>
<p>对于数据文件的读取而言，如果一次读取文件时出现未命中PageCache的情况，OS从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取，即：顺序读入紧随其后的少数几个页面。</p>
<p>这样，只要下次访问的文件已经被加载至PageCache时，读取操作的速度就基本等于访问内存。</p>
<p>而对于数据文件的写入，OS会先写入至Cache内，随后通过异步的方式由<strong>pdflush内核线程</strong>将PageCache内的数据刷盘至物理磁盘上。</p>
<p>RocketMQ对消息的读写的大致做法是：</p>
<ol>
<li>首先将数据文件映射到OS的虚拟内存中，通过JDK NIO的MappedByteBuffer实现，即Mmap，我们后面细讲</li>
<li>当消息写入时，首先写PageCache，并通过异步刷盘的方式将消息批量持久化（同时也支持同步刷盘）</li>
<li>当消息消费者订阅消息时，此时对CommitLog是随机读取。由于PageCache的局部性热点原理，并且整体对消息的读取还是从旧到新的有序读，因此大部分情况下消息还是可以直接从Page Cache中读取，不会产生太多的缺页中断（Page Fault）而从磁盘读取</li>
</ol>
<p>读写速度快，是PageCache的优点，当然它也存在一定问题。</p>
<blockquote>
<p>PageCache的不足</p>
</blockquote>
<p>当OS进行脏页回写、内存回收、内存交换（swap）时，会引起较大的消息读写延迟。</p>
<p>对这些情况，RocketMQ采用多种优化技术，如内存预分配，文件预热，mlock系统调用等，保证了在尽可能地发挥PageCache机制优点的同时，尽力减少其缺点带来的消息读写延迟。</p>
<p>上文提到了RocketMQ消息刷盘，这里进行较为详细的讲解。</p>
<h3 id="RocketMQ消息刷盘"><a href="#RocketMQ消息刷盘" class="headerlink" title="RocketMQ消息刷盘"></a>RocketMQ消息刷盘</h3><p>消息刷盘主要有同步刷盘和异步刷盘两种方式。</p>
<p>先上图：</p>
<p><img src="/2020/02/11/跟我学RocketMQ之消息持久化原理与Mmap/./flush.png" alt="刷盘模式"></p>
<h4 id="同步刷盘"><a href="#同步刷盘" class="headerlink" title="同步刷盘"></a>同步刷盘</h4><p>对于同步刷盘模式，当生产者发送消息到broker，broker收到该消息，会对消息进行刷盘操作，只有消息刷盘成功才会返回ACK到生产者，保证消息写入一定能够成功。</p>
<p>同步刷盘保证了消息的可靠性，但对性能有较大影响，因为这个过程是完全同步的，应用场景主要适用于金融、支付、物流等对消息可靠性要求高的领域。</p>
<p>RocketMQ同步刷盘的大致做法如下：</p>
<ol>
<li>基于生产者消费者模型，主线程创建刷盘请求实例GroupCommitRequest</li>
<li>将请求实例GroupCommitRequest放入刷盘写队列后唤醒同步刷盘线程GroupCommitService执行刷盘动作（通过CAS变量和CountDownLatch来保证线程间的同步）</li>
<li>RocketMQ源码中用读写双缓存队列（requestsWrite/requestsRead）来实现读写分离，好处在于内部消费生成的同步刷盘请求可以不用加锁，提高并发度</li>
</ol>
<h4 id="异步刷盘"><a href="#异步刷盘" class="headerlink" title="异步刷盘"></a>异步刷盘</h4><p>默认情况下，broker采用异步刷盘策略。</p>
<p>异步刷盘，顾名思义，当broker收到消息时，并不会直接将消息刷盘，而是先写入PageCache，写入过程是很快的，这里完全是一个内存写操作。</p>
<p>写入成功后，直接返回ACK给生产者。然后在后台通过异步刷盘线程将消息异步写入commitLog，降低了读写延迟，提高了MQ的性能和吞吐量。</p>
<p>不同于同步刷盘，异步过程下，主线程不会阻塞，主线程唤醒刷盘线程后就会继续执行后续操作，提升了吞吐量。</p>
<p>当系统对消息的可靠性要求较低，可以通过异步刷盘策略提升消息吞吐量及读写性能。</p>
<p>原因在于，PageCache本质上还是内存，当出现掉电等情况，os cache中的消息就会丢失。这种情况在极端条件会出现，因此做好异地容灾及跨域同步等高可用策略后，基本上可以降低掉电造成的影响。</p>
<h2 id="Mmap内存映射及RocketMQ中的应用"><a href="#Mmap内存映射及RocketMQ中的应用" class="headerlink" title="Mmap内存映射及RocketMQ中的应用"></a>Mmap内存映射及RocketMQ中的应用</h2><p>首先我们来复习一下Mmap内存映射（零拷贝 zero copy）机制。</p>
<p>首先我们复习一下传统的io操作。传统io分为缓冲io和直接io，如下图所示</p>
<p><img src="/2020/02/11/跟我学RocketMQ之消息持久化原理与Mmap/./zhijie-io.png" alt="直接IO"></p>
<p><img src="/2020/02/11/跟我学RocketMQ之消息持久化原理与Mmap/./huanchong-io.png" alt="缓冲IO"></p>
<p>内核缓冲区为OS的pageCache，为了加快磁盘IO，Linux将磁盘上的数据以Page为单位缓存在系统的内存中，这里的Page是Linux系统定义的一个逻辑概念，一个Page大小一般为4KB。</p>
<p>对于缓冲IO，对操作有三次数据拷贝，写操作则为反向的三次数据拷贝。</p>
<blockquote>
<p>读操作：</p>
</blockquote>
<pre><code>磁盘-&gt;内核缓冲区-&gt;用户缓冲区-&gt;应用程序内存
</code></pre><blockquote>
<p>写操作：</p>
</blockquote>
<pre><code>应用程序内存-&gt;用户缓冲区-&gt;内核缓冲区-&gt;磁盘
</code></pre><p>对于直接IO，少了用户缓冲区，因此对于读操作会有两次数据拷贝，对于写操作，会有反向的两次数据拷贝。</p>
<p>直接IO的意思就是没有了用户级别的缓冲，操作系统内核态的缓冲还是存在的。</p>
<blockquote>
<p>读操作：</p>
</blockquote>
<pre><code>磁盘-&gt;内核缓冲区-&gt;应用程序内存
</code></pre><blockquote>
<p>写操作：</p>
</blockquote>
<pre><code>应用程序内存-&gt;内核缓冲区-&gt;磁盘
</code></pre><p>如果RocketMQ采用传统的直接IO或者缓冲IO，则文件拷贝次数就会大大增加，降低读写效率， 因此引入了零拷贝策略。</p>
<p>零拷贝在Java中的实现是MappedByteBuffer，它的核心原理是内存映射文件，如图所示：</p>
<p><img src="/2020/02/11/跟我学RocketMQ之消息持久化原理与Mmap/neicun-yingshe.png" alt="内存映射文件"></p>
<p>通过将应用程序的逻辑内存地址直接映射到Linux操作系统的内核缓冲区，应用程序通过读写自己的逻辑内存，达到实际操作操作系统内核缓冲区的效果，减少了用户态与内核态之间的数据拷贝次数。</p>
<p>由于内核态与用户态之间没有数据拷贝，因此叫零拷贝。</p>
<p>这里我们要区分“拷贝”和“映射”两个概念：</p>
<blockquote>
<p>拷贝是将数据从一块内存复制到另一块内存中；而映射只是持有了数据的一个引用(即地址)，数据本身只有一个副本。</p>
</blockquote>
<p>在Linux中，零拷贝通过sendFile实现，在Java中，通过FileChannel.transferTo实现。</p>
<p>那么，<strong>RocketMQ具体是如何基于MappedByteBuffer内存映射文件实现高性能文件读写的呢？</strong></p>
<p>在Java中，MappedByteBuffer映射文件要求文件大小小于2GB，RocketMQ中的每个commitlog大小最大为1G，单个ConsumeQueue文件大小月维护5.7MB。是符合MappedByteBuffer文件映射要求的。</p>
<p>当commitlog通过MappedByteBuffer的map()函数映射到内存中后，就可以对其进行读写操作，操作本身完全是基于内存进行的，因此效率很高。消息直接写入到PageCache中，再异步地被异步刷盘线程持久化到磁盘文件中。</p>
<p>当进行读取操作时，文件如果在PageCache中，则直接从内存中读取，而大部分文件是会在内存中命中的，少部分不在PageCache中的文件需要发生一次缺页中断重新映射到内存页中，被读到。</p>
<p>我们在上文中已经知道，一个Page大小一般为4KB，因此一次缺页中断会将一批数据映射到内存中，这也是性能提高的原因之一。</p>
<h3 id="其他零拷贝策略"><a href="#其他零拷贝策略" class="headerlink" title="其他零拷贝策略"></a>其他零拷贝策略</h3><ul>
<li>硬件：基于DMA传输数据</li>
<li>软件：基于Linux的sendFile</li>
</ul>
<p>操作系统层面的零拷贝微观细节如下（假设应用为Java进程）：</p>
<ul>
<li>JVM向OS发出read()系统调用触发上下文切换，从用户态切换到内核态</li>
<li>从外部存储（如硬盘）读取文件内容，通过直接内存访问（DMA）存入内核地址空间的缓冲区</li>
<li>将数据从内核缓冲区拷贝到用户空间缓冲区，read()系统调用返回，并从内核态切换回用户态</li>
<li>JVM向OS发出write()系统调用，触发上下文切换，从用户态切换到内核态</li>
<li>将数据从用户缓冲区拷贝到内核中与目的地Socket关联的缓冲区</li>
<li>数据最终经由Socket通过DMA传送到硬件（如网卡）缓冲区，write()系统调用返回，并从内核态切换回用户态</li>
</ul>
<h3 id="内存预映射机制"><a href="#内存预映射机制" class="headerlink" title="内存预映射机制"></a>内存预映射机制</h3><p>RocketMQ在消息写入过程中，通过调用CommitLog的 <strong>putMessage()</strong> 方法，</p>
<p>CommitLog会先从MappedFileQueue队列中获取一个 MappedFile，如果没有就新建一个。</p>
<p>这里MappedFile的创建过程是先构建一个AllocateRequest请求，具体做法是：</p>
<ul>
<li>将下一个文件的路径、下下个文件的路径、文件大小为作为参数封装为AllocateRequest对象添加至队列中</li>
<li>在Broker启动时，后台创建并运行AllocateMappedFileService服务线程。该线程会不停地run；只要请求队列里存在请求，就会执行MappedFile映射文件的创建和预分配工作。</li>
<li>分配的时候有两种策略，一种是使用Mmap的方式来构建MappedFile实例，另一种是从TransientStorePool堆外内存池中获取相应的DirectByteBuffer来构建MappedFile（具体采用哪种策略与刷盘的方式有关）</li>
<li>在创建分配完下个MappedFile后，会将下下个MappedFile预先创建并保存至请求队列中等待下次获取时直接返回</li>
</ul>
<p>这种策略便是RocketMQ预分配MappedFile，也叫 <strong>内存预映射机制</strong>。 它的思路很巧妙，能够在下次获取时候直接返回MappedFile实例而不用等待MappedFile创建分配所产生的时间延迟。</p>
<h3 id="内存预热"><a href="#内存预热" class="headerlink" title="内存预热"></a>内存预热</h3><p>RocketMQ在创建并分配MappedFile的过程中预先写入了一些随机值到Mmap映射出的内存空间里。原因在于：</p>
<blockquote>
<p>仅分配内存并进行mlock系统调用后并不会为程序完全锁定这些分配的内存，原因在于其中的分页可能是写时复制的。因此，就有必要对每个内存页面中写入一个假的值。</p>
</blockquote>
<p>还有一个问题，当调用Mmap进行内存映射后，OS只是建立了虚拟内存地址至物理地址的映射表，而实际并没有加载任何文件至内存中。</p>
<p>程序要访问数据时，OS会检查该部分的分页是否已经在内存中，如果不在，则发出一次 <strong>缺页中断</strong>。X86的Linux中一个标准页面大小是4KB，那么1G的CommitLog需要发生 <strong>1024KB/4KB=256次</strong>  缺页中断，才能使得对应的数据完全加载至物理内存中。</p>
<p>为了避免OS检查分页是否在内存中的过程出现大量缺页中断，RocketMQ在做Mmap内存映射的同时进行了madvise系统调用，目的是使OS做一次内存映射后，使对应的文件数据尽可能多的预加载至内存中，降低缺页中断次数，从而达到内存预热的效果。</p>
<p>RocketMQ通过map+madvise映射后预热机制，将磁盘中的数据尽可能多的加载到PageCache中，保证后续对ConsumeQueue和CommitLog的读取过程中，能够尽可能从内存中读取数据，提升读写性能。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>根据上文的论述，结合我们的讨论和思考，我们能够总结RocketMQ存储架构的优缺点：</p>
<blockquote>
<p>优点</p>
</blockquote>
<ul>
<li>ConsumeQueue消息逻辑队列较为轻量级，易于理解</li>
<li>对磁盘的访问串行化，能够避免磁盘竟争，不会因为队列增加而导致IOWAIT增高</li>
</ul>
<blockquote>
<p>缺点</p>
</blockquote>
<ul>
<li>对于CommitLog而言，写入消息虽然是顺序写，但是读却变成了完全随机读</li>
<li>Consumer端订阅消费一条消息，需要先读ConsumeQueue，再读CommitLog，这在一定程度上增加了性能开销</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://cloud.tencent.com/developer/article/1352677" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1352677</a></p>
<p><a href="https://blog.csdn.net/smallcatbaby/article/details/93799959" target="_blank" rel="external">https://blog.csdn.net/smallcatbaby/article/details/93799959</a></p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大家好，跟我学RocketMQ系列并没有结束。随着笔者对RocketMQ的学习与感悟不断深入，我们的旅程也在继续。&lt;/p&gt;
&lt;p&gt;本文我将带领读者朋友们一睹RocketMQ实现高性能消息存储的原理，以及它背后的核心Mmap的风采。&lt;/p&gt;
&lt;h2 id=&quot;RocketMQ消息持久化-消息不丢失-原理&quot;&gt;&lt;a href=&quot;#RocketMQ消息持久化-消息不丢失-原理&quot; class=&quot;headerlink&quot; title=&quot;RocketMQ消息持久化(消息不丢失)原理&quot;&gt;&lt;/a&gt;RocketMQ消息持久化(消息不丢失)原理&lt;/h2&gt;&lt;p&gt;在之前的文章中我们已经得知，broker通过调用以下代码实现消息持久化&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;putMessageResult = this.brokerController.getMessageStore().putMessage(msgInner);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们先不深入putMessage方法的内部实现，单说它背后的原理：&lt;strong&gt;CommitLog的顺序写入机制&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;CommitLog顺序写，其实就是一个WAL，即write ahead log。&lt;/p&gt;
    
    </summary>
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/categories/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/tags/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
  </entry>
  
  <entry>
    <title>自己写RPC之实现服务注册与发现</title>
    <link href="http://wuwenliang.net/2020/02/03/%E8%87%AA%E5%B7%B1%E5%86%99RPC%E4%B9%8B%E5%AE%9E%E7%8E%B0%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0/"/>
    <id>http://wuwenliang.net/2020/02/03/自己写RPC之实现服务注册与发现/</id>
    <published>2020-02-03T09:56:22.000Z</published>
    <updated>2020-02-03T12:30:14.765Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文开始继续我们的造轮子之旅。</p>
<p>这个系列笔者将带领读者朋友实现简易的基于Netty、curator以及springBoot等技术的一个简易RPC通信轮子。</p>
<p>实现最基本的服务发现、服务注册、RPC通信等功能。该项目命名为：<strong>misaka</strong>，她是《某科学的超电磁炮》的女主角御坂美琴的名字。</p>
</blockquote>
<p>本文是该系列的第一篇，主要实现服务注册与发现功能。</p>
<p>我选择zookeeper作为服务注册发现的核心组件，使用curator作为与zookeeper通信的客户端。</p>
<p>curator提供了一个服务注册发现的实现，<strong>curator-x-discovery</strong>，只需要在项目中引入即可。</p>
<a id="more"></a>
<p>建立项目misaka-api，在pom中引入如下依赖：</p>
<pre><code>&lt;!-- https://mvnrepository.com/artifact/org.apache.curator/curator-framework --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;
    &lt;artifactId&gt;curator-framework&lt;/artifactId&gt;
    &lt;version&gt;4.2.0&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- https://mvnrepository.com/artifact/org.apache.curator/curator-x-discovery --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;
    &lt;artifactId&gt;curator-x-discovery&lt;/artifactId&gt;
    &lt;version&gt;4.2.0&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- https://mvnrepository.com/artifact/io.netty/netty-all --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;io.netty&lt;/groupId&gt;
    &lt;artifactId&gt;netty-all&lt;/artifactId&gt;
    &lt;version&gt;4.1.44.Final&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><p>引入netty作为网络通信组件。</p>
<h2 id="服务发现核心类：DiscoveryService"><a href="#服务发现核心类：DiscoveryService" class="headerlink" title="服务发现核心类：DiscoveryService"></a>服务发现核心类：DiscoveryService</h2><blockquote>
<p>声明一些变量以便后续使用</p>
</blockquote>
<pre><code>/**zk连接地址*/
private static final String connectString = &quot;127.0.0.1:2181&quot;;
/**CuratorFramework*/
private CuratorFramework client = null;
/**服务发现实例*/
private ServiceDiscovery&lt;String&gt; discovery = null;
/**服务提供者实例*/
private ServiceProvider&lt;String&gt; provider = null;
/**zk根节点*/
private static final String BASE_PATH = &quot;/misaka&quot;;
/**记录服务提供者列表，便于统一进行关闭*/
private List&lt;Closeable&gt; closeableList = Lists.newArrayList();

private Object lock = new Object();
/** 服务注册表*/
private Map&lt;String, ServiceProvider&lt;String&gt;&gt; providers = Maps.newConcurrentMap();
</code></pre><blockquote>
<p>对外提供一个init方法，用于初始化Curator客户端并启动</p>
</blockquote>
<pre><code>public void init() {
    // 初始化Curator客户端并启动
    client = CuratorFrameworkFactory.newClient(connectString, new RetryOneTime(1));
    client.start();
    // 构造服务发现
    discovery =
            ServiceDiscoveryBuilder.builder(String.class)
                    .basePath(BASE_PATH)
                    .client(client)
                    .build();
    try {
        discovery.start();
    } catch (Exception e) {
        throw new RuntimeException(&quot;启动服务发现异常&quot;, e);
    }
}
</code></pre><p>启动Curator客户端之后，通过ServiceDiscoveryBuilder这个构造器实例化一个ServiceDiscovery对象，并启动服务发现实例。</p>
<blockquote>
<p>对外提供一个服务注册方法，用于在应用启动阶段对服务进行注册，将服务元信息写入zookeeper中</p>
</blockquote>
<pre><code>public void registeService(ServiceInstanceNode node) throws Exception {
    if (node.getPort() == null) {
        return;
    }
    ServiceInstance&lt;String&gt; serviceInstance =
            ServiceInstance.&lt;String&gt;builder()
                    .payload(node.getPayload())
                    .name(node.getServiceName())
                    .port(Integer.valueOf(node.getPort())).build();
    discovery.registerService(serviceInstance);
}
</code></pre><p>我们定义了一个ServiceInstanceNode实例，通过ServiceInstanceNode实例初始化一个ServiceInstance实例，ServiceInstance作为服务实例通过discovery.registerService方法注册到zookeeper中。</p>
<blockquote>
<p>对外提供一个  <strong>getInstanceByName</strong> 方法，允许调用方根据服务名查询具体的ServiceInstance实例，从而获取服务的具体信息。</p>
</blockquote>
<p>我们看一下ServiceInstance具体有哪些属性：</p>
<pre><code>public class ServiceInstance&lt;T&gt; {
    private final String        name;                   // 服务名称
    private final String        id;                     // 服务id
    private final String        address;                // 可寻址的服务域名或ip
    private final Integer       port;                   // 服务暴露端口
    private final Integer       sslPort;                // ssl端口
    private final T             payload;                // 自定义信息
    ......
</code></pre><p>可以看到，ServiceInstance包含了服务名称、服务id、服务地址以及端口等信息，我们提供的getInstanceByName方法给客户端，方便客户端获取服务元信息，从而实现服务发现。</p>
<p>getInstanceByName方法逻辑如下：</p>
<pre><code>public ServiceInstance&lt;String&gt; getInstanceByName(String serviceName) throws Exception {
    ServiceProvider&lt;String&gt; provider = providers.get(serviceName);
    if (provider == null) {
        synchronized (lock) {
            provider = providers.get(serviceName);
            if (provider == null) {
                // 随机策略
                provider = discovery.serviceProviderBuilder().serviceName(serviceName)
                        .providerStrategy(new RandomStrategy&lt;String&gt;()).build();
                provider.start();
                closeableList.add(provider);
                providers.put(serviceName, provider);
            }
        }
    }
    return provider.getInstance();
}
</code></pre><p>providers是基于ConcurrentHashMap实现的一个服务注册表。每次来的时候先从provider中尝试获取ServiceProvider实例，如果获取不到则使用双重检查锁机制从discovery中获取服务提供者provider，这里使用了随机策略RandomStrategy(其余实现还有轮询策略、sticky粘滞策略)。</p>
<p>在serviceProvider中获取到provider提供者 之后，添加到providers中，同时添加到closeableList以便后续调用shutdown对所有的provider进行关闭操作。</p>
<blockquote>
<p>提供一个shutdown方法用于关闭资源</p>
</blockquote>
<pre><code>public synchronized void shutdown() {
    for (Closeable closeable : closeableList) {
        CloseableUtils.closeQuietly(closeable);
    }
    CloseableUtils.closeQuietly(discovery);
    CloseableUtils.closeQuietly(client);
}
</code></pre><p>这里我们对closeableList进行迭代，逐个关闭ServiceProvider实例。</p>
<p>ServiceProvider实现了Closeable接口，因此它也是一个Closeable的实例。</p>
<p>到这里，我们就实现了服务注册与发现的核心功能，接着通过一个demo案例去测试一下。</p>
<h2 id="服务注册实现"><a href="#服务注册实现" class="headerlink" title="服务注册实现"></a>服务注册实现</h2><p>新建服务misaka-provider，作为服务提供者，它在启动之后会对服务进行注册。</p>
<p>编写RegistyHandler添加@Configuration注解，标记为一个配置类。</p>
<h3 id="注册DiscoveryService实例"><a href="#注册DiscoveryService实例" class="headerlink" title="注册DiscoveryService实例"></a>注册DiscoveryService实例</h3><pre><code>@Bean(initMethod = &quot;init&quot;, destroyMethod = &quot;shutdown&quot;)
public DiscoveryService discoveryService() {
    DiscoveryService discoveryService = new DiscoveryService();
    return discoveryService;
}
</code></pre><p>声明并向Spring容器中注册DiscoveryService实例，标记初始化方法为init，销毁方法为shutdown</p>
<h3 id="注册服务实现类HelloServiceImpl"><a href="#注册服务实现类HelloServiceImpl" class="headerlink" title="注册服务实现类HelloServiceImpl"></a>注册服务实现类HelloServiceImpl</h3><p>定义RPC接口HelloService</p>
<pre><code>public interface HelloService {

    public String sayHello(String name, String content);
}
</code></pre><p>声明一个sayHello方法，用过dubbo等RPC框架的同学应当很熟悉了。没错，这里的HelloService在Dubbo中就是服务定义接口。</p>
<p>编写实现类HelloServiceImpl实现HelloService</p>
<pre><code>public class HelloServiceImpl implements HelloService {

    @Override
    public String sayHello(String name, String content) {
        return new StringBuilder(&quot;hello:&quot;).append(name).append(&quot;, content:&quot;).append(content).toString();
    }
}
</code></pre><blockquote>
<p>注册服务实现类</p>
</blockquote>
<p>在RegistyHandler注册类中，注册HelloServiceImpl实例，并将其元信息注册到discoveryService中。</p>
<pre><code>@Bean
@ConditionalOnBean(value = DiscoveryService.class)
public HelloServiceImpl helloServiceImpl(DiscoveryService discoveryService) {
    HelloServiceImpl helloService = new HelloServiceImpl();
    ServiceInstanceNode helloServiceNode = new ServiceInstanceNode();
    // 服务注册
    helloServiceNode.setPort(servicePort).setAddress(ipAddress).setServiceName(HelloServiceImpl.class.getName());
    try {
        discoveryService.registeService(helloServiceNode);
    } catch (Exception e) {
        throw new RuntimeException(&quot;注册HelloServiceImpl异常&quot;);
    }
    return helloService;
}
</code></pre><p>通过 <strong>@ConditionalOnBean(value = DiscoveryService.class)</strong> 条件注册告诉Spring容器只有存在DiscoveryService实例才注册HelloServiceImpl。</p>
<p>构造一个ServiceInstanceNode，设置属性后通过discoveryService.registeService方法将元信息注册到zookeeper中。</p>
<p>服务注册部分的开发就告一段落，我们接着看下服务发现部分的代码实现。</p>
<h2 id="服务发现实现"><a href="#服务发现实现" class="headerlink" title="服务发现实现"></a>服务发现实现</h2><p>新建服务misaka-consumer，作为服务提供者，它会根据服务名称取zookeeper进行查询，获取具体的服务元信息。</p>
<p>和服务提供者服务类似，定义一个RegistyHandler类，添加注解@Configuration。</p>
<blockquote>
<p>注册DiscoveryService实例</p>
</blockquote>
<pre><code>@Bean(initMethod = &quot;init&quot;, destroyMethod = &quot;shutdown&quot;)
public DiscoveryService discoveryService() {
    DiscoveryService discoveryNode = new DiscoveryService();
    return discoveryNode;
}
</code></pre><p>向Spring容器中注册DiscoveryService实例，用于服务发现。</p>
<pre><code>@Bean(destroyMethod = &quot;shutdown&quot;)
@ConditionalOnBean(DiscoveryService.class)
public RemoteClient remoteClient(DiscoveryService discoveryService) {
    RemoteClient remoteClient;
    try {
        // 服务发现
        ServiceInstance&lt;String&gt; serviceInstance = discoveryService.getInstanceByName(helloServiceName);
        LOGGER.info(JSON.toJSONString(serviceInstance));
        ...省略其他逻辑...

    } catch (Exception e) {
        throw new RuntimeException(&quot;init RuntimeException error!&quot;, e);
    }
    return remoteClient;
}
</code></pre><p>注册客户端通信RemoteClient实例，这个类的作用为封装Netty用于RPC网络通信，具体逻辑在后续的通信实现部分进行讲解。</p>
<p>我们注意看try-catch中的代码，从DiscoveryService中获取了服务名为 key = <strong> helloServiceName</strong> 的服务实现，value通过@Value注解获取，具体值配置在application.properties中。</p>
<pre><code>misaka.service.HelloService.name=com.snowalker.misaka.misaka.service.HelloServiceImpl
</code></pre><p>在RegistyHandler声明如下：</p>
<pre><code>@Value(&quot;${misaka.service.HelloService.name}&quot;)
private String helloServiceName;
</code></pre><p>通过discoveryService.getInstanceByName方法获取到helloServiceName对应的具体服务元信息后，我们通过日志进行打印。</p>
<p>接下来先后启动提供者服务，消费者服务，对服务注册发现逻辑进行测试。</p>
<h2 id="测试服务注册及发现"><a href="#测试服务注册及发现" class="headerlink" title="测试服务注册及发现"></a>测试服务注册及发现</h2><p>首先启动提供者服务，控制台输出如下：</p>
<pre><code>......
2020-02-03 20:11:34.304  INFO 31092 --- [           main] org.apache.zookeeper.ZooKeeper           : 
Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=60000 watcher=org.apache.curator.ConnectionState@31c269fd
2020-02-03 20:11:34.313  INFO 31092 --- [           main] org.apache.zookeeper.ClientCnxnSocket    : 
jute.maxbuffer value is 4194304 Bytes
2020-02-03 20:11:34.317  INFO 31092 --- [127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn          : 
Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2020-02-03 20:11:34.319  INFO 31092 --- [127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn          : 
Socket connection established, initiating session, client: /127.0.0.1:8071, server: 127.0.0.1/127.0.0.1:2181
2020-02-03 20:11:34.320  INFO 31092 --- [           main] o.a.c.f.imps.CuratorFrameworkImpl        : 
Default schema
2020-02-03 20:11:34.323  INFO 31092 --- [127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn          : 
Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x1000fa2eb800032, negotiated timeout = 40000
2020-02-03 20:11:34.327  INFO 31092 --- [ain-EventThread] o.a.c.f.state.ConnectionStateManager     : 
State change: CONNECTED
2020-02-03 20:14:27.180  INFO 31048 --- [           main] c.s.misaka.misaka.config.RegistyHandler  : 
service ： com.snowalker.misaka.misaka.service.HelloServiceImpl  registered success

......
</code></pre><p>可以看到已经与zookeeper建立了链接，并注册helloServiceImpl服务到zookeeper中。</p>
<p>接着启动服务消费者服务，控制台输出如下：</p>
<pre><code>2020-02-03 17:52:49.673  INFO 6824 --- [127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn          : 
Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2020-02-03 17:52:49.674  INFO 6824 --- [127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn          : 
Socket connection established, initiating session, client: /127.0.0.1:4290, server: 127.0.0.1/127.0.0.1:2181
2020-02-03 17:52:49.676  INFO 6824 --- [           main] o.a.c.f.imps.CuratorFrameworkImpl        : 
Default schema
2020-02-03 17:52:49.678  INFO 6824 --- [127.0.0.1:2181)] org.apache.zookeeper.ClientCnxn          : 
Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x1000fa2eb800031, negotiated timeout = 40000
2020-02-03 17:52:49.682  INFO 6824 --- [ain-EventThread] o.a.c.f.state.ConnectionStateManager     : 
State change: CONNECTED
2020-02-03 17:52:49.790  INFO 6824 --- [           main] c.s.m.m.config.RegistyHandler            : 
{&quot;address&quot;:&quot;192.168.0.100&quot;,&quot;enabled&quot;:true,&quot;id&quot;:&quot;5e44a372-f1e0-44da-8a22-daa81a346f37&quot;,
        &quot;name&quot;:&quot;com.snowalker.misaka.misaka.service.HelloServiceImpl&quot;,&quot;port&quot;:18083,
        &quot;registrationTimeUTC&quot;:1580723559543,&quot;serviceType&quot;:&quot;DYNAMIC&quot;}
</code></pre><p>注意观察最后一行日志，这里打印出了HelloServiceImpl服务的注册元信息，该元信息即是提供者服务启动时注册到zookeeper上的服务元信息。</p>
<blockquote>
<p>我们使用zk-cli观察一下zookeeper中的Node节点:</p>
</blockquote>
<p>笔者已经在本地部署了一套zk-ui，关于zk-ui的使用可以自行查看附录中的参考链接。</p>
<p><img src="/2020/02/03/自己写RPC之实现服务注册与发现/./zkui.png" alt="zkui.png"></p>
<p>从图中可以看出，服务HelloServiceImpl已经成功注册到zookeeper上，并且能够被服务消费者发现。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>作为 “自己写RPC” 系列的第一篇，本文详细的讲解了如何利用curator整合spring Boot框架实现跨服务的服务注册与发现功能，并且给出了详细的代码实现与讲解。</p>
<p>在开发过程中，笔者强烈地体会到掌握zookeeper组件对于后端开发者的必要性，zookeeper真的很强大。</p>
<p>在后续的文章中，我将继续带领读者，实现远程服务调用逻辑。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><a href="https://www.zifangsky.cn/1126.html" target="_blank" rel="external">zk-cli使用</a></p>
<p><a href="https://github.com/zifangsky/zkui/releases" target="_blank" rel="external">zk-ui代码地址</a></p>
<p><a href="cnblogs.com/duanxz/p/3783893.html">zk 10之：Curator之三：服务的注册及发现</a></p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文开始继续我们的造轮子之旅。&lt;/p&gt;
&lt;p&gt;这个系列笔者将带领读者朋友实现简易的基于Netty、curator以及springBoot等技术的一个简易RPC通信轮子。&lt;/p&gt;
&lt;p&gt;实现最基本的服务发现、服务注册、RPC通信等功能。该项目命名为：&lt;strong&gt;misaka&lt;/strong&gt;，她是《某科学的超电磁炮》的女主角御坂美琴的名字。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文是该系列的第一篇，主要实现服务注册与发现功能。&lt;/p&gt;
&lt;p&gt;我选择zookeeper作为服务注册发现的核心组件，使用curator作为与zookeeper通信的客户端。&lt;/p&gt;
&lt;p&gt;curator提供了一个服务注册发现的实现，&lt;strong&gt;curator-x-discovery&lt;/strong&gt;，只需要在项目中引入即可。&lt;/p&gt;
    
    </summary>
    
      <category term="自己写分布式组件系列" scheme="http://wuwenliang.net/categories/%E8%87%AA%E5%B7%B1%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%84%E4%BB%B6%E7%B3%BB%E5%88%97/"/>
    
    
      <category term="自己写分布式组件系列" scheme="http://wuwenliang.net/tags/%E8%87%AA%E5%B7%B1%E5%86%99%E5%88%86%E5%B8%83%E5%BC%8F%E7%BB%84%E4%BB%B6%E7%B3%BB%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Spring之Bean初始化与销毁</title>
    <link href="http://wuwenliang.net/2020/01/27/%E8%B7%9F%E6%88%91%E5%AD%A6Spring%E4%B9%8BBean%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%94%80%E6%AF%81/"/>
    <id>http://wuwenliang.net/2020/01/27/跟我学Spring之Bean创建与销毁/</id>
    <published>2020-01-27T04:13:40.000Z</published>
    <updated>2020-01-27T10:09:38.951Z</updated>
    
    <content type="html"><![CDATA[<p>Spring框架以Bean为核心，为开发者提供了一系列的特性和能力。其中最为核心的就是IOC特性，而IOC中又以Bean定义及其初始化/销毁为核心。</p>
<p>本文我们对Bean的初始化和销毁进行较为深入的整理与学习。</p>
<a id="more"></a>
<h2 id="Bean初始化"><a href="#Bean初始化" class="headerlink" title="Bean初始化"></a>Bean初始化</h2><p>一般而言，Bean初始化有三种方式：</p>
<ul>
<li>通过@PostConstruct注解对Bean进行初始化</li>
<li>通过实现InitializingBean接口的afterPropertiesSet()方式对Bean进行初始化</li>
<li>通过自定义初始化方法实现<ul>
<li>通过XML配置中的 <bean init-method="自定义初始化方法名" ...=""> 指定</bean></li>
<li>通过@Bean属性initMethod=”自定义初始化方法名” 指定</li>
<li>通过编程方式实现：使用AbstractBeanDefination#setInitMethodName(String) 实现，方法参数为自定义的初始化方法名</li>
</ul>
</li>
</ul>
<p>这三种方式还存在优先级，从高到低依次是：</p>
<pre><code>@PostConstruct注解 &gt; InitializingBean接口 &gt; 自定义初始化方法
</code></pre><p>接下来我们就详细进行说明。</p>
<h3 id="通过-PostConstruct注解对Bean进行初始化"><a href="#通过-PostConstruct注解对Bean进行初始化" class="headerlink" title="通过@PostConstruct注解对Bean进行初始化"></a>通过@PostConstruct注解对Bean进行初始化</h3><p>在一个Spring环境下，定义一个Bean名为BeanInitDemo，为其添加@PostConstruct标注的初始化方法，<strong>注意</strong> 该初始化方法不能有返回值（即：需要放回void）</p>
<pre><code>public class BeanInitDemo {

    private String name;

    public BeanInitDemo(String name) {
        this.name = name;
    }

    @PostConstruct
    public void init() {
        System.out.println(&quot;@PostConstruct方式初始化&quot;);
    }

}
</code></pre><p>init()方法此时就是一个初始化方法</p>
<h3 id="通过实现InitializingBean接口对Bean进行初始化"><a href="#通过实现InitializingBean接口对Bean进行初始化" class="headerlink" title="通过实现InitializingBean接口对Bean进行初始化"></a>通过实现InitializingBean接口对Bean进行初始化</h3><p>还是上面的BeanInitDemo类，我们让其实现接口InitializingBean，并实现afterPropertiesSet回调方法</p>
<pre><code>public class BeanInitDemo implements InitializingBean {

    private String name;

    public BeanInitDemo(String name) {
        this.name = name;
    }

    @PostConstruct
    public void init() {
        System.out.println(&quot;@PostConstruct方式初始化&quot;);
    }

    @Override
    public void afterPropertiesSet() throws Exception {
        System.out.println(&quot;InitializingBean方式初始化&quot;);
    }
}
</code></pre><p>我们可以在afterPropertiesSet()方法中实现bean初始化逻辑。</p>
<h3 id="通过自定义初始化方法进行初始化"><a href="#通过自定义初始化方法进行初始化" class="headerlink" title="通过自定义初始化方法进行初始化"></a>通过自定义初始化方法进行初始化</h3><p>通过bean定义时显式指定初始化方法，实现自定义初始化。</p>
<p>我们在上面的BeanInitDemo类上添加自定义方法：initMethod()，BeanInitDemo类现在如下所示</p>
<pre><code>public class BeanInitDemo implements InitializingBean {

    private String name;

    public BeanInitDemo(String name) {
        this.name = name;
    }

    @PostConstruct
    public void init() {
        System.out.println(&quot;@PostConstruct方式初始化&quot;);
    }

    @Override
    public void afterPropertiesSet() throws Exception {
        System.out.println(&quot;InitializingBean方式初始化&quot;);
    }

    public void initMethod() {
        System.out.println(&quot;自定义init方式&quot;);
    }
}
</code></pre><h3 id="测试三种初始化方式的优先级"><a href="#测试三种初始化方式的优先级" class="headerlink" title="测试三种初始化方式的优先级"></a>测试三种初始化方式的优先级</h3><p>想要实现对三种方式优先级的测试还需要对bean进行装配，这里使用javaConfig方式对BeanInitDemo进行装配。</p>
<p>编写一个配置类BeanInitConfig，对BeanInitDemo这个bean进行装配：</p>
<pre><code>@Configuration
public class BeanInitConfig {

    @Bean(initMethod = &quot;initMethod&quot;)
    public BeanInitDemo beanInitDemo() {
        return new BeanInitDemo(&quot;BeanInitDemo&quot;);
    }
}
</code></pre><p>通过@Configuration标记BeanInitConfig为配置bean，通过@Bean声明一个BeanInitDemo类型的bean，beanName就是方法名。通过属性initMethod指定初始化方法为initMethod()</p>
<p>声明一个main方法对bean初始化方式优先级进行测试：</p>
<pre><code>public class Client {

    public static void main(String[] args) {
        // 注解驱动应用上下文
        AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext();
        // 注册Configuration
        applicationContext.register(BeanInitConfig.class);
        // 启动应用上下文
        applicationContext.refresh();
        // 关闭应用上下文
        applicationContext.close();
    }
}
</code></pre><p>具体过程在注解中已经表明，这里主要关注一下refresh()方法，它的实现在 <strong>org.springframework.context.support.AbstractApplicationContext</strong> 类中，通过模板模式对bean进行了加载，感兴趣可以自行查看。</p>
<p>通过 applicationContext.register(BeanInitConfig.class);对配置bean进行加载，接着启动了应用上下文，运行该main方法，查看控制台：</p>
<pre><code>@PostConstruct方式初始化
InitializingBean方式初始化
自定义init方式
</code></pre><p>从输出我们能够明确看出，三种初始化方式的优先级为：</p>
<pre><code>@PostConstruct注解 &gt; InitializingBean接口 &gt; 自定义初始化方法
</code></pre><h2 id="Bean销毁"><a href="#Bean销毁" class="headerlink" title="Bean销毁"></a>Bean销毁</h2><p>我们接着学习Bean的销毁方式。</p>
<p>与Bean初始化类似，Bean的销毁也有三种方式：</p>
<ul>
<li>通过@PreDestroy注解标记方法</li>
<li>实现接口DisposableBean的destroy()方法</li>
<li>自定义销毁方式：<ul>
<li>通过XML方式：<bean destroy="自定义的destroy方式名" ...=""></bean></li>
<li>通过Java注解：@Bean(destroy=”自定义的destroy方式名”)</li>
<li>通过编程方式实现：使用AbstractBeanDefination#setDestroyMethodNames(String) 实现，方法参数为自定义的初始化方法名</li>
</ul>
</li>
</ul>
<p>上述三种方式的优先级：</p>
<pre><code>@PreDestroy注解 &gt; DisposableBean接口 &gt; 自定义销毁方式
</code></pre><p>我们编写代码验证一下这三种方式：</p>
<h3 id="通过-PreDestroy注解销毁Bean"><a href="#通过-PreDestroy注解销毁Bean" class="headerlink" title="通过@PreDestroy注解销毁Bean"></a>通过@PreDestroy注解销毁Bean</h3><p>在上文中的BeanInitDemo类中，添加方法preDestory()并添加注解@PreDestroy</p>
<pre><code>@PreDestroy
public void preDestory() {
    System.out.println(&quot;@PreDestroy方式销毁&quot;);
}
</code></pre><p>此时preDestory()就是一个销毁方法</p>
<h3 id="实现接口DisposableBean接口"><a href="#实现接口DisposableBean接口" class="headerlink" title="实现接口DisposableBean接口"></a>实现接口DisposableBean接口</h3><p>我们让BeanInitDemo实现接口DisposableBean，并实现回调方法destroy()</p>
<pre><code>public class BeanInitDemo implements InitializingBean, DisposableBean {

    private String name;

    public BeanInitDemo(String name) {
        this.name = name;
    }

    ......

    @PreDestroy
    public void preDestory() {
        System.out.println(&quot;@PreDestroy方式销毁&quot;);
    }

    @Override
    public void destroy() throws Exception {
        System.out.println(&quot;DisposableBean方式销毁&quot;);
    }

}
</code></pre><p>BeanInitDemo此时就是一个DisposableBean实例，它会在Context关闭时回调destroy()从而实现对资源的回收操作。</p>
<h3 id="自定义销毁方式"><a href="#自定义销毁方式" class="headerlink" title="自定义销毁方式"></a>自定义销毁方式</h3><p>通过bean定义时显式指定销毁方法，实现自定义销毁。</p>
<p>我们在上面的BeanInitDemo类上添加自定义方法：destroyMethod()，BeanInitDemo类现在如下所示</p>
<pre><code>public class BeanInitDemo implements InitializingBean, DisposableBean {

    private String name;

    public BeanInitDemo(String name) {
        this.name = name;
    }

    ......

    @PreDestroy
    public void preDestory() {
        System.out.println(&quot;@PreDestroy方式销毁&quot;);
    }

    @Override
    public void destroy() throws Exception {
        System.out.println(&quot;DisposableBean方式销毁&quot;);
    }

    public void destroyMethod() {
        System.out.println(&quot;自定义destroy方式&quot;);
    }
}
</code></pre><p>修改配置类BeanInitConfig，在BeanInitDemo装载方法上，添加属性destroyMethod</p>
<pre><code>@Bean(initMethod = &quot;initMethod&quot;, destroyMethod = &quot;destroyMethod&quot;)
public BeanInitDemo beanInitDemo() {
    return new BeanInitDemo(&quot;BeanInitDemo&quot;);
}
</code></pre><p>通过destroyMethod显式地指定自定义bean销毁方法：<strong>destroyMethod</strong>，注意没有括号。</p>
<h3 id="测试三种销毁方式的优先级"><a href="#测试三种销毁方式的优先级" class="headerlink" title="测试三种销毁方式的优先级"></a>测试三种销毁方式的优先级</h3><p>运行测试类，观察控制台输出：</p>
<pre><code>@PreDestroy方式销毁
DisposableBean方式销毁
自定义destroy方式
</code></pre><p>可以看到三种销毁方式的优先级确实满足如下顺序：</p>
<pre><code>@PreDestroy注解 &gt; DisposableBean接口 &gt; 自定义销毁方式
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>到此，我们对Spring框架中涉及到Bean的初始化及销毁的主要方式进行了学习总结，并对每种方式的优先级进行了测试。</p>
<p>在后续的开发中，如果涉及到对Bean进行初始化或者容器销毁时需要对Bean进行一些收尾工作，就可以使用本文中提到的方式进行操作。</p>
<p>在下文中，我们将对这些方式具体的源码实现进行探究。最后祝各位读者，鼠年大吉，工作顺利。</p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spring框架以Bean为核心，为开发者提供了一系列的特性和能力。其中最为核心的就是IOC特性，而IOC中又以Bean定义及其初始化/销毁为核心。&lt;/p&gt;
&lt;p&gt;本文我们对Bean的初始化和销毁进行较为深入的整理与学习。&lt;/p&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>跟我学Spring之@Conditional注解</title>
    <link href="http://wuwenliang.net/2020/01/13/%E8%B7%9F%E6%88%91%E5%AD%A6Spring%E4%B9%8B-Conditional%E6%B3%A8%E8%A7%A3/"/>
    <id>http://wuwenliang.net/2020/01/13/跟我学Spring之-Conditional注解/</id>
    <published>2020-01-13T05:43:01.000Z</published>
    <updated>2020-01-13T05:43:26.430Z</updated>
    
    <content type="html"><![CDATA[<p>在Spring项目中，我们希望bean的注入不是必须的，而是依赖条件的。</p>
<p>只有当项目中引入特定依赖库、或者只有当某个bean被创建、或者设置了某个环境变量时，才会创建这个bean。</p>
<p>在Spring4之前，这种条件注入的方式还不支持，在Spring4之后引入了一个新的注解 <strong>@Conditional</strong> ，这个注解作用在@Bean注解修饰的方式上。它能够通过判断指定条件是否满足来决定是否创建这样的Bean。</p>
<p>使用@Conditional注解需要满足一定条件：</p>
<blockquote>
<p>@Conditional注解的类要实现Condition接口，它提供了一个matches()方法。只有matches()方法返回true时， 则被@Conditional注解修饰的bean就会被创建出来，否则不会创建（即matches方法返回false）。</p>
</blockquote>
<p>接下来，我们对@Conditionl注解进行深入探讨。</p>
<a id="more"></a>
<h2 id="Conditional分析"><a href="#Conditional分析" class="headerlink" title="@Conditional分析"></a>@Conditional分析</h2><p>@Conditional是Spring4提供的新注解，源码如下：</p>
<pre><code>@Target({ElementType.TYPE, ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
@Documented
public @interface Conditional {

    /**
    * All {@link Condition Conditions} that must {@linkplain Condition#matches match}
    * in order for the component to be registered.
    */
    Class&lt;? extends Condition&gt;[] value();

}
</code></pre><p>可以看出，注解被用于标注类和方法，在运行时生效。</p>
<p>通过value()方法可以看出，它要求传入一个Class数组，并且需要继承Condition接口。</p>
<p>我们接着看下Condition接口源码。</p>
<h2 id="Condition接口"><a href="#Condition接口" class="headerlink" title="Condition接口"></a>Condition接口</h2><pre><code>@FunctionalInterface
public interface Condition {
    boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata);
}
</code></pre><p>业务类需要实现matches方法，返回true则对@Conditional标注的bean进行注入，否则不注入。这就是所谓的条件注入。</p>
<p>这里注意，matches() 方法参数 ConditionContext 为 Condition设计的接口类，调用者能够从中获取到Spring容器的以下信息：</p>
<pre><code>//获取bean定义的注册类
BeanDefinitionRegistry getRegistry();

// 获取ioc使用的beanFactory
@Nullable
ConfigurableListableBeanFactory getBeanFactory();

//获取当前环境信息
Environment getEnvironment();

//获取当前使用的资源加载器
ResourceLoader getResourceLoader();

//获取类加载器
@Nullable
ClassLoader getClassLoader();
</code></pre><p>我们写一个demo来对@Conditional进行更为直观的展示。</p>
<h2 id="样例展示"><a href="#样例展示" class="headerlink" title="样例展示"></a>样例展示</h2><blockquote>
<p>首先定一个Bean，作为条件注入的目标对象。当注解生效则注入该bean，否则不予注入。</p>
</blockquote>
<pre><code>public class Computer {

    public Computer(String name, Double price) {
        this.name = name;
        this.price = price;
    }

    private String name;
    private Double price;

    ...省略getter setter...
}
</code></pre><p>我们定义一个电脑pojo类。</p>
<blockquote>
<p>接着创建一个BeanConfig类，注入两个Computer实例，作为测试的基准。</p>
</blockquote>
<pre><code>@Configuration
public class BeanConfig {

    @Bean(name = &quot;msi&quot;)
    public Computer computer1(){
        return new Computer(&quot;MSI&quot;,7000.00);
    }

    @Bean(name = &quot;dell&quot;)
    public Computer computer2(){
        return new Computer(&quot;dell&quot;,5000.00);
    }
}
</code></pre><p>这里我们创建了两个Computer的实例（微星、戴尔），并为其设置名称与价格。</p>
<blockquote>
<p>测试一下是否成功注入了bean。</p>
</blockquote>
<pre><code>public static void main(String[] args) {
    ConfigurableApplicationContext applicationContext = 
        SpringApplication.run(DemoSnowalkerApplication.class, args);
    Computer msi = (Computer) applicationContext.getBeanFactory().getBean(&quot;msi&quot;);
    Computer dell = (Computer) applicationContext.getBeanFactory().getBean(&quot;dell&quot;);
    System.out.println(msi);
    System.out.println(dell);
}
</code></pre><p>demo工程使用spring2.2.1进行构建，我们尝试在main方法中通过bName的方式获取两个注入的bean。运行结果如下：</p>
<pre><code>Computer{name=&apos;MSI&apos;, price=7000.0}
Computer{name=&apos;dell&apos;, price=5000.0}
</code></pre><p>可以看到到目前为止bean是成功注入的，这种方式为静态注入。</p>
<p><strong>接着我们就编写代码实现条件注入。</strong></p>
<p>首先我们定义一个场景，在不同的环境下，注入不同的Computer实例，如：dev环境下注入msi（微星），prod下注入dell（戴尔），该如何实现呢？</p>
<p>我们的思路是根据环境变量中设置的env参数的不同，选择不同的bean进行注入，即：</p>
<pre><code>env=dev,  注入msi实例
env=prod，注入dell实例
</code></pre><p>这里就需要请@Conditional一显身手了。首先我们需要实现Condition接口。</p>
<h3 id="实现Condition接口"><a href="#实现Condition接口" class="headerlink" title="实现Condition接口"></a>实现Condition接口</h3><p>这里需要分别实现dev、prod下的两个condition实现类。</p>
<h4 id="DevCondition（开发环境Condition实现）"><a href="#DevCondition（开发环境Condition实现）" class="headerlink" title="DevCondition（开发环境Condition实现）"></a>DevCondition（开发环境Condition实现）</h4><pre><code>public class DevCondition implements Condition {

    @Override
    public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) {
        //获取ioc使用的beanFactory
        ConfigurableListableBeanFactory beanFactory = conditionContext.getBeanFactory();
        //获取类加载器
        ClassLoader classLoader = conditionContext.getClassLoader();
        //获取当前环境信息
        Environment environment = conditionContext.getEnvironment();
        //获取bean定义的注册类
        BeanDefinitionRegistry registry = conditionContext.getRegistry();

        // 获取环境变量
        String env = environment.getProperty(&quot;env&quot;);
        if (&quot;dev&quot;.equalsIgnoreCase(env)) {
            return true;
        }
        return false;
    }
}
</code></pre><h4 id="ProdCondition（生产环境Condition实现）"><a href="#ProdCondition（生产环境Condition实现）" class="headerlink" title="ProdCondition（生产环境Condition实现）"></a>ProdCondition（生产环境Condition实现）</h4><pre><code>public class ProdCondition implements Condition {

    @Override
    public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) {

        Environment environment = conditionContext.getEnvironment();
        String env = environment.getProperty(&quot;env&quot;);
        if (&quot;prod&quot;.equalsIgnoreCase(env)) {
            return true;
        }
        return false;
    }
}
</code></pre><p>我们在上文中已经对matches方法的两个参数的含义进行了解释。这里需要注意的是，matches方法参数中的conditionContext提供了多个方法，方便获取Bean的各种信息。</p>
<p>这些方法也是SpringBoot中派生注解@ConditonalOnXX的基础。</p>
<blockquote>
<p>我们接下来就使用这两个Condition的实现类对上面的例子进行修改。</p>
</blockquote>
<p>修改BeanConfig，为msi标注DevCondition，为dell标注ProdCondition。并为启动类配置env环境变量，笔者使用的是IDEA开发环境，因此在Run-&gt;Edit runconfigurations中编辑环境变量即可。</p>
<p>首先设置env=dev，修改启动类测试代码如下：</p>
<pre><code>public static void main(String[] args) {
    ConfigurableApplicationContext applicationContext = 
        SpringApplication.run(DemoSnowalkerApplication.class, args);
    Map&lt;String, Computer&gt; computers = applicationContext.getBeansOfType(Computer.class);
    System.out.println(computers);
}
</code></pre><p>我们尝试加载Computer所有的实例，并进行打印。运行结果如下：</p>
<pre><code>{msi=Computer{name=&apos;MSI&apos;, price=7000.0}}
</code></pre><p>只有msi实例加载，这符合我们的预期。</p>
<h4 id="注入Condition实例的数组"><a href="#注入Condition实例的数组" class="headerlink" title="注入Condition实例的数组"></a>注入Condition实例的数组</h4><blockquote>
<p>我们注意到，@Conditional注解传入的是一个继承了Condition接口的Class数组。也就是说，我们完全可以在@Conditional注解的values中设置一个数组，传入多个Condition实现类，确保在所有的条件都满足才进行bean注入。</p>
</blockquote>
<p>编写一个新的Condition实现类，matches方法返回true：</p>
<pre><code>public class DefaultCondition implements Condition {
    @Override
    public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) {
        return true;
    }
}
</code></pre><p>修改BeanConfig类，msi这个bean的@Conditional注解中增加DefaultCondition。</p>
<pre><code>@Bean(name = &quot;msi&quot;)
@Conditional({DevCondition.class, DefaultCondition.class})
public Computer computer1(){
    return new Computer(&quot;MSI&quot;,7000.00);
}
</code></pre><p>再次运行测试方法，返回如下：</p>
<pre><code>{msi=Computer{name=&apos;MSI&apos;, price=7000.0}}
</code></pre><p>可以看到，当多个condition返回均为true时，bean被注入了。我们修改DefaultCondition的matches方法返回false,再次运行测试方法，返回结果：</p>
<pre><code>{}
</code></pre><p>可以看到，当有一个Condition返回false，则bean就不会被注入。这有点像逻辑运算下的“逻辑与”。这种方式支持我们在复杂条件下对bean进行注入的要求。</p>
<p>ps: @Conditional注解在方法上，只能注入一个实例；如果注解在类上，则当前类下的所有bean实例都能够被注入。这里就不进行测试了，感兴趣的同学可以自行尝试。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文我们主要了解了@Conditional注解的原理及其使用方法，并且知道了该注解是Spring Boot条件注入的基础。</p>
<p>在后续开发中如果遇到需要根据某个条件来决定Bean注入的场景，我们首先就应该想到Spring为我们提供的@Conditional注解，并且能够准确的加以应用。</p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Spring项目中，我们希望bean的注入不是必须的，而是依赖条件的。&lt;/p&gt;
&lt;p&gt;只有当项目中引入特定依赖库、或者只有当某个bean被创建、或者设置了某个环境变量时，才会创建这个bean。&lt;/p&gt;
&lt;p&gt;在Spring4之前，这种条件注入的方式还不支持，在Spring4之后引入了一个新的注解 &lt;strong&gt;@Conditional&lt;/strong&gt; ，这个注解作用在@Bean注解修饰的方式上。它能够通过判断指定条件是否满足来决定是否创建这样的Bean。&lt;/p&gt;
&lt;p&gt;使用@Conditional注解需要满足一定条件：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;@Conditional注解的类要实现Condition接口，它提供了一个matches()方法。只有matches()方法返回true时， 则被@Conditional注解修饰的bean就会被创建出来，否则不会创建（即matches方法返回false）。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下来，我们对@Conditionl注解进行深入探讨。&lt;/p&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>初心，改变--致2019</title>
    <link href="http://wuwenliang.net/2019/12/31/%E5%88%9D%E5%BF%83%EF%BC%8C%E6%94%B9%E5%8F%98-%E8%87%B42019/"/>
    <id>http://wuwenliang.net/2019/12/31/初心，改变-致2019/</id>
    <published>2019-12-31T03:58:34.000Z</published>
    <updated>2019-12-31T16:17:29.254Z</updated>
    
    <content type="html"><![CDATA[<p>2019，岁末。</p>
<p>迟迟没有动笔，不知从何讲起。</p>
<p>2019，变化的一年，踏实的一年，平静的一年。</p>
<p>变化，在于工作。原计划2020年六月的行动，提前半年实施。</p>
<p>坐在新的办公桌前，我敲下这段文字，生活就是这样，一个个选择造就了现在的自己。关于工作更迭，不再多言，已经在之前的文章中详细写过。</p>
<a id="more"></a>
<p>踏实，在于爱情终成正果。感谢她，曾经的女友，现在的妻子，一直给我支持与鼓励，耐心倾听，默默陪伴，共享欢乐，分担忧愁。因为有你所以做事有了方向，你就是人间的四月天。</p>
<p>1016，一个平常日子里，我们结婚了。</p>
<p>于是今后的日子里，有你为伴。</p>
<p>感恩，并幸福地前行着。</p>
<p>平静，在于心态。九月，母亲生病，手术，所幸一切安好。</p>
<p>当时得知情况的自己就像挨了一记闷棍，惊醒了，于是知道身边人是最为重要的。</p>
<p>年中讨论热烈的 “996-ICU”，我亲历，明白身体真是的本钱。</p>
<p>之所以平静，是因为经历了这些，原本以为自己明白的普通道理，变得如此真切。</p>
<p>平静，倒不如说成长。</p>
<p>成长，总是伴随着阵痛，注定不会太轻松。</p>
<p>说的纷乱，思绪万千。</p>
<p>工作里，挑战接踵而至，已经不会再像初入职场那般惊慌。</p>
<p>有问题，解决问题，不多争辩，实事求是，多找找主观上的问题，少给自己开脱。适当的，自嘲一下，笑过之后继续做事。</p>
<p>有争执，放下争执，求同存异，事后再行复盘。</p>
<p>对技术，初心不灭。一年里，专注于消息队列技术，不断深挖，分享文章，参与社区，结识了一帮牛人，也因此受益良多。</p>
<p>心态已经趋于平常，虽热量依旧，但锐气已散。我欣然，渐进而立，是需要看开一些事，看重一些事。</p>
<p>就这么多吧，已过了洋洋洒洒写下数千字的年纪，朴实话语也能抵得过锦绣文字。</p>
<p>惟精惟一，始终谨记。</p>
<p>继续吧，12.31只是一个符号。</p>
<p>继续吧，未知的前方让我向往。</p>
<p>继续吧，平静地迎接新年的到来。</p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2019，岁末。&lt;/p&gt;
&lt;p&gt;迟迟没有动笔，不知从何讲起。&lt;/p&gt;
&lt;p&gt;2019，变化的一年，踏实的一年，平静的一年。&lt;/p&gt;
&lt;p&gt;变化，在于工作。原计划2020年六月的行动，提前半年实施。&lt;/p&gt;
&lt;p&gt;坐在新的办公桌前，我敲下这段文字，生活就是这样，一个个选择造就了现在的自己。关于工作更迭，不再多言，已经在之前的文章中详细写过。&lt;/p&gt;
    
    </summary>
    
      <category term="年度总结" scheme="http://wuwenliang.net/categories/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="年度总结" scheme="http://wuwenliang.net/tags/%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot之整合邮件发送功能</title>
    <link href="http://wuwenliang.net/2019/12/11/SpringBoot%E4%B9%8B%E6%95%B4%E5%90%88%E9%82%AE%E4%BB%B6%E5%8F%91%E9%80%81%E5%8A%9F%E8%83%BD/"/>
    <id>http://wuwenliang.net/2019/12/11/SpringBoot之整合邮件发送功能/</id>
    <published>2019-12-11T14:49:19.000Z</published>
    <updated>2019-12-11T14:50:44.244Z</updated>
    
    <content type="html"><![CDATA[<p>我们在开发中通常会使用邮件方式进行告警，传统的邮件发送整合起来较为繁琐，因此Spring Boot提供了一套更为简洁易用的整合方案，对Java Mail进行了封装，能够让业务更快的具备邮件发送能力。</p>
<p>本文主要讲解如何为Spring Boot应用添加邮件发送能力。</p>
<h2 id="依赖引入"><a href="#依赖引入" class="headerlink" title="依赖引入"></a>依赖引入</h2><p>首先还是要有一个Spring Boot应用，这个就不再赘述了。在maven中央仓库搜索Spring Boot邮件发送模块，将坐标添加到项目的pom下。（这里以2.2.1RELEASE举例）</p>
<pre><code>&lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-mail --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;
    &lt;version&gt;2.2.1.RELEASE&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><a id="more"></a>
<h2 id="进行配置"><a href="#进行配置" class="headerlink" title="进行配置"></a>进行配置</h2><p>Spring Boot强调约定优于配置，因此我们需要进行必要的配置。在application.properties中添加以下配置：（这里以qq邮箱举例）</p>
<pre><code>########################################################################
#
#     邮件配置
#
#########################################################################
# 邮件发送smtp服务域名
spring.mail.host=smtp.qq.com
# 发送账号
spring.mail.username=你的qq邮箱
# 发送授权码
spring.mail.password=发送授权码
# 邮件编码格式
spring.mail.default-encoding=UTF-8
# 是否进行认证
spring.mail.properties.mail.smtp.auth=true
spring.mail.properties.mail.smtp.starttls.enable=true
spring.mail.properties.mail.smtp.starttls.required=true
# smtp服务端口
spring.mail.port=465
spring.mail.properties.mail.smtp.socketFactory.port = 465
spring.mail.properties.mail.smtp.socketFactory.class = javax.net.ssl.SSLSocketFactory
spring.mail.properties.mail.debug=true
spring.mail.properties.mail.smtp.socketFactory.fallback = false
</code></pre><blockquote>
<p>smtp服务域名在linux下可能会有无法解析的问题，只需要将域名更换为ip地址即可。具体的做法是在命令行中通过ping命令来获取并替换即可。</p>
</blockquote>
<p>使用这个配置前需要在自己的qq邮箱中开通smtp服务，具体的步骤如下：</p>
<ol>
<li>首先登陆qq邮箱，点击顶部的导航栏的 <strong>“设置”</strong> 按钮</li>
<li>进入设置选项页面，在账户选项卡下方找到 <strong>POP3/SMTP</strong> 服务，单机后方的“开启”按钮，将功能开放。</li>
</ol>
<blockquote>
<p>在开启 <strong>POP3/SMTP</strong> 服务的过程中，需要根据引导发送短信，当操作完成之后，我们会获取到一个发送邮件授权码，这个授权码需要保存下来，以便在应用中进行配置（配置项为 <strong>spring.mail.password</strong>）</p>
</blockquote>
<ol>
<li>当获取到授权码之后，发送前的准备工作就基本完成。</li>
</ol>
<h2 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h2><p>基础配置完成，我们就可以编写代码进行邮件发送的测试。</p>
<p>在项目中编写一个邮件发送类，并标记为一个Spring的Bean。</p>
<pre><code>@Component
public class MailSenderClient {

    private static final Logger LOGGER = LoggerFactory.getLogger(MailSender.class);

    @Autowired
    JavaMailSender javaMailSender;

    public void sendSimpleMail(MailEntity mailEntity) {
        // 组装邮件发送实体
        SimpleMailMessage simpleMailMessage = new SimpleMailMessage();
        simpleMailMessage.setFrom(mailEntity.getFrom());
        simpleMailMessage.setTo(mailEntity.getTo());
        simpleMailMessage.setCc(mailEntity.getCc());
        simpleMailMessage.setSubject(mailEntity.getSubject());
        simpleMailMessage.setText(mailEntity.getContent());
        javaMailSender.send(simpleMailMessage);
        LOGGER.info(&quot;邮件发送完成: mailEntity={}&quot;, JSON.toJSONString(mailEntity));
    }

    /**
     * 邮件发送实体
     */
    public static class MailEntity {
        // 发件人
        private String from;
        // 收件人
        private String to;
        // 抄送
        private String cc;
        // 邮件主题（标题）
        private String subject;
        // 邮件正文，如果正文是HTML则需要手动拼接
        private String content;

        ...省略getter setter...
    }
}
</code></pre><p>核心逻辑很简单，我们要做的就是将JavaMailSender注入到发送邮件的bean中，构造一个SimpleMailMessage，设置发件人、收件人、抄送者（为空表示不抄送）、邮件主题、邮件正文，通过 <strong>javaMailSender.send(simpleMailMessage);</strong> 方法将邮件发送出去即可。</p>
<p>具体的调用方式如下，我们只需要通过这种方式，在需要进行邮件发送的地方如此调用即可。</p>
<pre><code>@Test
public void testSendMail() {
    MailSenderClient.MailEntity mailEntity = new MailSenderClient.MailEntity();
    mailEntity.setFrom(&quot;121xxxx591@qq.com&quot;)
            .setTo(&quot;xxxx@xxxx.com&quot;)
            .setCc(&quot;122xxxx121@xxxxxx.com&quot;)
            .setSubject(&quot;文件内容为空，请关注!&quot;)
            .setContent(&quot;截止到当前时间，分析结果文本中内容为空，请关注!&quot;);
    mailSender.sendSimpleMail(mailEntity);
}
</code></pre><p>运行该测试用例，查看控制台日志（由于在配置中设置了开启debug，因此能够看到详细的握手报文）。</p>
<pre><code>......
DEBUG SMTP: connected to host &quot;smtp.qq.com&quot;, port: 465
EHLO 10.3.4.197
250-smtp.qq.com
250-PIPELINING
250-SIZE 73400320
250-AUTH LOGIN PLAIN
250-AUTH=LOGIN
250-MAILCOMPRESS
250 8BITMIME
......
DEBUG SMTP: STARTTLS requested but already using SSL
DEBUG SMTP: protocolConnect login, host=smtp.qq.com, user=xxxxx@qq.com, password=&lt;non-null&gt;
......
DEBUG SMTP: Using mechanism LOGIN
DEBUG SMTP: AUTH LOGIN command trace suppressed
DEBUG SMTP: AUTH LOGIN succeeded
DEBUG SMTP: use8bit false
MAIL FROM:&lt;xxxxx@qq.com&gt;
250 Ok
RCPT TO:&lt;xxxxx@xxxx.com&gt;
250 Ok
RCPT TO:&lt;xxxxxxxx@xxxxxx.com&gt;
250 Ok
DEBUG SMTP: Verified Addresses
......
DATA
354 End data with &lt;CR&gt;&lt;LF&gt;.&lt;CR&gt;&lt;LF&gt;
Date: Wed, 11 Dec 2019 11:11:08 +0800 (CST)
From: xxxxxx@qq.com
To: xxxxxxx@xxxx.com
Cc: xxxxxx@xxxxx.com
Message-ID: &lt;1117448897.0.1576033868771@[10.3.4.197]&gt;
Subject: =?UTF-8?B?5o6o6I2Q5rGg5paH5Lu25YaF5a655Li656m677yM6K+35YWz5rOoIQ==?=
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: base64
......
.
250 Ok: queued as 
DEBUG SMTP: message successfully delivered to mail server
QUIT
221 Bye
......
</code></pre><p>这就表明邮件发送成功，我们要做的就是打开邮箱进行查看即可。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>到此我们就完成了Spring Boot对邮件发送功能的整合。需要补充的是，很多情况下，我们发送的邮件正文是具备一定格式的，</p>
<p>本文讲解的案例则是简单的文本格式邮件正文，实际上，文章中讲解的方式也是能够支持HTML内容发送的，我们要做的就是在</p>
<p>邮件发送的正文中，手动的拼接html文档，只要能够保证html文档拼接的正确性，邮件发送成功后，在邮箱客户端我们就能看到</p>
<p>渲染后的样式。虽然有一定的工作量，不过能够解决问题就是我们的直接目的。</p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在开发中通常会使用邮件方式进行告警，传统的邮件发送整合起来较为繁琐，因此Spring Boot提供了一套更为简洁易用的整合方案，对Java Mail进行了封装，能够让业务更快的具备邮件发送能力。&lt;/p&gt;
&lt;p&gt;本文主要讲解如何为Spring Boot应用添加邮件发送能力。&lt;/p&gt;
&lt;h2 id=&quot;依赖引入&quot;&gt;&lt;a href=&quot;#依赖引入&quot; class=&quot;headerlink&quot; title=&quot;依赖引入&quot;&gt;&lt;/a&gt;依赖引入&lt;/h2&gt;&lt;p&gt;首先还是要有一个Spring Boot应用，这个就不再赘述了。在maven中央仓库搜索Spring Boot邮件发送模块，将坐标添加到项目的pom下。（这里以2.2.1RELEASE举例）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-mail --&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-boot-starter-mail&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;2.2.1.RELEASE&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot之整合MongoDB</title>
    <link href="http://wuwenliang.net/2019/12/06/SpringBoot%E4%B9%8B%E6%95%B4%E5%90%88MongoDB/"/>
    <id>http://wuwenliang.net/2019/12/06/SpringBoot之整合MongoDB/</id>
    <published>2019-12-06T01:20:29.000Z</published>
    <updated>2020-01-14T02:28:53.222Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。 在高负载的情况下，添加更多的节点，可以保证服务器性能。 MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。 MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 – 引用自runnoob</p>
</blockquote>
<p>一个MongoDB的BSON数据结构如下：</p>
<p><img src="/2019/12/06/SpringBoot之整合MongoDB/./mongo-document.png" alt="BSON"></p>
<h2 id="引入spring-boot-starter-data-mongodb依赖"><a href="#引入spring-boot-starter-data-mongodb依赖" class="headerlink" title="引入spring-boot-starter-data-mongodb依赖"></a>引入spring-boot-starter-data-mongodb依赖</h2><p>首先当然需要建立一个SpringBoot项目，接着在项目的pom.xml中引入spring-boot-starter-data-mongodb。坐标如下：</p>
<pre><code>&lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-mongodb --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-data-mongodb&lt;/artifactId&gt;
    &lt;version&gt;2.2.1.RELEASE&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><a id="more"></a>
<h2 id="配置mongodb"><a href="#配置mongodb" class="headerlink" title="配置mongodb"></a>配置mongodb</h2><p>在项目的application.properties中添加Mongodb连接配置</p>
<pre><code>###################################################################
#                           MongoDB
###################################################################
# 进行认证的db，一般就是admin
spring.data.mongodb.authentication-database=admin
# 需要进行交互的db，需要实现创建
spring.data.mongodb.database=snowalker
# mongodb所在机器的ip/域名
spring.data.mongodb.host=127.0.0.1
# mongodb对外提供服务的端口，默认为27017
spring.data.mongodb.port=27017
# 用户名及密码，可主动在admin库中添加并授权
spring.data.mongodb.username=root
spring.data.mongodb.password=root
</code></pre><h2 id="编写实体类"><a href="#编写实体类" class="headerlink" title="编写实体类"></a>编写实体类</h2><p>需要编写一个实体类，该实体类就是一个POJO。这里我们以书籍信息为例</p>
<pre><code>public class BookInfo {

    private String id;
    private String name;
    private String author;

    // 省略getter setter
</code></pre><h2 id="编写Mongodb操作持久层"><a href="#编写Mongodb操作持久层" class="headerlink" title="编写Mongodb操作持久层"></a>编写Mongodb操作持久层</h2><p>这一步较为核心，我们通过编写一个MongoDB的交互接口，实现对MongoDB的操作。这一切只需要让自定义的接口继承MongoRepository接口即可，整体的风格类似于JPA。</p>
<pre><code>public interface BookInfoRepository extends MongoRepository&lt;BookInfo, String&gt; {
    List&lt;BookInfo&gt; findByAuthorContains(String author);
    BookInfo findByNameEquals(String bookName);
}
</code></pre><p>我们的接口一旦继承了MongoRepository就能够使用最基本的CURD等操作，而且我们可以根据自己的需求进行操作的扩展。</p>
<h3 id="扩展：SpringData方法扩展"><a href="#扩展：SpringData方法扩展" class="headerlink" title="扩展：SpringData方法扩展"></a>扩展：SpringData方法扩展</h3><p>这里介绍一下如何进行方法扩展。</p>
<p>实际上，在SpringData中，只要方法的定义符合既定规范，则Spring Data就可以分析出开发者的意图，从而避免开发者自定义一些诸如SQL、指令等操作。当然这就仁者见仁智者见智了，笔者认为，对于简单的操作，用这种方法可以很好的提升开发效率，但如果需要针对业务有一些复杂的操作，则自定义SQL、指令应当更加灵活。</p>
<blockquote>
<p>常见的命名规则如下：</p>
</blockquote>
<p>如果是简单条件查询的场景，如：需要对某一个实体类或者集合进行查询。</p>
<p>按照Spring Data的规范的规定，查询方法应当以 <strong>find | read | get开头</strong>（如：find、findBy、read、readBy、get、getBy）。</p>
<p>涉及到查询条件时，条件属性需要用条件关键字进行连接。这里需要注意：条件属性首字母应当大写。当框架进行方法名解析时，会先把方法名的多余前缀截取掉，然后对剩下部分进行解析。</p>
<p>对于直接在接口中定义的查询方法，如果符合规范的，则可以不用写实现，目前支持的关键字写法如下：</p>
<p><img src="/2019/12/06/SpringBoot之整合MongoDB/spring-data-method.png" alt="SpringData方法命名规范"></p>
<h2 id="编写测试接口"><a href="#编写测试接口" class="headerlink" title="编写测试接口"></a>编写测试接口</h2><p>到此我们就基本上完成了对MongoDB的整合，接下来就编写一个测试接口对MongoDB进行操作，验证整合是否有效。</p>
<pre><code>@RestController
public class BookInfoController {

    @Autowired
    BookInfoRepository bookInfoRepository;

    @GetMapping(value = &quot;mongo&quot;)
    public BookInfo testMongo() throws JsonProcessingException {

        List&lt;BookInfo&gt; bookInfos = new ArrayList&lt;&gt;();
        // 添加数据
        for (int i = 0; i &lt; 3; i++) {
            String uuid = UUID.randomUUID().toString();
            BookInfo bookInfo = new BookInfo();
            bookInfo.setId(i + &quot;_&quot; + uuid).setAuthor(&quot;刘慈欣&quot;).setName(&quot;三体_&quot; + uuid);
            bookInfos.add(bookInfo);
        }
        // 入Mongo
        bookInfoRepository.insert(bookInfos);
        // 查询By author
        List&lt;BookInfo&gt; queryList = bookInfoRepository.findByAuthorContains(&quot;刘慈欣&quot;);
        ObjectMapper objectMapper = new ObjectMapper();
        String s = objectMapper.writeValueAsString(queryList);
        System.out.println(s);
        // 查询by Name
        BookInfo bookInfo =  bookInfoRepository.findByNameEquals(&quot;三体_89e9a1e3-6597-4415-9699-b5bfb33632ae&quot;);
        if (bookInfo == null) {
            return new BookInfo();
        }
        return bookInfo;
    }
}
</code></pre><p>对上述的测试方法testMongo()稍作总结：</p>
<ol>
<li>我们定义了一个List，用于暂存待持久化的书籍信息</li>
<li>构造三个BookInfo对象并添加到List中</li>
<li>通过调用bookInfoRepository.insert(bookInfos);将list持久化到mongoDB中，这个insert方法就是MongoRepository为我们提供的，我们直接调用即可</li>
<li>接着我们通过自定义的查询方法对前面持久化的数据进行查询。</li>
</ol>
<p>启动项目，访问测试接口： <a href="http://localhost:10880/mongo，返回如下json串：" target="_blank" rel="external">http://localhost:10880/mongo，返回如下json串：</a></p>
<pre><code>{&quot;id&quot;:&quot;2_89e9a1e3-6597-4415-9699-b5bfb33632ae&quot;,&quot;name&quot;:&quot;三体_89e9a1e3-6597-4415-9699-b5bfb33632ae&quot;,&quot;author&quot;:&quot;刘慈欣&quot;}
</code></pre><p>这表明，我们对Mongodb的整合是ok的，并且成功实现了数据的插入和查询等操作。</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><blockquote>
<p>生产环节链接的mongodb多为集群方式，集群方式配置方式如下</p>
</blockquote>
<h3 id="数据源配置"><a href="#数据源配置" class="headerlink" title="数据源配置"></a>数据源配置</h3><p>如果mongodb端口是默认端口且没设置密码，可不配置密码</p>
<pre><code>spring.data.mongodb.uri=mongodb://localhost:27017/springboot-db
</code></pre><p>如果mongodb设置了密码，则链接这样配置：</p>
<pre><code>spring.data.mongodb.uri=mongodb://name:pass@localhost:27017/dbname
</code></pre><p>如果是多个ip集群，则如下配置即可：</p>
<pre><code>spring.data.mongodb.uri=mongodb://user:pass@ip1:port1,ip2:port2/database 
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文我们主要介绍了在SpringBoot中如何通过整合spring-boot-starter-data-mongodb实现对MongoDB的操作。</p>
<p>当我们对spring-data使用的更加熟练，就会感受到Spring对于开发者的友好，通过各种封装的组件实现了对不同底层数据源的统一封装，接口对开发者保持风格一致，降低了开发者的学习成本。这种封装思想也同样值得我们去学习借鉴。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://m.wang1314.com/doc/webapp/topic/20661591.html" target="_blank" rel="external">Spring boot中mongodb的使用-纯洁的微笑</a></p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。 在高负载的情况下，添加更多的节点，可以保证服务器性能。 MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。 MongoDB 将数据存储为一个文档，数据结构由键值(key=&amp;gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。 – 引用自runnoob&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一个MongoDB的BSON数据结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2019/12/06/SpringBoot之整合MongoDB/./mongo-document.png&quot; alt=&quot;BSON&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;引入spring-boot-starter-data-mongodb依赖&quot;&gt;&lt;a href=&quot;#引入spring-boot-starter-data-mongodb依赖&quot; class=&quot;headerlink&quot; title=&quot;引入spring-boot-starter-data-mongodb依赖&quot;&gt;&lt;/a&gt;引入spring-boot-starter-data-mongodb依赖&lt;/h2&gt;&lt;p&gt;首先当然需要建立一个SpringBoot项目，接着在项目的pom.xml中引入spring-boot-starter-data-mongodb。坐标如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-mongodb --&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-boot-starter-data-mongodb&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;2.2.1.RELEASE&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot动态定时任务开发指南</title>
    <link href="http://wuwenliang.net/2019/12/03/SpringBoot%E5%8A%A8%E6%80%81%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97/"/>
    <id>http://wuwenliang.net/2019/12/03/SpringBoot动态定时任务开发指南/</id>
    <published>2019-12-03T15:30:38.000Z</published>
    <updated>2019-12-03T15:31:12.028Z</updated>
    
    <content type="html"><![CDATA[<p>一般情况下，如果想在Spring Boot中使用定时任务，我们只需要@EnableScheduling开启定时任务支持，在需要调度的方法上添加@Scheduled注解。这样就能够在项目中开启定时调度功能了，并且这种方法支持通过cron表达式灵活的控制执行周期和频率。</p>
<p>上述的方式好处是快捷，轻量，缺点是周期一旦指定，想要更改必须要重启应用，如果我们想要动态的对定时任务的执行周期进行变更，甚至动态的增加定时调度任务则上述方式就不适用了。</p>
<p>本文我将讲解如何在Spring 定时任务的基础上进行扩展，实现动态定时任务。</p>
<a id="more"></a>
<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><ol>
<li>动态增加定时任务</li>
<li>热更新定时任务的执行周期（动态更新cron表达式）</li>
</ol>
<h2 id="方案1：仅实现动态变更任务周期"><a href="#方案1：仅实现动态变更任务周期" class="headerlink" title="方案1：仅实现动态变更任务周期"></a>方案1：仅实现动态变更任务周期</h2><blockquote>
<p>首先介绍的方案1能够实现动态变更已有任务的执行频率/周期。</p>
</blockquote>
<p>首先建立一个Spring Boot应用，这里不再展开。</p>
<p>建立一个任务调度类，实现接口SchedulingConfigurer，标记为Spring的一个bean。注意一定要添加注解 <strong>@EnableScheduling</strong> 开启定时任务支持。</p>
<pre><code>@EnableScheduling
@Component
public class DynamicCronHandler implements SchedulingConfigurer {

    private static final String DEFAULT_CRON = &quot;0/5 * * * * ?&quot;;
    private String taskCron = DEFAULT_CRON;

    @Override
    public void configureTasks(ScheduledTaskRegistrar scheduledTaskRegistrar) {
        scheduledTaskRegistrar.addTriggerTask(()-&gt;{
            LOGGER.info(&quot;执行任务&quot;);
        }, triggerContext -&gt; {
            // 刷新cron
            CronTrigger cronTrigger = new CronTrigger(taskCron);
            Date nextExecDate = cronTrigger.nextExecutionTime(triggerContext);
            return nextExecDate;
        });
    }
</code></pre><p>scheduledTaskRegistrar.addTriggerTask接受两个参数，分别为需要调度的任务实例（Runnable实例），Trigger实例，这里通过lambda方式注入，需要实现nextExecutionTime回调方法，返回下次执行时间。</p>
<p>通过该回调方法，在Runnable中执行业务逻辑代码，在Trigger修改定时任务的执行周期。</p>
<pre><code>    public DynamicCronHandler setTaskImplement(Runnable taskImplement) {
        this.taskImplement = taskImplement;
        return this;
    }

    public DynamicCronHandler setTaskCron(String taskCron) {
        this.taskCron = taskCron;
        return this;
    }

    public DynamicCronHandler taskCron(String taskCron) {
        System.out.println(&quot;更新cron=&quot; + taskCron);
        this.taskCron = taskCron;
        return this;
    }

    ...省略getter...

}
</code></pre><p>编写一个测试类，进行测试：</p>
<pre><code>@RequestMapping(&quot;execute&quot;)
@ResponseBody
public String executeTask(@RequestParam(value = &quot;cron&quot;, defaultValue = &quot;0/10 * * * * ?&quot;) String cron) {
    LOGGER.info(&quot;cron={}&quot;, cron);
    dynamicCronHandler.taskCron(cron);
    return &quot;success&quot;;
}
</code></pre><p>暴露一个http接口，接受参数cron，启动应用并访问/execute，首次传入参数cron=0/1 <em> </em> <em> </em> ?，表示每秒执行一次任务。日志如下：</p>
<pre><code>2019-12-03 15:32:40.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
2019-12-03 15:32:41.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
2019-12-03 15:32:42.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
2019-12-03 15:32:43.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
2019-12-03 15:32:44.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
</code></pre><p>可以看到每秒执行一次。</p>
<p>更改cron的值为0/5 <em> </em> <em> </em> ?，观察到控制台输出发生变化：</p>
<pre><code>更新cron=0/5 * * * * ?
2019-12-03 15:33:30.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
2019-12-03 15:33:35.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
2019-12-03 15:33:40.001  INFO 7196 --- [TaskScheduler-1] c.s.s.t.d.d.DynamicCronHandler           : 执行任务
</code></pre><p>此时定时任务执行频率更新为5秒一次，表明通过SchedulingConfigurer.configureTasks回调，动态的更新了定时任务执行频率。</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>到目前为止，实现了动态变更定时任务的执行频率，但是不能实现动态的提交定时任务。方案二就是为了解决这个疑问而实现的，</p>
<h2 id="方案二：动态提交定时任务并更新任务执行频率"><a href="#方案二：动态提交定时任务并更新任务执行频率" class="headerlink" title="方案二：动态提交定时任务并更新任务执行频率"></a>方案二：动态提交定时任务并更新任务执行频率</h2><p>首先建立一个DynamicTaskScheduler类，内容如下：</p>
<pre><code>@Scope(value = &quot;singleton&quot;)
@Component
@EnableScheduling
public class DynamicTaskScheduler {

    private ScheduledFuture&lt;?&gt; future;

    @Autowired
    private ThreadPoolTaskScheduler threadPoolTaskScheduler;

    @Bean
    public ThreadPoolTaskScheduler threadPoolTaskScheduler() {
        return new ThreadPoolTaskScheduler();
    }

    public void startCron(Runnable task, String cron) {
        stopCron();
        future = threadPoolTaskScheduler.schedule(
                task, new CronTrigger(cron)
        );
    }

    public void stopCron() {
        if (future != null) {
            future.cancel(true);
            System.out.println(&quot;stopCron()&quot;);
        }
    }
}
</code></pre><p>这里通过startCron提交一个新的任务，通过cron表达式进行调度，在开始之前进行判断是否关闭老的，必须关闭老的才能开启新的。</p>
<p>通过stopCron对老任务进行关闭。</p>
<p>编写一个测试方法测试该动态任务调度类。</p>
<pre><code>@RequestMapping(&quot;execute1&quot;)
@ResponseBody
public String executeTask1(@RequestParam(value = &quot;cron&quot;, defaultValue = &quot;0/10 * * * * ?&quot;) String cron) {
    LOGGER.info(&quot;cron={}&quot;, cron);
    dynamicTaskScheduler.startCron(
            () -&gt; {
                LOGGER.info(&quot;模拟执行作业,cron={}&quot;, cron);
            },
            cron
    );
    return &quot;success&quot;;
}
</code></pre><p>启动方法中初始化一个 ThreadPoolTaskScheduler 实例。</p>
<pre><code>@SpringBootApplication
public class SnowalkerTestDemoApplication {

    public static void main(String[] args) {
        SpringApplication.run(SnowalkerTestDemoApplication.class, args);
    }

    @Bean
    public ThreadPoolTaskScheduler threadPoolTaskScheduler() {
        ThreadPoolTaskScheduler executor = new ThreadPoolTaskScheduler();
        executor.setPoolSize(20);
        executor.setThreadNamePrefix(&quot;taskExecutor-&quot;);
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.setAwaitTerminationSeconds(60);
        return executor;
    }

}
</code></pre><p>运行启动类，访问测试接口/execute1，先传入cron=0/1 <em> </em> <em> </em> ?，表示每秒执行一次任务。日志如下：</p>
<p>更改cron的值为0/5 <em> </em> <em> </em> ?，观察到控制台输出发生变化：</p>
<p>可以看到这种方式同样实现了动态的变更定时任务执行频率，相比上述的方法，该方式更加灵活，能够动态的增加任务到线程池中进行调度，我们可以定义一个Map保存future，从而实现创建并维护多个定时任务，具体可以参考这篇文章 <a href="https://blog.csdn.net/qq_32711309/article/details/84944534" target="_blank" rel="external">ThreadPoolTaskScheduler的使用，定时任务开启与关闭</a> ，思路如下：</p>
<ol>
<li>自定义Task类，实现Runnable，定义属性name</li>
<li>定义一个ConcurrentHashMap,KEY=name,value=ScheduledFuture</li>
<li>通过 <strong>ScheduledFuture&lt;?&gt; schedule(Runnable task, Trigger trigger)</strong> 进行任务调度时，传入自定义Task，构造/setter 注入任务名称（全局唯一）, 并将该task实现类设置到步骤2的map中，key=name，value=当前通过schedule调度返回的ScheduledFuture</li>
<li>停止该任务时，通过name在map中找到ScheduledFuture实例，调用scheduledFuture.cancel(true);方法停止任务即可</li>
</ol>
<p>核心代码如下：</p>
<blockquote>
<p>任务存储Map</p>
</blockquote>
<pre><code>public static ConcurrentHashMap&lt;String, ScheduledFuture&gt; map = new ConcurrentHashMap&lt;&gt;();
</code></pre><blockquote>
<p>启动任务</p>
</blockquote>
<pre><code>@Component
@Scope(&quot;prototype&quot;)
public class DynamicTask {


    @Autowired
    private ThreadPoolTaskScheduler threadPoolTaskScheduler;
    private ScheduledFuture future;

    public void startCron() {
        cron = &quot;0/1 * * * * ?&quot;;
        System.out.println(Thread.currentThread().getName());
        String name = Thread.currentThread().getName();
        future = threadPoolTaskScheduler.schedule(new myTask(name), new CronTrigger(cron));
        App.map.put(name, future);
    }
</code></pre><blockquote>
<p>停止任务</p>
</blockquote>
<pre><code>    public void stop() {
        if (future != null) {
            future.cancel(true);
        }
    }
}
</code></pre><blockquote>
<p>自定义Task定义</p>
</blockquote>
<pre><code>public class MyTask implements Runnable {
    private String name;

    myTask(String name) {
        this.name = name;
    }

    @Override
    public void run() {
        System.out.println(&quot;test&quot; + name);
    }
}
</code></pre><blockquote>
<p>测试接口</p>
</blockquote>
<pre><code>@Autowired
private DynamicTask task;

@RequestMapping(&quot;/start&quot;)
public void test() throws Exception {
    // 开启定时任务，对象注解Scope是多利的。
    task.startCron();

}

@RequestMapping(&quot;/stop&quot;)
public void stop() throws Exception {
    // 提前测试用来测试线程1进行对比是否关闭。
    ScheduledFuture scheduledFuture = App.map.get(&quot;http-nio-8081-exec-2&quot;);
    scheduledFuture.cancel(true);
    // 查看任务是否在正常执行之前结束,正常返回true
    boolean cancelled = scheduledFuture.isCancelled();
    while (!cancelled) {
        scheduledFuture.cancel(true);
    }
}
</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>以上就是SpringBoot动态定时任务相关的讲解，这种方式在轻量级环境下能够很好的工作。如果我们的定时任务要求分布式，高可用，则需要引入额外的组件，如果有必要则需要引入如ejob，xxl-job，quartz等定时调度组件。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://segmentfault.com/a/1190000018788967" target="_blank" rel="external">SpringBoot中并发定时任务的实现、动态定时任务的实现</a></p>
<p><a href="https://my.oschina.net/kevin2kelly/blog/1548237" target="_blank" rel="external">使用ThreadPoolTaskScheduler实现定时关单</a></p>
<p><a href="https://blog.csdn.net/qq_32711309/article/details/84944534" target="_blank" rel="external">ThreadPoolTaskScheduler的使用，定时任务开启与关闭</a></p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般情况下，如果想在Spring Boot中使用定时任务，我们只需要@EnableScheduling开启定时任务支持，在需要调度的方法上添加@Scheduled注解。这样就能够在项目中开启定时调度功能了，并且这种方法支持通过cron表达式灵活的控制执行周期和频率。&lt;/p&gt;
&lt;p&gt;上述的方式好处是快捷，轻量，缺点是周期一旦指定，想要更改必须要重启应用，如果我们想要动态的对定时任务的执行周期进行变更，甚至动态的增加定时调度任务则上述方式就不适用了。&lt;/p&gt;
&lt;p&gt;本文我将讲解如何在Spring 定时任务的基础上进行扩展，实现动态定时任务。&lt;/p&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>我的编码备忘录</title>
    <link href="http://wuwenliang.net/2019/12/02/%E6%88%91%E7%9A%84%E7%BC%96%E7%A0%81%E5%A4%87%E5%BF%98%E5%BD%95/"/>
    <id>http://wuwenliang.net/2019/12/02/我的编码备忘录/</id>
    <published>2019-12-02T14:05:27.000Z</published>
    <updated>2020-02-11T10:50:23.718Z</updated>
    
    <content type="html"><![CDATA[<p>本文将主要记录在日常开发中遇到的各种问题。以技术类别进行章节划分，作为个人的编码备忘录随时进行查阅，并长期进行置顶。</p>
<h2 id="JavaCore相关"><a href="#JavaCore相关" class="headerlink" title="JavaCore相关"></a>JavaCore相关</h2><blockquote>
<p>该模块主要记录JavaCore相关的技术点</p>
</blockquote>
<h3 id="bigdecimal四舍五入"><a href="#bigdecimal四舍五入" class="headerlink" title="bigdecimal四舍五入"></a>bigdecimal四舍五入</h3><pre><code>BigDecimal.ROUND_HALF_UP: 遇到.5的情况时往上近似,例: 1.5 -&gt;;2
BigDecimal.ROUND_HALF_DOWN : 遇到.5的情况时往下近似,例: 1.5 -&gt;;1
</code></pre><h3 id="bigDecimal转换为百分比，保留若干小数"><a href="#bigDecimal转换为百分比，保留若干小数" class="headerlink" title="bigDecimal转换为百分比，保留若干小数"></a>bigDecimal转换为百分比，保留若干小数</h3><pre><code>DecimalFormat decimalFormat = new DecimalFormat(&quot;0.00%&quot;);
BigDecimal decimal = new BigDecimal(count.intValue()).divide(new BigDecimal(allCount), 5, ROUND_HALF_UP);
String formatted = decimalFormat.format(sdPercent);
</code></pre><h3 id="bigDecimal精确度"><a href="#bigDecimal精确度" class="headerlink" title="bigDecimal精确度"></a>bigDecimal精确度</h3><pre><code>BigDecimal.setScale(5,  BigDecimal.ROUND_HALF_UP)  --&gt;保留五位小数,最后一位遇到.5的情况时往上近似
</code></pre><a id="more"></a>
<h3 id="bigDecimal除法"><a href="#bigDecimal除法" class="headerlink" title="bigDecimal除法"></a>bigDecimal除法</h3><ol>
<li>Java的BigDecimal在使用除法（divide方法）时，应该手动指定精度和舍入的方式。</li>
<li>如果不指定精度和舍入方式，在除不尽的时候会报异常。</li>
</ol>
<h3 id="bigdecimal详解"><a href="#bigdecimal详解" class="headerlink" title="bigdecimal详解"></a>bigdecimal详解</h3><p>这里直接参考别人的文章：</p>
<p>(BigDecimal的用法详解)[<a href="https://www.cnblogs.com/jpfss/p/8072379.html" target="_blank" rel="external">https://www.cnblogs.com/jpfss/p/8072379.html</a>]</p>
<h3 id="java8的optionnal"><a href="#java8的optionnal" class="headerlink" title="java8的optionnal"></a>java8的optionnal</h3><p><a href="https://www.cnblogs.com/zhangboyu/p/7580262.html" target="_blank" rel="external">理解、学习与使用 JAVA 中的 OPTIONAL</a></p>
<p>举个例子：</p>
<pre><code>public static void main(String[] args) {
    Double d = 2.2;
    Optional.ofNullable(d).ifPresent(a -&gt; {
        System.out.println(a);
    });
    Double a = null;
    System.out.println(Optional.ofNullable(a).orElse(2.2));
}

ofNullable：如果对象即可能是 null 也可能是非 null，你就应该使用 ofNullable() 方法：不会抛出NullPointerException

ifPresent：检查是否有值的。该方法除了执行检查，还接受一个Consumer(消费者) 参数，如果对象不是空的，就对执行传入的 Lambda 表达式：

orElse：如果有值则返回该值，否则返回传递给它的参数值
</code></pre><h2 id="并发框架"><a href="#并发框架" class="headerlink" title="并发框架"></a>并发框架</h2><blockquote>
<p>这部分主要讲并发框架相关</p>
</blockquote>
<ol>
<li>CompletableFuture用法</li>
</ol>
<p>CompletableFuture需要单独总结，这里直接放一个参考链接。 <a href="https://www.jianshu.com/p/6bac52527ca4" target="_blank" rel="external">CompletableFuture 使用详解</a></p>
<h2 id="集合框架"><a href="#集合框架" class="headerlink" title="集合框架"></a>集合框架</h2><blockquote>
<p>本模块主要记录集合相关的问题</p>
</blockquote>
<h3 id="如何对一个list进行subList操作"><a href="#如何对一个list进行subList操作" class="headerlink" title="如何对一个list进行subList操作"></a>如何对一个list进行subList操作</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">List&lt;Object&gt; list = new Arraylist&lt;&gt;();</div><div class="line">List&lt;Object&gt; subList = list.subList(0, 5);</div><div class="line">其中subList(0, 5)取得的是下标为0到4的元素,不包含下标为5的元素.</div></pre></td></tr></table></figure>
<p>注意：subList看起来好像是取出了原list的子list，实际上仅仅是取到原list的引用。</p>
<blockquote>
<p>subList方法的返回值，只是ArrayList的一个映像而已。</p>
</blockquote>
<p>也就是说，当我们使用子集合subList进行元素的修改操作时，会影响原有的list集合。</p>
<p>如果要生成原有list的子集合，还是采用硬复制的方式比较好，也就是新建一个新的list，对原list集合进行解析后装载到新的list中。</p>
<h3 id="集合如何进行排序操作？（如：如何对一个list中的元素进行排序）"><a href="#集合如何进行排序操作？（如：如何对一个list中的元素进行排序）" class="headerlink" title="集合如何进行排序操作？（如：如何对一个list中的元素进行排序）"></a>集合如何进行排序操作？（如：如何对一个list中的元素进行排序）</h3><p>集合排序，一般有三种方法。</p>
<blockquote>
<p>利用集合框架提供的Collections.sort实现排序，待进行排序的实体需要实现比较器Comparable接口的compareTo方法。</p>
</blockquote>
<p>返回当前入参的实体和this对象的属性差（该属性为排序依据），</p>
<pre><code>属性差如果为负数表示当前入参比本身小，

属性差如果为0表示当前入参与本身相等，

属性差如果为正数表示当前入参比本身大，
</code></pre><p>最后调用Collections.sort(temp);返回的集合排序方式为自然排序。</p>
<blockquote>
<p>通过调用 <strong>Collections.sort(List<t> list, Comparator&lt;? super T&gt; c)</t></strong> 方法，传入Comparator实现，这种方式下，实体不需要实现Comparable接口。</p>
<p>对于JDK1.8，可以通过stream流实现排序，对象本身不需要实现Comparable接口，示例代码如下</p>
</blockquote>
<pre><code>//3.利用Java8的stream流和Comparator实现集合排序
list = list.stream().sorted(Comparator.comparing(Person::getAge)).collect(Collectors.toList());
</code></pre><h2 id="数据库相关-包含Dao相关组件"><a href="#数据库相关-包含Dao相关组件" class="headerlink" title="数据库相关(包含Dao相关组件)"></a>数据库相关(包含Dao相关组件)</h2><blockquote>
<p>本章节主要记录数据库相关的技术点，包含Dao相关组件的使用，如JPA、Mybatis等</p>
</blockquote>
<h3 id="mybatis-update自动追加逗号-“-”"><a href="#mybatis-update自动追加逗号-“-”" class="headerlink" title="mybatis update自动追加逗号 “,”"></a>mybatis update自动追加逗号 “,”</h3><p>一个推荐的SQL如下：</p>
<pre><code>&lt;update id=&quot;updateOne&quot;  parameterType=&quot;com.inspur.search.data.EntityRelation&quot;&gt;
    UPDATE ENTITY_RELATION
        &lt;trim prefix=&quot;set&quot; suffixOverrides=&quot;,&quot;&gt;
        &lt;if test=&quot;srcId!=null&quot;&gt;SRC_ID=#{srcId},&lt;/if&gt;
        &lt;if test=&quot;srcType!=null&quot;&gt;SRC_TYPE=#{srcType},&lt;/if&gt;
        &lt;if test=&quot;destId!=null&quot;&gt;DEST_ID=#{destId},&lt;/if&gt;
        &lt;if test=&quot;destType!=null&quot;&gt;DEST_TYPE=#{destType},&lt;/if&gt;
        &lt;if test=&quot;relType!=null&quot;&gt;REL_TYPE=#{relType},&lt;/if&gt;
        &lt;if test=&quot;status!=null&quot;&gt;STATUS=#{status},&lt;/if&gt;
        &lt;if test=&quot;snId!=null&quot;&gt;SN_ID=#{snId},&lt;/if&gt;
        &lt;/trim&gt;
    WHERE id=#{id}
&lt;/update&gt;
</code></pre><p>这种方式是动态SQL拼接，使用trim是为了删掉最后字段的“,”</p>
<p>不用单独写SET了，因为set被包含在trim中了</p>
<h3 id="mybatis使用truncate语句"><a href="#mybatis使用truncate语句" class="headerlink" title="mybatis使用truncate语句"></a>mybatis使用truncate语句</h3><p>在mybatis中使用truncate语句刷新表。</p>
<pre><code>&lt;update id=&quot;truncateTable&quot;&gt;
    truncate table [表名]
&lt;/update&gt;
</code></pre><h3 id="mybatis使用-Param传参以及批量插入"><a href="#mybatis使用-Param传参以及批量插入" class="headerlink" title="mybatis使用@Param传参以及批量插入"></a>mybatis使用@Param传参以及批量插入</h3><blockquote>
<p>@Param Parameter N/A 如果你的映射器的方法需要多个参数, 这个注解可以被应用于映射器的方法参数来给每个参数一个名字。否则,多参数将会以它们的顺序位置来被命名 (不包括任何 RowBounds 参数) 比如。 #{param1} , #{param2} 等 , 这是默认的。 </p>
<p>如果使用 @Param(“person”), 则参数应该被命名为 #{person}。</p>
</blockquote>
<p>这里重点总结一个场景，批量insert。对应sql如下：</p>
<pre><code>&lt;insert id=&quot;batchInsertIdList&quot; parameterType=&quot;java.util.List&quot;&gt;
    insert into idlist_tmp (id, record_date) values
    &lt;foreach collection=&quot;tmpInfos&quot; item=&quot;tmpInfo&quot; separator=&quot;,&quot;&gt;
        (#{tmpInfo.id}, #{tmpInfo.recordDate})
    &lt;/foreach&gt;
&lt;/insert&gt;
</code></pre><p>这里通过#{tmpInfo.id}指定insert参数为对象tmpInfo的某个属性</p>
<pre><code>int batchInsert(@Param(value = &quot;tmpInfos&quot;) List&lt;TmpInfo&gt; tmpInfos);
</code></pre><p>这里通过@Param指定insert的sql中的参数引用。</p>
<h3 id="MySQL建表时设置timestamp精度到毫秒"><a href="#MySQL建表时设置timestamp精度到毫秒" class="headerlink" title="MySQL建表时设置timestamp精度到毫秒"></a>MySQL建表时设置timestamp精度到毫秒</h3><pre><code>CREATE TABLE `table1` (
`tab1_id` VARCHAR(11) DEFAULT NULL,
`create` TIMESTAMP(3) NULL DEFAULT NULL,
`create2` DATETIME(3) DEFAULT NULL
) ENGINE=INNODB DEFAULT CHARSET=utf8
</code></pre><p>设置精度的方式为：</p>
<pre><code>TIMESTAMP(3)与 DATETIME(3)意思是保留3位毫秒数

TIMESTAMP(6)与 DATETIME(6)意思是保留6位毫秒数
</code></pre><p>修改字段精度的方式为：</p>
<pre><code>ALTER TABLE tb_financial MODIFY CREATE_TIME DATETIME(3) DEFAULT NULL COMMENT &apos;录入时间&apos;;
</code></pre><p>插入日期可以用NOW(3)来控制精确的毫秒数，如：SELECT CURRENT_TIMESTAMP(3);也是可以的</p>
<h3 id="mybatis批量插入"><a href="#mybatis批量插入" class="headerlink" title="mybatis批量插入"></a>mybatis批量插入</h3><p><a href="https://www.jianshu.com/p/c3c890e17e4c" target="_blank" rel="external">mybatis 批量插入数据</a></p>
<p><a href="https://www.cnblogs.com/shuaifing/p/9327465.html" target="_blank" rel="external">mybatis批量保存的两种方式(高效插入)</a></p>
<h3 id="MySQL的case-when语法"><a href="#MySQL的case-when语法" class="headerlink" title="MySQL的case when语法"></a>MySQL的case when语法</h3><p>语法如下：</p>
<pre><code>select 字段1, 字段2,       
    case 字段3     
    when 值1 then 新值       
    when 值2 then 新值      
    end as 重新命名字段3的名字       
from table      
where ……      
order by ……  
</code></pre><p>如：</p>
<pre><code>SELECT
    m.id AS id0,
    v.id AS id1,
    v.user_id AS userId,
    v.app_id AS appId
    (
    CASE
            WHEN v.label = 0 THEN
            &apos;清晰&apos; 
            WHEN v.label = 1 THEN
            &apos;模糊&apos; 
            WHEN v.label = 2 THEN
            &apos;超清晰&apos; ELSE &apos;其他&apos; 
        END 
        ) AS &apos;清晰度&apos;
    FROM
        video_info v,
        media_info m 
    WHERE
        v.id = m.id 
        AND m.id IN (
            &apos;123123&apos;,
            &apos;123124&apos;,
            &apos;123125&apos;
        )
</code></pre><p>参考资料: <a href="https://blog.csdn.net/helloxiaozhe/article/details/78124138" target="_blank" rel="external">MySQL 的CASE WHEN 语句使用说明</a>    </p>
<h2 id="前端相关"><a href="#前端相关" class="headerlink" title="前端相关"></a>前端相关</h2><blockquote>
<p>人生苦短，不会点儿前端都没法儿混了。不喜欢也得会写点儿啊…</p>
</blockquote>
<h3 id="vue页面间传参"><a href="#vue页面间传参" class="headerlink" title="vue页面间传参"></a>vue页面间传参</h3><p>来自：<a href="https://blog.csdn.net/qq_29918313/article/details/82862548" target="_blank" rel="external">https://blog.csdn.net/qq_29918313/article/details/82862548</a></p>
<blockquote>
<p>A页面带着参数传给B页面，B页面带着该参数请求接口或者有其他用途</p>
</blockquote>
<p>A页面：</p>
<pre><code>/* 编辑 */
handleEdit (aa) {
let params = {
    aaId: aa.aaId
}
this.$router.push({
    path: &apos;/bb/edit&apos;,
    name: &apos;Edit&apos;,
    params: params
})
},
</code></pre><p>B页面：</p>
<p>首先要接收A页面传递过来的参数：</p>
<pre><code>let aaId = this.$route.params.aaId
</code></pre><p>接收方式就是代码中使用的this.$route.params.aaId。B页面中需要带着aaId请求数据，则直接使用即可。另外，跳转页面使用this.$router.push({})。</p>
<h3 id="vue-element获取一行数据"><a href="#vue-element获取一行数据" class="headerlink" title="vue element获取一行数据"></a>vue element获取一行数据</h3><p>来自： <a href="https://blog.csdn.net/qq_33616027/article/details/90411872" target="_blank" rel="external">https://blog.csdn.net/qq_33616027/article/details/90411872</a></p>
<blockquote>
<p>使用slot-scope获取数据</p>
</blockquote>
<pre><code>在操作列，对操作按钮先用带有slot-scope属性的dom进行包装，即可获取当前行的数据，
具体的代码，除了操作列不同外，还需要删除el-table标签中绑定的*@row-click*方法，
剩下的都一样：

&lt;el-table-column label=&quot;操作尝试2&quot;&gt;
    &lt;template slot-scope=&quot;scope&quot;&gt;
        &lt;el-button type=&quot;text&quot; @click=&quot;checkDetail(scope.row)&quot;&gt;查看详情&lt;/el-button&gt;
    &lt;/template&gt;
&lt;/el-table-column&gt;
&lt;script&gt;
export default {
        name: &quot;dengmiQuery&quot;,
        data() {
            return {
                modifyForm:{
                    formLabelWidth:&apos;120px&apos;,
                    mimian:&apos;&apos;,
                    mimu:&apos;&apos;
                },
                dengmiQueryForm: {
                    dialogVisible: false,
                    list: [],
                }
            };
        },
        methods: {
            checkDetail(val){
                console.log(val)
            }

        }
    }
&lt;/script&gt;
</code></pre><blockquote>
<p>通过<strong>template slot-scope=“scope”</strong>来定义当前行的数据对象，然后通过scope.row来获取当前行的数据。</p>
</blockquote>
<h3 id="vue-element视频播放组件"><a href="#vue-element视频播放组件" class="headerlink" title="vue element视频播放组件"></a>vue element视频播放组件</h3><p>来自：<a href="https://blog.csdn.net/abelethan/article/details/89016678" target="_blank" rel="external">https://blog.csdn.net/abelethan/article/details/89016678</a></p>
<blockquote>
<p>搞视频相关业务的一定会接触播放组件</p>
</blockquote>
<p>使用  vue-vedio-player</p>
<ul>
<li><p>首先我们先安装这个插件</p>
<pre><code>npm install vue-video-player -s
</code></pre></li>
<li><p>我们需要在main.js里面导入并引用</p>
<pre><code>import VideoPlayer from &apos;vue-video-player&apos;
import &apos;vue-video-player/src/custom-theme.css&apos;
import &apos;video.js/dist/video-js.css&apos;

Vue.use(VideoPlayer)
</code></pre></li>
<li><p>html部分</p>
<pre><code>&lt;template&gt;
    &lt;div class=&apos;demo&apos;&gt;
        &lt;video-player class=&quot;video-player vjs-custom-skin&quot;
                    ref=&quot;videoPlayer&quot;
                    :playsinline=&quot;true&quot;
                    :options=&quot;playerOptions&quot;&gt;
        &lt;/video-player&gt;
    &lt;/div&gt;
&lt;/template&gt;
</code></pre></li>
<li><p>js部分</p>
<pre><code>&lt;script&gt;
    export default {
        data() {
            return {
                playerOptions: {
                    //播放速度
                    playbackRates: [0.5, 1.0, 1.5, 2.0], 
                    //如果true,浏览器准备好时开始回放。
                    autoplay: false, 
                    // 默认情况下将会消除任何音频。
                    muted: false, 
                    // 导致视频一结束就重新开始。
                    loop: false, 
                    // 建议浏览器在&lt;video&gt;加载元素后是否应该开始下载视频数据。auto浏览器选择最佳行为,立即开始加载视频（如果浏览器支持）
                    preload: &apos;auto&apos;, 
                    language: &apos;zh-CN&apos;,
                    // 将播放器置于流畅模式，并在计算播放器的动态大小时使用该值。值应该代表一个比例 - 用冒号分隔的两个数字（例如&quot;16:9&quot;或&quot;4:3&quot;）
                    aspectRatio: &apos;16:9&apos;,
                    // 当true时，Video.js player将拥有流体大小。换句话说，它将按比例缩放以适应其容器。
                    fluid: true,
                    sources: [{
                        //类型
                        type: &quot;video/mp4&quot;,
                        //url地址
                        src: &apos;&apos; 
                    }],
                    //你的封面地址
                    poster: &apos;&apos;, 
                    //允许覆盖Video.js无法播放媒体源时显示的默认信息。
                    notSupportedMessage: &apos;此视频暂无法播放，请稍后再试&apos;,
                    controlBar: {
                        timeDivider: true,
                        durationDisplay: true,
                        remainingTimeDisplay: false,
                        //全屏按钮
                        fullscreenToggle: true  
                    }
                }

            }
        }
    }
&lt;/script&gt;
</code></pre></li>
<li><p>style部分</p>
<pre><code>&lt;style scoped&gt;
.demo{
    display: inline-block;
    width: 600px;
    height: 338px;
    text-align: center;
    line-height: 100px;
    border: 1px solid transparent;
    border-radius: 4px;
    overflow: hidden;
    background: #fff;
    position: relative;
    box-shadow: 0 1px 1px rgba(0, 0, 0, .2);
    margin-right: 4px;
}

.demo:hover{
    display: block;
}
&lt;/style&gt;
</code></pre></li>
</ul>
<h2 id="工具技巧"><a href="#工具技巧" class="headerlink" title="工具技巧"></a>工具技巧</h2><blockquote>
<p>本章节主要记录各种工具使用的技巧，如IDEA GIT等</p>
</blockquote>
<h3 id="跨主机进行文件复制"><a href="#跨主机进行文件复制" class="headerlink" title="跨主机进行文件复制"></a>跨主机进行文件复制</h3><p><a href="https://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/scp.html" target="_blank" rel="external">参考链接</a></p>
<blockquote>
<p>从本地服务器复制到远程服务器</p>
</blockquote>
<p>复制文件:</p>
<pre><code>$scp local_file remote_username@remote_ip:remote_folder
$scp local_file remote_username@remote_ip:remote_file
$scp local_file remote_ip:remote_folder
$scp local_file remote_ip:remote_file
指定了用户名，命令执行后需要输入用户密码；如果不指定用户名，命令执行后需要输入用户名和密码；
</code></pre><p>复制目录:</p>
<pre><code>$scp -r local_folder remote_username@remote_ip:remote_folder
$scp -r local_folder remote_ip:remote_folder
第1个指定了用户名，命令执行后需要输入用户密码； 第2个没有指定用户名，命令执行后需要输入用户名和密码；
</code></pre><p>注解</p>
<blockquote>
<p>从远程复制到本地的scp命令与上面的命令一样，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。</p>
</blockquote>
<p>使用示例</p>
<p>实例1：从远处复制文件到本地目录</p>
<pre><code>$scp root@10.6.159.147:/opt/soft/demo.tar /opt/soft/
说明： 从10.6.159.147机器上的/opt/soft/的目录中下载demo.tar 文件到本地/opt/soft/目录中
</code></pre><p>实例2：从远处复制到本地</p>
<pre><code>$scp -r root@10.6.159.147:/opt/soft/test /opt/soft/
说明： 从10.6.159.147机器上的/opt/soft/中下载test目录到本地的/opt/soft/目录来。
</code></pre><p>实例3：上传本地文件到远程机器指定目录</p>
<pre><code>$scp /opt/soft/demo.tar root@10.6.159.147:/opt/soft/scptest
说明： 复制本地opt/soft/目录下的文件demo.tar 到远程机器10.6.159.147的opt/soft/scptest目录
</code></pre><p>实例4：上传本地目录到远程机器指定目录</p>
<pre><code>$scp -r /opt/soft/test root@10.6.159.147:/opt/soft/scptest
说明： 上传本地目录 /opt/soft/test到远程机器10.6.159.147上/opt/soft/scptest的目录中
</code></pre><h3 id="idea自动导包"><a href="#idea自动导包" class="headerlink" title="idea自动导包"></a>idea自动导包</h3><pre><code>IDEA自动导包配置方式如下：

在菜单中选择如下选项：
 Settings→
    Editor→
        General→
            Auto Import 
然后勾选Add unambiguous imports on the fly以及Optimize imports on the fly
</code></pre><p>解释一下含义：</p>
<pre><code>Add unambiguous imports on the fly：        快速添加明确的导入。
Optimize imports on the fly：               快速优化导入，优化的意思即自动帮助删除无用的导入。
</code></pre><h2 id="打包运行相关"><a href="#打包运行相关" class="headerlink" title="打包运行相关"></a>打包运行相关</h2><blockquote>
<p>本章节主要记录打包相关的问题</p>
</blockquote>
<h3 id="如何通过命令行传递带空格的参数"><a href="#如何通过命令行传递带空格的参数" class="headerlink" title="如何通过命令行传递带空格的参数?"></a>如何通过命令行传递带空格的参数?</h3><p>通过参数引用即可解决该问题，在UNIX环境下，通过引号将带空格的参数括起来即可，如：</p>
<pre><code>$ java PrintFileSizes &quot;/home/steve/Test File.txt&quot;
</code></pre><h3 id="Linux下找出进程正在侦听的端口号"><a href="#Linux下找出进程正在侦听的端口号" class="headerlink" title="Linux下找出进程正在侦听的端口号"></a>Linux下找出进程正在侦听的端口号</h3><p>在Linux下快速查到正在侦听的端口号，命令如下：</p>
<pre><code># 安装工具包，默认已安装，centos下为yum install
sudo apt install net-tools
# 查看侦听中的端口
sudo netstat -ltnp
</code></pre><h3 id="nohup的作用"><a href="#nohup的作用" class="headerlink" title="nohup的作用"></a>nohup的作用</h3><p>举个例子，有启动命令如下</p>
<pre><code>nohup java -jar XXX.jar &gt; /dev/null 2&gt;&amp;1 &amp; 
</code></pre><p>nohup表示：不挂断运行命令,当账户退出或终端关闭时,程序仍然运行。</p>
<p>/dev/null 是一个特殊的文件，写入到它的内容都会被丢弃；这里表示”禁止输出”。我们应当在程序内配置日志打印。</p>
<p>其他用法：<a href="https://blog.csdn.net/wngpenghao/article/details/83022185" target="_blank" rel="external">linux环境下nohup的执行jar</a></p>
<h2 id="git-相关"><a href="#git-相关" class="headerlink" title="git 相关"></a>git 相关</h2><blockquote>
<p>本模块主要整理git相关操作</p>
</blockquote>
<h3 id="git更新带子module的工程"><a href="#git更新带子module的工程" class="headerlink" title="git更新带子module的工程"></a>git更新带子module的工程</h3><ol>
<li><p>首先克隆父工程</p>
<pre><code>git clone ssh://xxxxxx.git

查看子模块

git submodule

子模块前面有一个-，说明子模块文件还未检入（空文件夹）。
</code></pre></li>
<li><p>然后在父工程根路径下初始化子工程</p>
<pre><code>git submodule init

初始化模块只需在克隆父项目后运行一次。
</code></pre></li>
<li><p>更新子工程</p>
<pre><code>git submodule update
</code></pre></li>
</ol>
<h3 id="git更新子工程终极方案"><a href="#git更新子工程终极方案" class="headerlink" title="git更新子工程终极方案"></a>git更新子工程终极方案</h3><pre><code>cd  project　（自己工程目录根目录）
cd  submodule-project （依赖的子工程根路径）
git pull origin branch  （拉取子工程最新提交）
cd  “your-project-root-path”  （进入你自己工程的根目录）
git commit -m &apos;upgrade model&apos; &amp;&amp; git push -u origin your-branch （生成一个提交并push）
</code></pre><p>这样服务端的子工程也就随之发生更新</p>
<h3 id="git-强制使用远程覆盖本地"><a href="#git-强制使用远程覆盖本地" class="headerlink" title="git 强制使用远程覆盖本地"></a>git 强制使用远程覆盖本地</h3><pre><code>git fetch --all &amp;&amp; git reset --hard dev&amp;&amp; git pull
</code></pre><h3 id="git去除某个历史提交"><a href="#git去除某个历史提交" class="headerlink" title="git去除某个历史提交"></a>git去除某个历史提交</h3><p>首先获取git提交记录的commitId</p>
<pre><code>git rebase -i 20624e607934da393719a0a046c27a4be32944f1
</code></pre><p>skip更新为drop或者删除掉</p>
<pre><code>git push origin master --force
</code></pre><h2 id="方法2–递归方式"><a href="#方法2–递归方式" class="headerlink" title="方法2–递归方式"></a>方法2–递归方式</h2><p>递归克隆整个项目</p>
<pre><code>git clone ssh://xxxx.git assets --recursive 
</code></pre><p>递归克隆整个项目，子模块已经同时更新了，一步到位。</p>
<h2 id="Spring相关"><a href="#Spring相关" class="headerlink" title="Spring相关"></a>Spring相关</h2><blockquote>
<p>这部分主要整理Spring框架相关的问题，包括SpringBoot</p>
</blockquote>
<h3 id="Spring中通过-Value-设置默认值"><a href="#Spring中通过-Value-设置默认值" class="headerlink" title="Spring中通过 @Value 设置默认值"></a>Spring中通过 @Value 设置默认值</h3><pre><code>1.1 字符串类型的属性设置默认值

    @Value(&quot;${some.key:my default value}&quot;)
    private String stringWithDefaultValue;

    如果默认值设为空，也将会被设置成默认值。

    @Value(&quot;${some.key:}&quot;)
    private String stringWithBlankDefaultValue;

1.2 基本类型设置默认值

    布尔类型
    @Value(&quot;${some.key:true}&quot;)
    private boolean booleanWithDefaultValue;

    数字类型
    @Value(&quot;${some.key:42}&quot;)
    private int intWithDefaultValue;

1.3 包装类型设置默认值

    布尔类型
    @Value(&quot;${some.key:true}&quot;)
    private Boolean booleanWithDefaultValue;

    数字类型
    @Value(&quot;${some.key:42}&quot;)
    private Integer intWithDefaultValue;

1.4 数组的默认值使用逗号分割

    @Value(&quot;${some.key:one,two,three}&quot;)
    private String[] stringArrayWithDefaults;

    @Value(&quot;${some.key:1,2,3}&quot;)
    private int[] intArrayWithDefaults;

1.5 使用 Spring Expression Language (SpEL) 设置默认值

    @Value(&quot;#{systemProperties[&apos;some.key&apos;] ?: &apos;my default system property value&apos;}&quot;)
    private String spelWithDefaultValue;

    这表示：在systemProperties属性文件中，如果没有设置 some.key 的值，my default system property value 会被设置成默认值。
</code></pre><h2 id="Jersey相关"><a href="#Jersey相关" class="headerlink" title="Jersey相关"></a>Jersey相关</h2><blockquote>
<p>这里记录Jersey相关的内容</p>
</blockquote>
<h3 id="Jersey常用方法"><a href="#Jersey常用方法" class="headerlink" title="Jersey常用方法"></a>Jersey常用方法</h3><p><a href="https://blog.csdn.net/itchiang/article/details/50582979" target="_blank" rel="external">jersey获取各个参数的总结</a></p>
<h3 id="查看Jersey-REST服务的WADL服务定义"><a href="#查看Jersey-REST服务的WADL服务定义" class="headerlink" title="查看Jersey REST服务的WADL服务定义"></a>查看Jersey REST服务的WADL服务定义</h3><p>查看WADL服务定义通过下方URL即可访问到。</p>
<pre><code>http://ip:port/应用根路径/application.wadl
</code></pre><h2 id="Json解析相关"><a href="#Json解析相关" class="headerlink" title="Json解析相关"></a>Json解析相关</h2><blockquote>
<p>这里主要解析Json解析相关的技术点。</p>
</blockquote>
<h3 id="Jackson整理"><a href="#Jackson整理" class="headerlink" title="Jackson整理"></a>Jackson整理</h3><ol>
<li><p>在Jackson中将JsonNode转换为Object</p>
<p>mapper.convertValue(jsonNode, MyPojo.class)</p>
</li>
<li><p>Jackson中的TypeReference</p>
<p> 一般情况下如果没有TypeReference的话，JsonNode转换过来的是LinkedHashMap而不是对象本身，因此<br> 需要使用TypeReference来进行Json的解析</p>
<p> 如：</p>
<pre><code>List&lt;TableColumns&gt; columns = mapper.convertValue(params.get(&quot;columns&quot;), new TypeReference&lt;List&lt;TableColumns&gt;&gt;() {});

对于复杂Json的转换，需要通过TypeReference解析，TypeReference可以正确反序列化嵌套多层的List或Map，例如List&lt;Map&lt;String,String&gt;&gt;
</code></pre><p> 举个实际的例子，</p>
<pre><code>Map&lt;String, MarkDetail&gt; markDetailMap = objectMapper.convertValue(haveMarkDetails, new TypeReference&lt;Map&lt;String,MarkDetail&gt;&gt;() {});

Map&lt;String, String&gt; dataDetailMap = objectMapper.convertValue(dataDetailJsonNode, new TypeReference&lt;Map&lt;String, String&gt;&gt;() {});
</code></pre><p> 可以看到，convertValue的第二个参数是一个TypeReference实例。通过这种方式能够将复杂json类型进行正确解析。</p>
</li>
</ol>
<h2 id="ffmpeg"><a href="#ffmpeg" class="headerlink" title="ffmpeg"></a>ffmpeg</h2><p>ffmpeg的几个工具基本用法</p>
<blockquote>
<p>ffmpeg是用于转码的应用程序。</p>
</blockquote>
<p>一个简单的转码命令可以这样写：</p>
<pre><code>将input.avi转码成output.ts，并设置视频的码率为640kbps

ffmpeg -i videoplayback.mp4 -b:v 640k output.ts
</code></pre><blockquote>
<p>用于播放的应用程序。</p>
</blockquote>
<pre><code>一个简单的播放命令可以这样写：

播放test.avi

ffplay test.avi
</code></pre><blockquote>
<p>ffprobe是用于查看文件格式的应用程序。</p>
</blockquote>
<p>这个就不多介绍了。</p>
<pre><code>ffprobe videoplayback.mp4
</code></pre><h2 id="维护多个sshkey"><a href="#维护多个sshkey" class="headerlink" title="维护多个sshkey"></a>维护多个sshkey</h2><blockquote>
<p>在公司与开源社区之间切换，有多个git账户，因此需要一个策略用于维护多个sshkey</p>
</blockquote>
<p>由于有两个账号，生成密钥时通常都是三个回车一撸到底，那么后执行的会覆盖先执行的。</p>
<p>三个回车中，第一个回车的意思是保存地址，那我们不直接回车，而是输入保存地址就可以。</p>
<h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><ol>
<li><p>生成第一个密钥</p>
<pre><code>ssh-keygen -t rsa -C &quot;myoschina@qq.com&quot;
</code></pre></li>
</ol>
<p>连续三个回车，将oschina的密钥默认保存</p>
<ol>
<li>生成第二个密钥</li>
</ol>
<p>在C盘/Users/用户名/.ssh下建立一个新的目录，如：github</p>
<p>接着运行命令生成第二个sshkey</p>
<pre><code>ssh-keygen -t rsa -C &quot;github@gmail.com&quot;
</code></pre><p>出现Enter file in which to save the key时输入</p>
<pre><code>/c/Users/用户名/.ssh/github/id_rsa
</code></pre><p>表示将本次生成的key保存在建立的github目录下</p>
<ol>
<li>创建config文件</li>
</ol>
<p>C盘/Users/用户名/.ssh下新建config文件，该文件没有后缀名的，用途是配置映射功能，填入下面代码：</p>
<pre><code>#github配置
Host github.com
    HostName github.com
    IdentityFile c:\\Users\\xxxxx\\github\\.ssh\\id_rsa
    PreferredAuthentications publickey
    User usernameOfGithub

#gitoschina的配置
Host git.oschina.net
    HostName git.oschina.net
    IdentityFile c:\\Users\\xxxxx\\.ssh\\id_rsa
    PreferredAuthentications publickey
    User usernameOfOsChina
</code></pre><blockquote>
<p>HostName是服务器域名，IdentityFile 是密钥的地址.</p>
</blockquote>
<h2 id="vim临时显示行号"><a href="#vim临时显示行号" class="headerlink" title="vim临时显示行号"></a>vim临时显示行号</h2><p>如果只是临时显示vim的行号，只须按ESC键退出编辑内容模式，输入</p>
<pre><code>：set number  后按回车键显示行号
</code></pre><p>行号显示只是暂时的，退出vim后再次打开vim就不显示行号了。</p>
<h2 id="python相关"><a href="#python相关" class="headerlink" title="python相关"></a>python相关</h2><blockquote>
<p>此模块主要整理python相关内容</p>
</blockquote>
<h3 id="1-init-py的作用"><a href="#1-init-py的作用" class="headerlink" title="1. init.py的作用"></a>1. <strong>init</strong>.py的作用</h3><pre><code>1、__init__.py是Python中package的标识

__init__.py 文件的一个主要作用是将文件夹变为一个Python模块，
Python 中的每个模块的包中，都有__init__.py 文件

2、批量引入（定义__all__用来模糊导入）

我们在python中导入一个包时，实际上是导入了它的__init__.py文件，
这样我们可以在__init__.py文件中批量导入我们所需要的模块，而不再需要一个一个的导入。

3、配置模块的初始化操作，这个文件也是一个正常的python代码文件，因此可以将初始化代码放入该文件中。
</code></pre><blockquote>
<p>python中<strong>init</strong>.py文件的作用实例：</p>
</blockquote>
<pre><code>python的每个模块的包中，都有一个__init__.py文件，有了这个文件，
我们才能导入这个目录下的module。

__init__.py里面还是可以有内容的，我们在导入一个包时，
实际上导入了它的__init__.py文件。我们可以再__init__.py文件中再导入其他的包，或者模块。

这样，当我们导入这个包的时候，__init__.py文件自动运行。
帮我们导入了这么多个模块，我们就不需要将所有的import语句写在一个文件里了，
也可以减少代码量。不需要一个个去导入module了。
</code></pre><p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将主要记录在日常开发中遇到的各种问题。以技术类别进行章节划分，作为个人的编码备忘录随时进行查阅，并长期进行置顶。&lt;/p&gt;
&lt;h2 id=&quot;JavaCore相关&quot;&gt;&lt;a href=&quot;#JavaCore相关&quot; class=&quot;headerlink&quot; title=&quot;JavaCore相关&quot;&gt;&lt;/a&gt;JavaCore相关&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;该模块主要记录JavaCore相关的技术点&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;bigdecimal四舍五入&quot;&gt;&lt;a href=&quot;#bigdecimal四舍五入&quot; class=&quot;headerlink&quot; title=&quot;bigdecimal四舍五入&quot;&gt;&lt;/a&gt;bigdecimal四舍五入&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;BigDecimal.ROUND_HALF_UP: 遇到.5的情况时往上近似,例: 1.5 -&amp;gt;;2
BigDecimal.ROUND_HALF_DOWN : 遇到.5的情况时往下近似,例: 1.5 -&amp;gt;;1
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;bigDecimal转换为百分比，保留若干小数&quot;&gt;&lt;a href=&quot;#bigDecimal转换为百分比，保留若干小数&quot; class=&quot;headerlink&quot; title=&quot;bigDecimal转换为百分比，保留若干小数&quot;&gt;&lt;/a&gt;bigDecimal转换为百分比，保留若干小数&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;DecimalFormat decimalFormat = new DecimalFormat(&amp;quot;0.00%&amp;quot;);
BigDecimal decimal = new BigDecimal(count.intValue()).divide(new BigDecimal(allCount), 5, ROUND_HALF_UP);
String formatted = decimalFormat.format(sdPercent);
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&quot;bigDecimal精确度&quot;&gt;&lt;a href=&quot;#bigDecimal精确度&quot; class=&quot;headerlink&quot; title=&quot;bigDecimal精确度&quot;&gt;&lt;/a&gt;bigDecimal精确度&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;BigDecimal.setScale(5,  BigDecimal.ROUND_HALF_UP)  --&amp;gt;保留五位小数,最后一位遇到.5的情况时往上近似
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Java" scheme="http://wuwenliang.net/categories/Java/"/>
    
    
      <category term="Java" scheme="http://wuwenliang.net/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot整合Jersey开发RESTful服务</title>
    <link href="http://wuwenliang.net/2019/11/19/Spring-Boot%E6%95%B4%E5%90%88Jersey%E5%BC%80%E5%8F%91RESTful%E6%9C%8D%E5%8A%A1/"/>
    <id>http://wuwenliang.net/2019/11/19/Spring-Boot整合Jersey开发RESTful服务/</id>
    <published>2019-11-19T12:19:33.000Z</published>
    <updated>2019-11-19T12:24:44.087Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文整理一下在Spring Boot中整合Jersey实现RESTful接口的开发。</p>
</blockquote>
<p>最官方的，当然是Spring Boot的官方文档，<a href="https://docs.spring.io/spring-boot/docs/2.2.1.RELEASE/reference/htmlsingle/#boot-features-jersey" target="_blank" rel="external">官方文档中关于整合Jersey的说明</a></p>
<p>我们主要关注如下步骤</p>
<blockquote>
<p>To get started with Jersey, include the spring-boot-starter-jersey as a dependency </p>
</blockquote>
<p>想要在Spring Boot中使用Jersey首先需要引入Jersey支持，坐标如下(我的项目是基于2.2.1.RELEASE构建的)：</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
    &lt;artifactId&gt;spring-boot-starter-jersey&lt;/artifactId&gt;
    &lt;version&gt;2.2.1.RELEASE&lt;/version&gt;
&lt;/dependency&gt;
</code></pre><a id="more"></a>
<blockquote>
<p>and then you need one @Bean of type ResourceConfig in which you register all the endpoints, as shown in the following example:</p>
</blockquote>
<p>引入依赖之后，需要定义一个ResourceConfig资源配置，并且将需要对外暴露为RESTful接口的方法所在的类注册给jersey</p>
<pre><code>@Component
public class JerseyConfig extends ResourceConfig {

    public JerseyConfig() {
        register(Endpoint.class);
    }
}
</code></pre><p>JerseyConfig继承了ResourceConfig，在构造中对Endpoint进行了注册，Endpoint为对外暴露RESTful接口的实现类。</p>
<p>这里需要注意的是，jersey无法通过扫描package的方式扫描可执行jar以及WEB-INF/classs中的包中的endpoints。解决方法也很直接，避免使用package方法进行扫描，改为对每个endPoint类单独进行注册，也就是上面提到的这种方式。<strong>这里想表达的是，ResourceConfig.register方法支持链式调用，我们可以一次性将所有需要对外暴露的endPoint类都注册给ResourceConfig</strong></p>
<blockquote>
<p>For more advanced customizations, you can also register an arbitrary number of beans that implement ResourceConfigCustomizer.</p>
</blockquote>
<p>官网在这里补充到，我们还可以通过实现ResourceConfigCustomizer接口从而实现endPoint资源的注入，一个样例如下：</p>
<pre><code>@Component
public class DemoResourceConfigCustomizer implements ResourceConfigCustomizer {
    @Override
    public void customize(ResourceConfig config) {
        config.register(SpringbootResource.class);
    }
}
</code></pre><p>这里我建立了一个实现ResourceConfigCustomizer接口的配置类DemoResourceConfigCustomizer，实现它的回调方法customize，通过ResourceConfig.register同样实现了资源的注册。</p>
<blockquote>
<p>All the registered endpoints should be @Components with HTTP resource annotations (@GET and others), as shown in the following example:</p>
</blockquote>
<p>完成前面的整合，进入了最为关键的阶段，官网告诉我们，每一个注册的endPoint都应该是一个Spring的bean（标注了@Component，@Service等注解，声明为一个Spring的bean），并且需要在类上，方法上标注javax.ws.rs注解，如：<strong>@GET、@Path等</strong>。</p>
<p>到此我们就完成了基本的Jersey整合Spring Boot的操作，启动应用，通过@Path声明的路径调用我们的接口即可。</p>
<p>来看一个最简单的endPoint代码样例，我直接引用官方的代码样例：</p>
<pre><code>@Component
@Path(&quot;/hello&quot;)
public class Endpoint {

    @GET
    public String message() {
        return &quot;Hello&quot;;
    }

}
</code></pre><p>对于使用过SpringMVC的开发者，这段代码就很亲切了，它其实就类似于SpringMVC中的controller。</p>
<p>总的来说，还是比较简单直观的，习惯了使用SpringMVC之后，换一种开发方式也不失为一种新的体验，更重要的，Jersey框架不仅实现了JAX-RS规范,还提供了自有API以扩展JAX-RS, 它作为官方的实现是值得我们去学习的。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://docs.spring.io/spring-boot/docs/2.2.1.RELEASE/reference/htmlsingle/#boot-features-jersey" target="_blank" rel="external">官方文档中关于整合Jersey的说明</a></p>
<p><a href="https://www.jianshu.com/p/c14a9028e6e7" target="_blank" rel="external">Jersey 开发RESTful（十八） Springboot集成Jersey</a></p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文整理一下在Spring Boot中整合Jersey实现RESTful接口的开发。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最官方的，当然是Spring Boot的官方文档，&lt;a href=&quot;https://docs.spring.io/spring-boot/docs/2.2.1.RELEASE/reference/htmlsingle/#boot-features-jersey&quot;&gt;官方文档中关于整合Jersey的说明&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我们主要关注如下步骤&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;To get started with Jersey, include the spring-boot-starter-jersey as a dependency &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;想要在Spring Boot中使用Jersey首先需要引入Jersey支持，坐标如下(我的项目是基于2.2.1.RELEASE构建的)：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;spring-boot-starter-jersey&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;2.2.1.RELEASE&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
  <entry>
    <title>音视频相关业务名词解释</title>
    <link href="http://wuwenliang.net/2019/11/18/%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B8%E5%85%B3%E4%B8%9A%E5%8A%A1%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/"/>
    <id>http://wuwenliang.net/2019/11/18/音视频相关业务名词解释/</id>
    <published>2019-11-18T03:23:11.000Z</published>
    <updated>2019-11-18T03:27:50.291Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文为扫盲文，为刚进入音视频行业的同学提供一些专业名词的解释，方便更快熟悉音视频相关的业务。</p>
</blockquote>
<h2 id="视频上传"><a href="#视频上传" class="headerlink" title="视频上传"></a>视频上传</h2><p>顾名思义，<strong>视频上传</strong> 即通过使用相关工具，将视频从本地导入到云端服务器进行存储的过程。</p>
<a id="more"></a>
<h2 id="stream视频流"><a href="#stream视频流" class="headerlink" title="stream视频流"></a>stream视频流</h2><p>视频流是指视频数据的传输，例如，它能够被作为一个稳定的和连续的流通过网络处理。</p>
<p>因为流动，客户机浏览器或插件能够在整个文件被传输完成前显示多媒体数据。</p>
<blockquote>
<p>视频流式传输的优点：</p>
<ol>
<li>启动时延大幅度缩短，边下边播，不需要等待所有内容下载完成才开始浏览。网络状况较好的情况下，卡顿较少，但快进、快退需要时间等待</li>
<li>对系统缓存容量的需求大大降低， 由于Internet是以包传输为基础进行断续的异步传输，数据被分解为许多包进行传输，动态变化的网络使各个包可能选择不同的路由，故到达用户计算机的时间延迟也就不同。所以，在客户端需要缓存系统来弥补延迟和抖动的影响和保证数据包传输顺序的正确，使媒体数据能连续输出，不会因网络暂时拥堵而使播放出现停顿。虽然流式传输仍需要缓存，但由于不需要把所有的动画、视音频内容都下载到缓存中，因此，对缓存的要求降低。</li>
<li>流式传输的实现有特定的实时传输协议采用RTSP等实时传输协议，更加适合动画、视音频在网上的流式实时传输。</li>
</ol>
</blockquote>
<h3 id="流媒体的组成部分"><a href="#流媒体的组成部分" class="headerlink" title="流媒体的组成部分"></a>流媒体的组成部分</h3><ol>
<li>编码工具：用于创建、捕捉和编辑多媒体数据，形成流媒体格式</li>
<li>流媒体数据</li>
<li>服务器：存放和控制流媒体的数据</li>
<li>网络：适合多媒体传输协议甚至是实时传输协议的网络</li>
<li>播放器：供客户端浏览流媒体文件 这5个部分有些是网站需要的，有些是客户端需要的，而且不同的流媒体标准和不同公司的解决方案会在某些方面有所不同。 3、各种多媒体信息的流媒体传输格式 在Internet上所传输的多媒体格式中，基本上只有文本、图形可以照原格式在网上传输。动画、音频、视频等虽然可以直接播放在网上播放，但文件偏大，即使使用专线上网，也要等完全下载后才能观看，这三种类型的媒体均要采用流式技术来进行处理以便于在网上传输。另外，还有一些如PowerPoint文件、多媒体课件等内容也需要用流式技术进行传输。 流媒体格式是将一个资料（动画、影音等）分段传送，用户不必等待整个内容传送完毕，就可以观看到即时的连续的内容，甚至可以随时的暂停、快进、快倒。由于不同的公司发展的文件格式不同，传送的方式也有所差异，因此，我们必须非常清楚各种流媒体文件的格式。</li>
</ol>
<h3 id="视频播放"><a href="#视频播放" class="headerlink" title="视频播放"></a>视频播放</h3><p>流媒体是从英语Streaming Media中翻译过来，它是一种可以使音频、视频和其它多媒体能在Internet及Intranet上以实时的、无需下载等待的方式进行播放的技术。流媒体文件格式是支持采用流式传输及播放的媒体格式。流式传输方式是将动画、视音频等多媒体文件经过特殊的压缩方式分成一个个压缩包用户不必像非流式播放那样等到整个文件全部下载完毕后才能看到当中的内容，而是只需经过几秒或几十秒的启动延时即可在用户的计算机上利用相应的播放器或其它的硬件、软件对压缩的动画、视音频等流式多媒体文件解压后进行播放和观看，多媒体文件的剩余部分将在后台的服务器内继续下载。</p>
<h3 id="流式下载编辑"><a href="#流式下载编辑" class="headerlink" title="流式下载编辑"></a>流式下载编辑</h3><p>下载边播放的BT软件,下载时必须要从电影的开头下载,而并非是其它BT软件的下载方式.，这种可以边下载边播放的下载 方式,就可以称为流式下载… 　如果想要边下载边播放的话,就推荐你用流式下载. 　如果是其它无法在线播放的资源,推荐使用非流式下载. 　媒体是指采用流式传输的方式在Internet播放的媒体格式。 　流式传输方式则是将整个A/V及3D等多媒体文件经过特殊的压缩方式分成一个个压缩包，由视频服务器向用户计算机连续、实时传送。在采用流式传输方式的系统中，用户不必像采用下载方式那样等到整个文件全部下载完毕，而是只需经过几秒或几十秒的启动延时即可在用户的计算机上利用解压设备(硬件或软件)对压缩的A/V、3D等多媒体文件解压后进行播放和观看。此时多媒体文件的剩余部分将在后台的服务器内继续下载。 　与单纯的下载方式相比，这种对多媒体文件边下载边播放的流式传输方式不仅使启动延时大幅度地缩短，而且对系统缓存容量的需求也大大降低。（ChinaByte）</p>
<h2 id="H264-H265"><a href="#H264-H265" class="headerlink" title="H264 H265"></a>H264 H265</h2><p>H.265与H.264有何不同,同等画质体积仅为一半、带宽占用省一半、画质更细腻等诸多优势  首先分别介绍一下：H.264与H.265</p>
<p>1、H.264也称作MPEG-4AVC(Advanced Video Codec，高级视频编码)，是一种视频压缩标准，同时也是一种被广泛使用的高精度视频的录制、压缩和发布格式。H.264因其是蓝光光盘的一种编解码标准而着名，所有蓝光播放器都必须能解码H.264。H.264相较于以前的编码标准有着一些新特性，如多参考帧的运动补偿、变块尺寸运动补偿、帧内预测编码等，通过利用这些新特性，H.264比其他编码标准有着更高的视频质量和更低的码率</p>
<p>2、H.265/HEVC的编码架构大致上和H.264/AVC的架构相似，也主要包含：帧内预测(intra prediction)、帧间预测(inter prediction)、转换 (transform)、量化 (quantization)、去区块滤波器(deblocking filter)、熵编码(entropy coding)等模块。但在HEVC编码架构中，整体被分为了三个基本单位，分别是：编码单位(coding unit，CU)、预测单位(predict unit，PU) 和转换单位(transform unit，TU )。<br>H.265是新的编码协议，也即是H.264的升级版。</p>
<h2 id="视频拆条"><a href="#视频拆条" class="headerlink" title="视频拆条"></a>视频拆条</h2><p>视频拆条是因互联网视频和新媒体短视频内容平台的需要，对传统电视媒体节目进行二次加工，将原来完整的一条节目内容，按照某种逻辑思维或特定需要，将其拆分成多条视频。</p>
<p>互联网视频内容的主要来源包括传统电视媒体的节目、各类机构视频成品、影视公司影视作品，通过将这些视频拆条，可以深度挖掘有价值的信息，重新编目后，可用于IPTV、OTT、手机电视和新媒体短视频平台，满足新媒体视听节目碎片化要求，是视音频编目行业一个新的尝试和探索。</p>
<h2 id="视频抽帧"><a href="#视频抽帧" class="headerlink" title="视频抽帧"></a>视频抽帧</h2><p>从视频流中截取关键的帧</p>
<p>简单的说，视频抽帧就是从视频中把要做抽帧的片段在轨道里放到能看见一帧帧画面的模式，用刀片再割断删除你要去除的某帧，比如每隔一帧去除一帧，或者每隔三帧去除二帧<del>~</del>等等等等，然后将剩下的帧移动紧挨对齐，这就是抽帧。</p>
<h3 id="视频帧率"><a href="#视频帧率" class="headerlink" title="视频帧率"></a>视频帧率</h3><p>视频帧率（Frame rate）是用于测量显示帧数的量度。所谓的测量单位为每秒显示帧数(Frames per Second，简：FPS）或“赫兹”（Hz）。此词多用于影视制作和电子游戏。</p>
<p>视频帧率（Frame rate）是用于测量显示帧数的量度。所谓的测量单位为每秒显示帧数(Frames per Second，简：FPS）或“赫兹”（Hz）。此词多用于影视制作和电子游戏。</p>
<h2 id="视频分发"><a href="#视频分发" class="headerlink" title="视频分发"></a>视频分发</h2><p>一般视频分发基于算法进行</p>
<p>视频来源多样化，有的基于UCG（用户主动生成），有些基于爬虫，爬取其他平台的内容（一般都有水印）</p>
<p>视频分发还可以配合精准推荐算法，进行定向广告投放</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">在爱奇艺头条的商业化上，爱奇艺与百度合作力求让信息流广告更精准：依托从百度搜索引擎的数据，知道每个爱奇艺用户过去三十天在百度上搜索过什么类型关键词，根据他的消费兴趣爱好，可以帮助广告主投放定向广告。无论对于广告主、视频创作者还是爱奇艺，这都是件好事。</div></pre></td></tr></table></figure>
<h2 id="视频缩略图"><a href="#视频缩略图" class="headerlink" title="视频缩略图"></a>视频缩略图</h2><p>一般会抽取视频第一帧作为视频缩略图，或者支持后台上传图片作为视频缩略图</p>
<h2 id="ffmpeg"><a href="#ffmpeg" class="headerlink" title="ffmpeg"></a>ffmpeg</h2><p>FFmpeg是一个开源免费跨平台的视频和音频流方案，属于自由软件，采用LGPL或GPL许可证（依据你选择的组件）。它提供了录制、转换以及流化音视频的完整解决方案。它包含了非常先进的音频/视频编解码库libavcodec，为了保证高可移植性和编解码质量，libavcodec里很多codec都是从头开发的。</p>
<p>这个项目最早由Fabrice Bellard发起，现在由Michael Niedermayer维护。许多FFmpeg的开发人员都来自MPlayer项目，而且当前FFmpeg也是放在MPlayer项目组的服务器上。项目的名称来自MPEG视频编码标准，前面的”FF”代表”Fast Forward”。</p>
<p>FFmpeg是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序。它包括了目前领先的音/视频编码库libavcodec。 FFmpeg是在Linux下开发出来的，但它可以在包括Windows在内的大多数操作系统中编译。这个项目是由Fabrice Bellard发起的，现在由Michael Niedermayer主持。可以轻易地实现多种视频格式之间的相互转换，例如可以将摄录下的视频avi等转成现在视频网站所采用的flv格式。</p>
<h2 id="misc转码"><a href="#misc转码" class="headerlink" title="misc转码"></a>misc转码</h2><p>个人认为这是混合转码的一种综合说法</p>
<h2 id="视频加工"><a href="#视频加工" class="headerlink" title="视频加工"></a>视频加工</h2><p>通过工具对视频进行基本分析（长宽），格式校验，压缩，编解码，格式转换，缩略图抽取等操作</p>
<h2 id="视频格式"><a href="#视频格式" class="headerlink" title="视频格式"></a>视频格式</h2><blockquote>
<p>问题：本地视频文件常见有MP4、MKV、AVI等，这些都是什么？有什么区别？</p>
</blockquote>
<p>首先，MP4、AVI、MKV都是本地视频文件的后缀，在windows系统下，用于提示操作系统应该采用哪个应用程序打开。而在流媒体领域，这些都被称为『视频封装格式』，因为除了音视频流之外，它们还包含了一些辅助信息以及组织视音频的方式。不同格式的视频在不同平台上用户体验不同，很大原因在于对视音频的组织方式带来的差异。笔者以为百度百科上的解释蛮通俗易懂的（维基百科的说法不够直白）：</p>
<p>视频格式是视频播放软件为了能够播放视频文件而赋予视频文件的一种识别符号。<br>简言之，视频格式规定了和播放器的通信协议。</p>
<h2 id="视频协议"><a href="#视频协议" class="headerlink" title="视频协议"></a>视频协议</h2><blockquote>
<p>问题：在腾讯视频、哔哩哔哩网上看的视频，与本地播放的MP4、MKV、AVI文件，有什么区别？</p>
</blockquote>
<p>『视频协议』是针对网络流媒体而言的，也就是只有在有网络时通过浏览器或者移动端APP才能看到的视频，目前常见的协议有RTSP、RTMP、HLS、HTTP等。笔者短暂地接触过GStreamer开发，在连接到RSTP视频时，发现除了视音频流和metadata之外，还携带了播放的信令。</p>
<p>也有文章会把『视频协议』归入『视频封装格式』。笔者看来，这么分类也有其道理：『视频协议』和『视频封装格式』都同时携带了视音频和metadata，以及协议/格式需要的其他信息。以FFMpeg为例，并不区分视频格式和视频协议；但是GStreamer的话，还时需要指定『视频协议』，但是不区分『视频封装格式』。</p>
<p>剥开『视频封装格式』和『视频协议』的外壳，接下来了解视音频流本身，这才是流媒体领域中真正的主角。本文仅介绍视频流。</p>
<h2 id="视频流-以及-编解码-视频转码"><a href="#视频流-以及-编解码-视频转码" class="headerlink" title="视频流 以及 编解码 / 视频转码"></a>视频流 以及 编解码 / 视频转码</h2><p>就视频流而言，相信大家平时一定经常听到类似“h264码流”、“yuv流”、“编码流”、“解码流”，“原始流”、“裸流”，“压缩后的流”或者“未压缩的流”等等。归纳而言，提到『视频流』的时候，一定只有两种形式：</p>
<p>经过压缩算法压缩的流数据，称为『编码流』，又因为目前压缩/编码算法以H264为主，因此也常常称为『H264码流』。<br>未经压缩的流数据，是解码后的流数据，称为『原始流』，可以想象视频是由一幅一幅在时间上连续的“图像”组成的，而因为视频内部的“图像”是『YUV』（后文将介绍），因此也常常称为『YUV流』。<br>总结出现的名称，“h264码流”、“编码流”、“压缩后的流”是压缩/编码后的视频流；而“yuv流”、“解码流”、“未压缩的流”则是未经压缩/编码的视频流。“裸流”是一个具有歧义的词，是上下文内容，既可以是前者，也可以是后者。</p>
<p>因此，如果以后阅读任何流媒体相关的文章时，看到『视频流』都应该搞清楚，这究竟是编码/压缩的，还是没有。在生活中，接触到的视频文件绝大部分都是编码/压缩后的；在网络传输场景中，绝大部分也是编码/压缩后的。只有在视频播放时，观众观赏到的时一帧帧被『转码』为『RGB』的解码后视频流。</p>
<p>编码/压缩在流媒体领域是一项非常重要的技术：从『H264码流』到『YUV流』的过程称为解码，反之称为编码。</p>
<h2 id="常见的帧名词"><a href="#常见的帧名词" class="headerlink" title="常见的帧名词"></a>常见的帧名词</h2><ol>
<li>帧率（FPS）<br>『帧率』，FPS，全称Frames Per Second。指每秒传输的帧数，或者每秒显示的帧数，一般来说，『帧率』影响画面流畅度，且成正比：帧率越大，画面越流畅；帧率越小，画面越有跳动感。一个较权威的说法：<br>当视频帧率不低于24fps时，人眼才会觉得视频时连贯的，称为“视觉暂留”现象。<br>因此，才有说法：尽管『帧率』越高越流畅，但在很多实际应用场景中24fps就可以了。</li>
<li>分辨率（Resolution）<br>『分辨率』，也常被俗称为『图像的尺寸』或者『图像的大小』。指一帧图像包含的像素的多少，常见有1280x720（720P），1920X1080（1080P）等规格。『分辨率』影响图像大小，且与之成正比：『分辨率』越高，图像越大；反之，图像越小。</li>
<li>码率（BPS）<br>『码率』，BPS，全称Bits Per Second。指每秒传送的数据位数，常见单位KBPS（千位每秒）和MBPS（兆位每秒）。笔者认为这个概念真正要理解起来还是需要好好说明的，网上一说：“『码率』与体积成正比：码率越大，体积越大；码率越小，体积越小”；另一说：“『码率』越大，说明单位时间内取样率越大，数据流精度就越高，这样表现出来的的效果就是：视频画面更清晰画质更高”；还有说法是：”『码率』就是『失真度』“。但是笔者有一段时间就是不理解，每秒传输的数据越大，为什么必然就对应画面更清晰？还有体积怎么理解呢？且看下文”三者之间的关系“。</li>
</ol>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://baike.baidu.com/item/%E8%A7%86%E9%A2%91%E6%B5%81" target="_blank" rel="external">https://baike.baidu.com/item/%E8%A7%86%E9%A2%91%E6%B5%81</a></p>
<p><a href="https://www.cnblogs.com/LLBFWH/p/11660530.html" target="_blank" rel="external">https://www.cnblogs.com/LLBFWH/p/11660530.html</a></p>
<p><a href="https://www.pingwest.com/a/120094" target="_blank" rel="external">https://www.pingwest.com/a/120094</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/36109778" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/36109778</a></p>
<p><a href="https://blog.csdn.net/shenhuan1104/article/details/72824076" target="_blank" rel="external">https://blog.csdn.net/shenhuan1104/article/details/72824076</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/61747783" target="_blank" rel="external">视频和视频帧：视频和帧基础知识整理</a></p>
<p><a href="https://www.bgteach.com/article/134" target="_blank" rel="external">视频文件格式知多少 | avi、mpeg、mp4、mov、ProRes、DNxHR、mfx、mkv、wmv、flv、rmvb、webm…</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文为扫盲文，为刚进入音视频行业的同学提供一些专业名词的解释，方便更快熟悉音视频相关的业务。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;视频上传&quot;&gt;&lt;a href=&quot;#视频上传&quot; class=&quot;headerlink&quot; title=&quot;视频上传&quot;&gt;&lt;/a&gt;视频上传&lt;/h2&gt;&lt;p&gt;顾名思义，&lt;strong&gt;视频上传&lt;/strong&gt; 即通过使用相关工具，将视频从本地导入到云端服务器进行存储的过程。&lt;/p&gt;
    
    </summary>
    
      <category term="音视频" scheme="http://wuwenliang.net/categories/%E9%9F%B3%E8%A7%86%E9%A2%91/"/>
    
    
      <category term="音视频" scheme="http://wuwenliang.net/tags/%E9%9F%B3%E8%A7%86%E9%A2%91/"/>
    
  </entry>
  
  <entry>
    <title>精诚所至,我的跳槽之路</title>
    <link href="http://wuwenliang.net/2019/11/01/%E7%B2%BE%E8%AF%9A%E6%89%80%E8%87%B3-%E6%88%91%E7%9A%84%E8%B7%B3%E6%A7%BD%E4%B9%8B%E8%B7%AF/"/>
    <id>http://wuwenliang.net/2019/11/01/精诚所至-我的跳槽之路/</id>
    <published>2019-11-01T03:56:16.000Z</published>
    <updated>2019-11-01T05:58:09.402Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>天行健，君子以自强不息。 – 《周易·乾·象》</p>
</blockquote>
<p>踏出熟悉的公司大门，我回头，知道以后应该不会经常回来，抬手，拍照，朋友圈里，我留下了“莫愁前路无知己，天下谁人不识君”的文字。</p>
<p>再见了，我将拥抱新的生活。再见了，我会有新的工作。</p>
<a id="more"></a>
<p>别人看起来我走的突然，但换工作是迟早的事情，是我的主观选择，年初就着手开始准备。</p>
<p>裸面是大忌，因此我指定了计划，分模块进行复习:</p>
<ul>
<li>java核心：集合框架，重点复习HashMap、ConcurrentHashMap、ArrayList、LinkedList</li>
<li>java核心：多线程，重点复习synchronized、volitaile、JMM模型、Lock</li>
<li>缓存：重点复习Redis</li>
<li>消息队列：重点复习RocketMQ</li>
<li>RPC：重点复习Dubbo</li>
<li>Spring：以IOC、AOP为核心，了解其源码，同时看了事务相关的源码</li>
<li>JVM相关：主要从项目出发，学习整理了JVM结构以及调优相关的知识</li>
<li>数据库：重点复习了索引、redoLog、undoLog、MVCC以及简单看了看调优</li>
<li>数据结构和算法：我没有复习很多，主要是在面试之前把二分查找、快排、二叉树、链表的核心代码看了看就去面试了</li>
<li>分布式协议，如RAFT，Gossip等，这部分是我给自己的要求</li>
<li>项目：主要总结了核心流程，能够做到胸有成竹的画出各种流程图，对于项目中的遇到的问题能够流畅的表达出来</li>
</ul>
<p>有了计划，在执行中，重点学习了极客时间的专栏，这里不是打广告，极客时间的高并发和MySQL两个专栏对我帮助很大。文末会放出参考资料。</p>
<p>在上下班的路上，每天能有30-50分钟时间学习，基本上是看极客时间，高并发专栏我来来回回看了三遍，在后面的面试中，基本上没有没被问住。</p>
<p>在整个面试的准备过程中，我花心思多准备了消息队列相关的东西，用了三个月左右的时间把RocketMQ的重要源码都看了一遍并在博客中写了源码分析文章。在后面的面试中，只要问到MQ相关的问题，我都能滔滔不绝的和面试官聊，基本上能够从架构聊到应用再深入到底层，这里也要感谢老东家给我实战的机会，让我能够更加直观体会到MQ带来的便利和优越性，并以此为契机，让我能够加入到RocketMQ的社区中。</p>
<p>我想说的是，如果想要找一个满意的岗位，前期的准备是必不可少的，虽然我没有像少数的同学，废寝忘食，但是功夫还是下了的，至少在MQ和JVM上，我投入了大量的精力，这也让我很受益。</p>
<blockquote>
<p>如何在面试中展示自己的亮点，让自己能够吸引面试官的注意呢？</p>
</blockquote>
<p>对于这个问题，我在很多地方都回答过，说起来并不难。</p>
<ul>
<li>一个稳定更新的github，有互动有文档，有一定数量的star；</li>
<li>一个稳定更新的博客，公众号，如果有一定的知名度那更好；</li>
<li>能够积极参与开源社区，有一定的贡献；</li>
<li>对某方面技术有较为深刻的认识或研究</li>
</ul>
<p>上述这些，如果满足一项，都会超出很多人，但没有一个不需要花费半年甚至更多的时间去准备。</p>
<p>说说我的经验吧，我写了五年多博客，参与过开源社区，写过很多轮子，对于造轮子的思考以及总结都及时发布到自己的博客中。</p>
<p>三年里没有说每天起早贪黑，也差不多就是每天持续学习两小时左右。</p>
<p>在面试的过程中，我用一周时间拿到三个互联网offer，当然和那些面经中的大佬们比还是差距很大，但我已经很满意。</p>
<p>怎么说呢，一句话就是皇天不负有心人。道理大家都明白，但是客观上就是会有明显的差异。</p>
<p>有的人喜欢问别人在面试的过程中被问了哪些问题，包括我看过的很多面经都是对自己面试经过的复述，基本上就是一个面试问题的记录。不能说没用吧，从这些问题中是能够分析出一些共性的，但我不想这么写。我认为，好的方法要胜过知识本身，当一个人有了一套自己的学习方法与思考框架，那么他做什么事情都能够游刃有余。所谓，授之以鱼不如授之以渔，大概就是这个道理的体现。</p>
<p>在真实的面试中，面试官问什么问题是取决于我们面试的岗位，更重要的，是取决于面试者简历上展示的能力以及项目中的亮点，如果能够准备一份亮眼的简历，并且刻意去进行引导，相信不需要问别人也大致能够知道自己需要准备哪些问题。毕竟面试这个事儿，还是因人而异。</p>
<p>我在文章的开始部分，罗列了我自己的复习计划，基本上这就是Java开发岗都需要关注并掌握的核心知识，到哪里都逃不开的。知识的复习，是一个体系的建立，所以指望问别人几道题就能够通过面试，这种侥幸心理是要不得的，功夫不负有心人，不愿付出努力，不愿总结就想要轻松通过面试，斩获大厂offer，我觉得不是不可能，但至少可能性不会很大。</p>
<p>之前也加过一些群，看着群里不断有同学拿到了好offer，然后一帮人在后面追问学习方法，面试问题，我只觉得好笑。学习方法可以借鉴，但不能照搬，一方面，我为这些拿到满意offer的同学感到佩服，另一方面，我也暗下决心，有一天，我也会找到我想要的工作，去做我想做的事情。</p>
<p>君子以自强不息，好的执行力，是成功的必要条件。我享受计划的制定与实施，客观的对自己的优劣势进行分析，不断复习，整理，复盘，总结，对简历持续进行打磨，在某个事情的刺激下，国庆节前，我开始了简历的投递。</p>
<p>国庆节，是漫长的等待。节后，我陆续收到了面试邀约，连续面试一周，高强度的持续实战与复盘，我最终收获了心仪的offer，也最终选择了爱奇艺。</p>
<p>“简单想，简单做”。</p>
<p>这是实干家的乐园。</p>
<p>这里分享一个面试的技巧，如果能画图就多画图，用画图的方式表达自己的思路，能够变被动为主动，而且也可以让自己的心态放松，从而更加条理清晰的表达出自己的观点。面试官也基本上会从面试者画的图中进行提问，从而有效避免了一问一答的“审问”式面试。</p>
<p>或许有人会认为我的经验难以实践，但我曾经也是一个迷茫的小菜鸟，也曾在群里发问，问题的简单与无脑让我现在回想起来都觉得可爱。</p>
<p>但我始终相信梦想的力量，相信实践的力量，相信持续的付出一定会有所收获。</p>
<p>我始终感谢大二时候的我做出的决定，我要写博客。这一写，就是五年。这也是我到现在坚持最久的一个习惯，当你开始着手实施一个看起来不可能完成的任务，并不断坚持从而变成习惯的时候，这件事也就不那么困难，每天进步一点点，当多年以后，曾经的那颗幼苗，已经长成了大树。</p>
<p>我纷乱的表达，只为了向你分享一个观点，想清楚再做，比盲目去做要更容易接近所谓的成功。想明白自己要什么，树立一个明确的目标，并制定适合自己的计划，坚持执行下去，一切都是水到渠成。</p>
<p>没有什么成功，生活是没有终点的，如果有，那就是死亡。</p>
<p>我还有呼吸，我还有心跳，我未来的每一天都是未知且新奇的，不断的反思过往，不断的树立目标并坚持完成，这是我生活的乐趣。与己斗，其乐无穷。</p>
<p>感谢遇到的每一个人，感谢经历的每一个挫折，感谢低谷中不服输的自己，感激并不断努力着。</p>
<p>精诚所至，金石为开。一切都没结束，一切才刚刚开始，这是我平凡的跳槽之路，这是一个普通人的内心独白。这是我的狂欢，这是我的舞台。</p>
<p>–写于午后。</p>
<h2 id="附录：复习资料"><a href="#附录：复习资料" class="headerlink" title="附录：复习资料"></a>附录：复习资料</h2><ul>
<li>MySQL实战45讲（极客时间）</li>
<li>Java并发编程实战（极客时间）</li>
<li>《Java并发编程的艺术》</li>
<li>《MySQL技术内幕：InnoDB存储引擎》</li>
<li>《RocketMQ技术内幕》</li>
<li>《Redis 深度历险：核心原理与应用实践》</li>
<li>《深入理解Apache Dubbo与实战》</li>
<li><a href="https://github.com/doocs/advanced-java" target="_blank" rel="external">java工程师面试突击笔记</a></li>
<li><a href="https://www.bilibili.com/video/av71505947" target="_blank" rel="external">java工程师面试突击视频</a></li>
</ul>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;天行健，君子以自强不息。 – 《周易·乾·象》&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;踏出熟悉的公司大门，我回头，知道以后应该不会经常回来，抬手，拍照，朋友圈里，我留下了“莫愁前路无知己，天下谁人不识君”的文字。&lt;/p&gt;
&lt;p&gt;再见了，我将拥抱新的生活。再见了，我会有新的工作。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://wuwenliang.net/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://wuwenliang.net/tags/%E9%9A%8F%E7%AC%94/"/>
    
  </entry>
  
  <entry>
    <title>探秘CouchDB之centos下搭建CouchDB</title>
    <link href="http://wuwenliang.net/2019/10/22/%E6%8E%A2%E7%A7%98CouchDB%E4%B9%8BCouchDB%E6%90%AD%E5%BB%BA/"/>
    <id>http://wuwenliang.net/2019/10/22/探秘CouchDB之CouchDB搭建/</id>
    <published>2019-10-22T05:34:49.000Z</published>
    <updated>2019-10-22T07:35:20.204Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>CouchDB 是一个开源的面向文档的数据库管理系统，它具有高度可伸缩性，提供了高可用性和高可靠性。</p>
<p>CouchDB发布于2005年，2008年成为Apache软件基金会项目。CouchDB是一个面向文档的NoSQL数据库。</p>
</blockquote>
<p>本系列笔者将记录对CouchDB的学习实战相关内容。</p>
<p>作为本系列第一篇，本文主要介绍如何在centos下搭建CouchDB，以及CouchDB的简单使用。</p>
<a id="more"></a>
<h2 id="安装EPEL库"><a href="#安装EPEL库" class="headerlink" title="安装EPEL库"></a>安装EPEL库</h2><blockquote>
<p>在Centos7上安装CouchDB之前需要安装基础EPEL基础环境，使用以下命令添加EPEL存储库。</p>
</blockquote>
<pre><code>yum -y install epel-release
</code></pre><h2 id="安装Apache-CouchDB"><a href="#安装Apache-CouchDB" class="headerlink" title="安装Apache CouchDB"></a>安装Apache CouchDB</h2><blockquote>
<p>由于CouchDB提供了RedHat的rpm包，因此我们能够直接从apache的库中安装CouchDB。</p>
</blockquote>
<p>执行以下指令，编辑apache-couchdb.repo文件</p>
<pre><code>cd /etc/yum.repos.d/
vim apache-couchdb.repo
</code></pre><p>添加以下内容到apache-couchdb.repo中</p>
<pre><code>[bintray--apache-couchdb-rpm]
name=bintray--apache-couchdb-rpm
baseurl=http://apache.bintray.com/couchdb-rpm/el$releasever/$basearch/
gpgcheck=0
repo_gpgcheck=0
enabled=1
</code></pre><p>保存并退出编辑器，执行install安装couchDB</p>
<pre><code>yum -y install couchdb
</code></pre><p>等待安装完成，设置couchDB为开机自启：</p>
<pre><code>systemctl start couchdb
systemctl enable couchdb
</code></pre><p>设置完成之后，执行状态检查及服务器端口检查</p>
<pre><code>systemctl status couchdb
netstat -plntu
</code></pre><p>到此，couchDB便被成功安装到centos7机器上，运行的默认端口为5984。</p>
<h2 id="启用CouchDB的图形化界面"><a href="#启用CouchDB的图形化界面" class="headerlink" title="启用CouchDB的图形化界面"></a>启用CouchDB的图形化界面</h2><blockquote>
<p>CouchDB提供了一个图形化界面，供我们进行友好的交互，支持开发者进行可视化的数据库创建、删除、数据同步等操作。</p>
</blockquote>
<p>这里我们配置启用CouchDB的图形化界面。</p>
<p>进入CouchDB的 <strong>/opt/couchdb</strong> 目录，编辑’etc/‘目录下的’default.ini’配置文件。</p>
<pre><code>cd /opt/couchdb
vim etc/default.ini
</code></pre><p>跳转到 <strong>[chttpd]</strong> 配置行，将bind_address的值设置为本地ip地址。</p>
<pre><code>[chttpd]
port = 5984
bind_address = 0.0.0.0
</code></pre><p>保存并退出，重启CouchDB服务。</p>
<pre><code>systemctl restart couchdb
</code></pre><p>进入CouchDB管理页面，在浏览器输入服务器ip地址，如： <strong><a href="http://ip:5984/_utils/" target="_blank" rel="external">http://ip:5984/_utils/</a></strong></p>
<p><strong>注意</strong> </p>
<p>如果服务器上运行了firewalld，需要使用firewall-cmd命令开放couchdb端口5984</p>
<pre><code>firewall-cmd --add-port=5984/tcp --permanent
firewall-cmd --reload
</code></pre><p>如果是iptables则，使用如下命令开放5984端口</p>
<pre><code>/sbin/iptables -I INPUT -p tcp --dport 5984 -j ACCEPT
</code></pre><p>如图，便是CouchDB后台页面：</p>
<p><img src="/2019/10/22/探秘CouchDB之CouchDB搭建/ui.png" alt="ui.png"></p>
<h2 id="配置管理员帐户CouchDB"><a href="#配置管理员帐户CouchDB" class="headerlink" title="配置管理员帐户CouchDB"></a>配置管理员帐户CouchDB</h2><p>首次登陆图形化管理页面按照提示设置管理员账户即可，一定要记住账户密码，后续使用命令进行操作的时候也需要指定账户/账户密码。</p>
<h2 id="通过curl简单使用CouchDB"><a href="#通过curl简单使用CouchDB" class="headerlink" title="通过curl简单使用CouchDB"></a>通过curl简单使用CouchDB</h2><p>这里介绍一下如何在命令行界面下通过curl简单使用CouchDB。关于图形化方式的使用，读者可以自行摸索。</p>
<p>要获得有关已安装的couchdb服务器的信息，我们可以使用’ GET ‘参数，如下所示。</p>
<pre><code>curl -X GET http://localhost:5984/  
</code></pre><p>通过无密码方式创建新的数据库【这种方式在设置了管理员账户之后会失效】</p>
<pre><code>curl -X PUT http://localhost:5984/hakase_db
</code></pre><p>通过带密码的方式创建新的数据库</p>
<pre><code>curl -X PUT curl -X PUT http://账户名:密码@localhost:5984/hakase_db
</code></pre><p>本文我们就主要讲解一下如何安装及简单使用CouchDB，更多的原理及使用细节，在后续的文章中将逐步呈现。</p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;CouchDB 是一个开源的面向文档的数据库管理系统，它具有高度可伸缩性，提供了高可用性和高可靠性。&lt;/p&gt;
&lt;p&gt;CouchDB发布于2005年，2008年成为Apache软件基金会项目。CouchDB是一个面向文档的NoSQL数据库。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本系列笔者将记录对CouchDB的学习实战相关内容。&lt;/p&gt;
&lt;p&gt;作为本系列第一篇，本文主要介绍如何在centos下搭建CouchDB，以及CouchDB的简单使用。&lt;/p&gt;
    
    </summary>
    
      <category term="CouchDB" scheme="http://wuwenliang.net/categories/CouchDB/"/>
    
    
      <category term="CouchDB" scheme="http://wuwenliang.net/tags/CouchDB/"/>
    
  </entry>
  
  <entry>
    <title>跟我学RocektMQ之理解长轮询机制</title>
    <link href="http://wuwenliang.net/2019/09/22/%E8%B7%9F%E6%88%91%E5%AD%A6RocektMQ%E4%B9%8B%E7%90%86%E8%A7%A3%E9%95%BF%E8%BD%AE%E8%AF%A2%E6%9C%BA%E5%88%B6/"/>
    <id>http://wuwenliang.net/2019/09/22/跟我学RocektMQ之理解长轮询机制/</id>
    <published>2019-09-22T08:35:26.000Z</published>
    <updated>2019-09-24T03:34:02.409Z</updated>
    
    <content type="html"><![CDATA[<p>RoceketMQ提供了两种消息消费者，DefaultMQPushConsumer、DefaultMQPullConsumer。我们都知道DefaultMQPullConsumer是基于拉模式的消费，而DefaultMQPushConsumer是基于推模式的消费。我们先简单复习一下推拉模式的概念。</p>
<blockquote>
<p>推模式：当服务端有数据立即通知客户端，这个策略依赖服务端与客户端之间的长连接，它具有高实时性、客户端开发简单等优点；同时缺点也很明显，比如：服务端需要感知与它建立链接的客户端，要实现客户端节点的发现，服务端本身主动推送，需要服务端对消息做额外的处理，以便能够及时将消息分发给客户端。</p>
<p>拉模式：客户端主动对服务端的数据进行拉取。客户端拉取数据，拉取成功后处理数据，处理完成再次进行拉取，循环执行。缺点是如果不能很好的设置拉取的频率，时间间隔，过多的空轮询会对服务端造成较大的访问压力，数据的实时性也不能得到很好的保证。</p>
</blockquote>
<p>基于对上述两个策略的优缺点的综合考虑，RocketMQ的DefaultMQPushConsumer采用了结合了推拉模式两者优点的长轮询机制，对消息进行消费。这样，既能保证主动权在客户端，还能保证数据拉取的实时性。</p>
<p>本文我们就对RocketMQ的长轮询机制进行分析讲解，从而更好的理解RocketMQ的设计精巧之处。</p>
<p>首先了解一下什么是 <strong>长轮询</strong> 机制：</p>
<a id="more"></a>
<h2 id="什么是“长轮询”机制"><a href="#什么是“长轮询”机制" class="headerlink" title="什么是“长轮询”机制"></a>什么是“长轮询”机制</h2><blockquote>
<p>长轮询机制，顾名思义，它不同于常规轮询方式。常规的轮询方式为客户端发起请求，服务端接收后该请求后立即进行相应的方式。</p>
<p>长轮询本质上仍旧是轮询，它与轮询不同之处在于，当服务端接收到客户端的请求后，服务端不会立即将数据返回给客户端，而是会先将这个请求hold住，判断服务器端数据是否有更新。如果有更新，则对客户端进行响应，如果一直没有数据，则它会在长轮询超时时间之前一直hold住请求并检测是否有数据更新，直到有数据或者超时后才返回。</p>
</blockquote>
<h2 id="RocketMQ如何实现长轮询–客户端实现"><a href="#RocketMQ如何实现长轮询–客户端实现" class="headerlink" title="RocketMQ如何实现长轮询–客户端实现"></a>RocketMQ如何实现长轮询–客户端实现</h2><p>了解了长轮询机制的概念，我们就容易理解RocketMQ对长轮询机制的应用了。请跟随笔者的思路，进入到源码中一探究竟。</p>
<p>首先复习一下客户端如何进行消息拉取：</p>
<p>从上文中，我们已经得知，DefaultMQPushConsumer应用了长轮询机制，从之前的源码分析文章中，我们知道RocketMQ消息拉取是通过消息拉取线程PullMessageService实现的，关于这部分的逻辑可以移步 <a href="http://wuwenliang.net/2019/08/20/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ%E4%B9%8B%E6%B6%88%E6%81%AF%E6%8B%89%E5%8F%96%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/">跟我学RocketMQ之消息拉取源码解析</a>。</p>
<p>我们进入PullMessageService类，重点看它的 <strong>run()</strong> 方法。</p>
<pre><code>[PullMessageService.java]
@Override
public void run() {
    log.info(this.getServiceName() + &quot; service started&quot;);

    while (!this.isStopped()) {
        try {
            PullRequest pullRequest = this.pullRequestQueue.take();
            this.pullMessage(pullRequest);
        } catch (InterruptedException ignored) {
        } catch (Exception e) {
            log.error(&quot;Pull Message Service Run Method exception&quot;, e);
        }
    }

    log.info(this.getServiceName() + &quot; service end&quot;);
}
</code></pre><p>当broker启动后，会在启动MQClientInstance过程中启动PullMessageService，当PullMessageService启动后一直执行run方法进行消息拉取（只要stopped == false）。</p>
<p>回顾一下PullRequest的结构：</p>
<pre><code>public class PullRequest {
    // 消费者组
    private String consumerGroup;
    // 待拉取到消息队列
    private MessageQueue messageQueue;
    // 消息处理队列，消息从broker中拉取以后会先存到该ProcessQueue中，然后再提交给消费者线程池进行消费
    private ProcessQueue processQueue;
    // 带拉取消息的偏移量
    private long nextOffset;
    // 是否锁定
    private boolean lockedFirst = false;
</code></pre><p>对于每个MessageQueue，都有对应的一个pullRequest，每个MessageQueue还对应一个processQueue，保存该MessageQueue消息处理的快照；通过nextOffset来标识当前读取的位置。</p>
<p>消息拉取最终是由PullAPIWrapper.java执行的，在它的pullKernelImpl()方法中，真正的消息拉取逻辑如下：</p>
<pre><code>[PullAPIWrapper.java.pullKernelImpl()]

// 组装消息拉取请求头
PullMessageRequestHeader requestHeader = new PullMessageRequestHeader();
requestHeader.setConsumerGroup(this.consumerGroup);
requestHeader.setTopic(mq.getTopic());
requestHeader.setQueueId(mq.getQueueId());
requestHeader.setQueueOffset(offset);
requestHeader.setMaxMsgNums(maxNums);
requestHeader.setSysFlag(sysFlagInner);
requestHeader.setCommitOffset(commitOffset);
// 设置broker最大阻塞时间，默认为15秒，BROKER_SUSPEND_MAX_TIME_MILLIS = 1000 * 15;
requestHeader.setSuspendTimeoutMillis(brokerSuspendMaxTimeMillis);
requestHeader.setSubscription(subExpression);
requestHeader.setSubVersion(subVersion);
requestHeader.setExpressionType(expressionType);

// 获取拉取broker地址
String brokerAddr = findBrokerResult.getBrokerAddr();
if (PullSysFlag.hasClassFilterFlag(sysFlagInner)) {
    brokerAddr = computPullFromWhichFilterServer(mq.getTopic(), brokerAddr);
}

// 执行消息拉取
PullResult pullResult = this.mQClientFactory.getMQClientAPIImpl().pullMessage(
    brokerAddr,
    requestHeader,
    timeoutMillis,
    communicationMode,
    pullCallback);
return pullResult;
</code></pre><p>这里的参数brokerSuspendMaxTimeMillis（默认值为15s）代表进行消息拉取时，broker的最长阻塞时间。</p>
<p>当进行消息拉取时，如果broker端没有消息，则进行阻塞，否则会对消息体进行打包并直接返回。</p>
<h2 id="RocketMQ如何实现长轮询–服务端实现"><a href="#RocketMQ如何实现长轮询–服务端实现" class="headerlink" title="RocketMQ如何实现长轮询–服务端实现"></a>RocketMQ如何实现长轮询–服务端实现</h2><p>RocketMQ的长轮询是在broker上实现的，具体的代码实现在PullMessageProcessor中。我们进入代码中一窥芳容。</p>
<p>它的启动链路如下：</p>
<pre><code>BrokerStartup
    |-start()
        |-createBrokerController(String[] args) 
            |-BrokerController() // BrokerController构造方法
            |-new PullMessageProcessor(this);
</code></pre><p>当broker启动完成之后，PullMessageProcessor便能够被远程的消费者访问到，通过网络进行消息拉取调用操作。</p>
<p>我们重点看方法processRequest，它是消息拉取网络交互的核心方法。</p>
<h3 id="processRequest"><a href="#processRequest" class="headerlink" title="processRequest()"></a>processRequest()</h3><blockquote>
<p>processRequest为broker对外提供消息拉取的服务方法，它提供针对不同拉取结果的处理逻辑。</p>
</blockquote>
<pre><code>[PullMessageProcessor.java.processRequest]
// 根据客户端发送的拉取消息头，构建拉取结果响应体
RemotingCommand response = RemotingCommand.createResponseCommand(PullMessageResponseHeader.class);
...各种前置校验...
// 从请求头中取出消费者组、主题、队列id、offset、消息最大拉取条数、过滤条件等，去commitLog中查找对应的消息
switch (getMessageResult.getStatus()) {
        case FOUND:
            response.setCode(ResponseCode.SUCCESS);
            break;
        case MESSAGE_WAS_REMOVING:
            response.setCode(ResponseCode.PULL_RETRY_IMMEDIATELY);
            break;
...省略其他case分支...

// 根据上面拉取结果中设置的code进行处理           
switch (response.getCode()) {
    ...省略其他case分支...
    case ResponseCode.PULL_NOT_FOUND:

            if (brokerAllowSuspend &amp;&amp; hasSuspendFlag) {
                long pollingTimeMills = suspendTimeoutMillisLong;
                if (!this.brokerController.getBrokerConfig().isLongPollingEnable()) {
                    pollingTimeMills = this.brokerController.getBrokerConfig().getShortPollingTimeMills();
                }

                String topic = requestHeader.getTopic();
                long offset = requestHeader.getQueueOffset();
                int queueId = requestHeader.getQueueId();
                PullRequest pullRequest = new PullRequest(request, channel, pollingTimeMills,
                    this.brokerController.getMessageStore().now(), offset, subscriptionData, messageFilter);
                this.brokerController.getPullRequestHoldService().suspendPullRequest(topic, queueId, pullRequest);
                response = null;
                break;
            }
</code></pre><p>对于ResponseCode.SUCCESS的拉取响应码，RocektMQ将消息拉取结果以byte数组形式设置到拉取响应中，并会返回给客户端；我们重点关注 <strong>ResponseCode.PULL_NOT_FOUND</strong> 类型，即 <strong>当前未拉取到消息</strong>。</p>
<p>对于ResponseCode.PULL_NOT_FOUND类型，RocketMQ会调用PullRequestHoldService将请求holkd住，不会返回客户端响应，这里就是长轮询的核心逻辑，代码如下：</p>
<pre><code>case ResponseCode.PULL_NOT_FOUND:
    // 判断broker是否允许被挂起
    if (brokerAllowSuspend &amp;&amp; hasSuspendFlag) {
        // 获取长轮询超时时长
        long pollingTimeMills = suspendTimeoutMillisLong;
        // 如果长轮询支持未开启，则pollingTimeMills为短轮询时间，ShortPollingTimeMills默认为1秒
        if (!this.brokerController.getBrokerConfig().isLongPollingEnable()) {
            pollingTimeMills = this.brokerController.getBrokerConfig().getShortPollingTimeMills();
        }

        String topic = requestHeader.getTopic();
        long offset = requestHeader.getQueueOffset();
        int queueId = requestHeader.getQueueId();
        // 根据入参request，Nio的channel，轮询时间，当前消息存储时间戳，消息拉取offset，订阅信息，消息过滤表达式等信息构建长轮询拉取请求
        PullRequest pullRequest = new PullRequest(request, channel, pollingTimeMills,
            this.brokerController.getMessageStore().now(), offset, subscriptionData, messageFilter);
        // 通过PullRequestHoldService对拉取请求进行hold，使用pullRequest对指定topic、queueId的队列进行长轮询消息拉取
        this.brokerController.getPullRequestHoldService().suspendPullRequest(topic, queueId, pullRequest);
        // 设置拉取返回为null，不对客户端进行返回
        response = null;
        break;
    }
</code></pre><p>我们总结一下这里的逻辑：</p>
<ul>
<li>首先判断broker是否允许被hold，如果允许则执行长轮询业务逻辑</li>
<li>获取长轮询超时时长，该参数可配置，如果长轮询支持未开启则改用短轮询时间，默认为1s</li>
<li>从消息拉取请求头中获取topic、队列offset、队列id</li>
<li>构造长轮询消息拉取请求对象PullRequest</li>
<li>调用PullRequestHoldService进行长轮询操作</li>
<li>拉取返回为空，在超时之前不对客户端进行返回</li>
</ul>
<h2 id="PullRequestHoldService核心逻辑"><a href="#PullRequestHoldService核心逻辑" class="headerlink" title="PullRequestHoldService核心逻辑"></a>PullRequestHoldService核心逻辑</h2><p>从上面的分析我们得知，长轮询真正的执行者为PullRequestHoldService，我们看下这个类的代码，PullRequestHoldService继承了ServiceThread，我们重点关注其run方法。</p>
<pre><code>@Override
public void run() {
    log.info(&quot;{} service started&quot;, this.getServiceName());
    while (!this.isStopped()) {
        try {
            // 如果支持长轮询，则等待5秒
            if (this.brokerController.getBrokerConfig().isLongPollingEnable()) {
                this.waitForRunning(5 * 1000);
            } else {
                // 短轮询则默认等待1s
                this.waitForRunning(this.brokerController.getBrokerConfig().getShortPollingTimeMills());
            }

            long beginLockTimestamp = this.systemClock.now();
            // 检测hold请求
            this.checkHoldRequest();
            // 如果检测花费时间超过5s打印日志
            long costTime = this.systemClock.now() - beginLockTimestamp;
            if (costTime &gt; 5 * 1000) {
                log.info(&quot;[NOTIFYME] check hold request cost {} ms.&quot;, costTime);
            }
        } catch (Throwable e) {
            log.warn(this.getServiceName() + &quot; service has exception. &quot;, e);
        }
    }
    log.info(&quot;{} service end&quot;, this.getServiceName());
}
</code></pre><p>run方法不断检测被hold住的请求，它不断检查是否有消息获取成功。检测方法通过执行方法suspendPullRequest实现</p>
<pre><code>private ConcurrentMap&lt;String/* topic@queueId */, ManyPullRequest&gt; pullRequestTable =
    new ConcurrentHashMap&lt;String, ManyPullRequest&gt;(1024);

public void suspendPullRequest(final String topic, final int queueId, final PullRequest pullRequest) {
    String key = this.buildKey(topic, queueId);
    // 从pullRequestTable中获取对应topic+queueId下的拉取请求ManyPullRequest
    ManyPullRequest mpr = this.pullRequestTable.get(key);
    if (null == mpr) {
        mpr = new ManyPullRequest();
        ManyPullRequest prev = this.pullRequestTable.putIfAbsent(key, mpr);
        if (prev != null) {
            mpr = prev;
        }
    }
    // 将等待检测的pullRequest添加到ManyPullRequest中
    mpr.addPullRequest(pullRequest);
}
</code></pre><p>注意，这里的ManyPullRequest对象实际上是一组PullRequest的集合，它封装了一个topic+queueId下的一批消息。</p>
<p>具体的检测逻辑通过方法checkHoldRequest()实现。</p>
<pre><code>private void checkHoldRequest() {
    // 迭代PullRequest Map，key=topic@queueId
    for (String key : this.pullRequestTable.keySet()) {
        // 解析出topic  queueId
        String[] kArray = key.split(TOPIC_QUEUEID_SEPARATOR);
        if (2 == kArray.length) {
            String topic = kArray[0];
            int queueId = Integer.parseInt(kArray[1]);
            // 获取当前获取的数据的最大offset
            final long offset = this.brokerController.getMessageStore().getMaxOffsetInQueue(topic, queueId);
            try {
                // 通知消息到达
                this.notifyMessageArriving(topic, queueId, offset);
            } catch (Throwable e) {
                log.error(&quot;check hold request failed. topic={}, queueId={}&quot;, topic, queueId, e);
            }
        }
    }
}
</code></pre><p>checkHoldRequest()方法解析pullRequestTable的keySet，对key进行解析，取出topic及queueId，获取topic+queueId对应的当前MessageQueue的最大offset，并与当前的offset对比从而确定是否有新消息到达，具体逻辑在notifyMessageArriving(topic, queueId, offset);方法中实现</p>
<blockquote>
<p>这里的检测逻辑整体是异步的，后台检测线程PullRequestHoldService一直在运行；在PullMessageProcessor中提交待检测的PullRequest到PullRequestHoldService，将其放入pullRequestTable，等待被PullRequestHoldService进行处理。</p>
</blockquote>
<h3 id="notifyMessageArriving-topic-queueId-offset"><a href="#notifyMessageArriving-topic-queueId-offset" class="headerlink" title="notifyMessageArriving(topic, queueId, offset)"></a>notifyMessageArriving(topic, queueId, offset)</h3><pre><code>public void notifyMessageArriving(final String topic, final int queueId, final long maxOffset, final Long tagsCode,
    long msgStoreTime, byte[] filterBitMap, Map&lt;String, String&gt; properties) {
    String key = this.buildKey(topic, queueId);
    ManyPullRequest mpr = this.pullRequestTable.get(key);
    if (mpr != null) {
        // 根据key=topic@queueId从pullRequestTable获取ManyPullRequest
        // 如果ManyPullRequest不为空，拷贝ManyPullRequest中的List&lt;PullRequest&gt;
        List&lt;PullRequest&gt; requestList = mpr.cloneListAndClear();
        if (requestList != null) {
            // 构造响应list
            List&lt;PullRequest&gt; replayList = new ArrayList&lt;PullRequest&gt;();
            // 迭代请求list
            for (PullRequest request : requestList) {
                long newestOffset = maxOffset;
                // 如果当前最新的offset小于等于请求的offset
                if (newestOffset &lt;= request.getPullFromThisOffset()) {
                    // 当前最新的offset就是队列的最大offset
                    newestOffset = this.brokerController.getMessageStore().getMaxOffsetInQueue(topic, queueId);
                }
                // 如果当前最新offset大于请求offset，也就是有新消息到来
                if (newestOffset &gt; request.getPullFromThisOffset()) {
                    // 判断消息是否满足过滤表达式
                    boolean match = request.getMessageFilter().isMatchedByConsumeQueue(tagsCode,
                        new ConsumeQueueExt.CqExtUnit(tagsCode, msgStoreTime, filterBitMap));
                    // match by bit map, need eval again when properties is not null.
                    if (match &amp;&amp; properties != null) {
                        match = request.getMessageFilter().isMatchedByCommitLog(null, properties);
                    }
                    if (match) {
                        try {
                            // 消息匹配，则将消息返回客户端
                            this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(),
                                request.getRequestCommand());
                        } catch (Throwable e) {
                            log.error(&quot;execute request when wakeup failed.&quot;, e);
                        }
                        continue;
                    }
                }
                // 判断是否超时
                if (System.currentTimeMillis() &gt;= (request.getSuspendTimestamp() + request.getTimeoutMillis())) {
                    try {
                        // 如果当前时间 &gt;= 请求超时时间+hold时间，则返回客户端消息未找到
                        this.brokerController.getPullMessageProcessor().executeRequestWhenWakeup(request.getClientChannel(),
                            request.getRequestCommand());
                    } catch (Throwable e) {
                        log.error(&quot;execute request when wakeup failed.&quot;, e);
                    }
                    continue;
                }
                replayList.add(request);
            }
            if (!replayList.isEmpty()) {
                mpr.addPullRequest(replayList);
            }
        }
    }
}
</code></pre><p>总结一下，notifyMessageArriving主要作用为判断消息是否到来，并根据判断结果对客户端进行相应。</p>
<ul>
<li>比较maxOffset与当前的offset，如果当前最新offset大于请求offset，也就是有新消息到来，则将新消息返回给客户端</li>
<li>校验是否超时，如果当前时间 &gt;= 请求超时时间+hold阻塞时间，则返回客户端消息未找到</li>
</ul>
<blockquote>
<p>该方法会在PullRequestHoldService中循环调用进行检查，也会在DefaultMessageStore中消息被存储的时候调用。这里体现了主动检查与被动通知共同作用的思路。</p>
</blockquote>
<p>当服务端处理完成之后，相应客户端，客户端会在消息处理完成之后再次将拉取请求pullRequest放到PullMessageService中，等待下次轮询。这样就能够一直进行消息拉取操作。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文对RocketMQ消息拉取的长轮询机制进行了分支，我们得知：</p>
<blockquote>
<p>RocektMQ并没有使用推模式或者拉模式，而是使用了结合两者优点的长轮询机制，它本质上还是拉模式，但服务端能够通过hold住请求的方式减少客户端对服务端的频繁访问，从而提高资源利用率及消息响应实时性。这种策略在服务端开发的其他方向如：IM等领域都有广泛的实践，因此了解它的原理是有必要的。</p>
</blockquote>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RoceketMQ提供了两种消息消费者，DefaultMQPushConsumer、DefaultMQPullConsumer。我们都知道DefaultMQPullConsumer是基于拉模式的消费，而DefaultMQPushConsumer是基于推模式的消费。我们先简单复习一下推拉模式的概念。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;推模式：当服务端有数据立即通知客户端，这个策略依赖服务端与客户端之间的长连接，它具有高实时性、客户端开发简单等优点；同时缺点也很明显，比如：服务端需要感知与它建立链接的客户端，要实现客户端节点的发现，服务端本身主动推送，需要服务端对消息做额外的处理，以便能够及时将消息分发给客户端。&lt;/p&gt;
&lt;p&gt;拉模式：客户端主动对服务端的数据进行拉取。客户端拉取数据，拉取成功后处理数据，处理完成再次进行拉取，循环执行。缺点是如果不能很好的设置拉取的频率，时间间隔，过多的空轮询会对服务端造成较大的访问压力，数据的实时性也不能得到很好的保证。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;基于对上述两个策略的优缺点的综合考虑，RocketMQ的DefaultMQPushConsumer采用了结合了推拉模式两者优点的长轮询机制，对消息进行消费。这样，既能保证主动权在客户端，还能保证数据拉取的实时性。&lt;/p&gt;
&lt;p&gt;本文我们就对RocketMQ的长轮询机制进行分析讲解，从而更好的理解RocketMQ的设计精巧之处。&lt;/p&gt;
&lt;p&gt;首先了解一下什么是 &lt;strong&gt;长轮询&lt;/strong&gt; 机制：&lt;/p&gt;
    
    </summary>
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/categories/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/tags/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
  </entry>
  
  <entry>
    <title>跟我学RocketMQ之定时消息源码解析</title>
    <link href="http://wuwenliang.net/2019/09/15/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ%E4%B9%8B%E5%AE%9A%E6%97%B6%E6%B6%88%E6%81%AF%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    <id>http://wuwenliang.net/2019/09/15/跟我学RocketMQ之定时消息源码解析/</id>
    <published>2019-09-15T11:13:15.000Z</published>
    <updated>2019-09-16T09:34:44.924Z</updated>
    
    <content type="html"><![CDATA[<p>本文我们单独对RocketMQ的定时消息进行源码解析。</p>
<p>同事务消息类似，RocketMQ定时消息也是通过Topic替换，后台线程异步发送实现的。具体逻辑是通过org.apache.rocketmq.store.schedule.ScheduleMessageService实现的。</p>
<h2 id="定时消息原理概述"><a href="#定时消息原理概述" class="headerlink" title="定时消息原理概述"></a>定时消息原理概述</h2><p>在正式进行源码分析之前，我们先从概念上对定时消息做一个较为宏观的认知。</p>
<blockquote>
<p>RocketMQ支持指定级别的消息延迟，默认为1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h。</p>
<p>RocketMQ消息重试以及定时消息均是通过定时任务实现的。重试消息以及定时消息在存入commitLog之前会判断重试次数，如果大于0，则会将消息的topic设置为SCHEDULE_TOPIC_XXXX。</p>
<p>ScheduleMessageService在实例化之后会对SCHEDULE_TOPIC_XXXX主题下的消息进行定时调度，从而实现定时投递。</p>
</blockquote>
<a id="more"></a>
<h2 id="ScheduleMessageService源码解析"><a href="#ScheduleMessageService源码解析" class="headerlink" title="ScheduleMessageService源码解析"></a>ScheduleMessageService源码解析</h2><p>我们接着对ScheduleMessageService进行解析，了解RocketMQ具体是如何实现定时消息机制的。</p>
<h3 id="重要变量"><a href="#重要变量" class="headerlink" title="重要变量"></a>重要变量</h3><p>在正式分析之前，先对ScheduleMessageService的重要成员变量做一下了解：</p>
<blockquote>
<p>delayLevelTable,记录了对延迟级别的解析结果，key=延迟级别，value=对应延迟级别的毫秒数</p>
</blockquote>
<pre><code>private final ConcurrentMap&lt;Integer /* level */, Long/* delay timeMillis */&gt; delayLevelTable =
    new ConcurrentHashMap&lt;Integer, Long&gt;(32);
</code></pre><blockquote>
<p>offsetTable,延迟级别对应的消费进度，key=延迟级别，value=对应延迟级别下的消费进度</p>
</blockquote>
<pre><code>private final ConcurrentMap&lt;Integer /* level */, Long/* offset */&gt; offsetTable =
    new ConcurrentHashMap&lt;Integer, Long&gt;(32);
</code></pre><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>ScheduleMessageService的初始化是在DefaultMessageStore实现的，具体的调用链如下：</p>
<pre><code>BrokerStartup
    |-main
        |-start
            |-createBrokerController
                |-BrokerController.initialize()    
                |-controller.start()
                    |-DefaultMessageStore.start()
                        |-new ScheduleMessageService(this)
                        |-scheduleMessageService.start()
</code></pre><p>从调用链可以看出，当broker启动完成，ScheduleMessageService就开始对定时消息进行调度。</p>
<p>对于ScheduleMessageService我们主要关注：</p>
<ul>
<li>load()方法</li>
<li>start()方法</li>
</ul>
<h3 id="ScheduleMessageService-load"><a href="#ScheduleMessageService-load" class="headerlink" title="ScheduleMessageService.load()"></a>ScheduleMessageService.load()</h3><p>首先关注一下load()方法逻辑。</p>
<pre><code>[ScheduleMessageService.java]
public boolean load() {
    boolean result = super.load();
    result = result &amp;&amp; this.parseDelayLevel();
    return result;
}
</code></pre><p>load()方法的逻辑比较清晰，它的主要职责为：</p>
<ol>
<li>通过super.load()方法获取配置文件,加载延迟消息的消费进度</li>
<li>初始化delayLevelTable</li>
</ol>
<blockquote>
<p>RocketMQ将延时消息的消费进度存储于 ${RocketMQ_Home}/store/config/delayOffset.json下。</p>
</blockquote>
<p>我们重点看一下parseDelayLevel();如何完成解析延时配置，并组装为delayLevelTable的。</p>
<pre><code>[ScheduleMessageService.java]
public boolean parseDelayLevel() {
    // 初始化一个时间单位map，key为秒、分、时、天；value为对应单位的毫秒数
    HashMap&lt;String, Long&gt; timeUnitTable = new HashMap&lt;String, Long&gt;();
    timeUnitTable.put(&quot;s&quot;, 1000L);
    timeUnitTable.put(&quot;m&quot;, 1000L * 60);
    timeUnitTable.put(&quot;h&quot;, 1000L * 60 * 60);
    timeUnitTable.put(&quot;d&quot;, 1000L * 60 * 60 * 24);

    // 从defaultMessageStore中获取配置文件，从配置文件中获取延迟级别配置串，即：messageDelayLevel
    String levelString = this.defaultMessageStore.getMessageStoreConfig().getMessageDelayLevel();
    try {

        // 根据空格进行拆分，分解为String数组
        String[] levelArray = levelString.split(&quot; &quot;);

        // 遍历String数组
        for (int i = 0; i &lt; levelArray.length; i++) {
            String value = levelArray[i];
            String ch = value.substring(value.length() - 1);
            Long tu = timeUnitTable.get(ch);

            // key=延迟级别，等于下标+1
            int level = i + 1;
            if (level &gt; this.maxDelayLevel) {
                this.maxDelayLevel = level;
            }
            long num = Long.parseLong(value.substring(0, value.length() - 1));
            // value=单位对应毫秒数 * 解析得到的时间单位
            long delayTimeMillis = tu * num;
            // 存放到delayLevelTable
            this.delayLevelTable.put(level, delayTimeMillis);
        }
    } catch (Exception e) {
        log.error(&quot;parseDelayLevel exception&quot;, e);
        log.info(&quot;levelString String = {}&quot;, levelString);
        return false;
    }
    return true;
}
</code></pre><p>这段代码很好理解，就是对配置中的延时串通过空格进行分割为数组，按照下标及单位，计算得到每个等级对应的毫秒数，最终存放在delayLevelTable中实现delayLevelTable的初始化，便于后续在代码逻辑中进行使用。</p>
<p>如果没有设置则使用代码中的默认值。</p>
<h3 id="ScheduleMessageService-start"><a href="#ScheduleMessageService-start" class="headerlink" title="ScheduleMessageService.start()"></a>ScheduleMessageService.start()</h3><p>我们接着看一下start()方法的逻辑，该方法是延迟消息(定时消息)调度的核心逻辑。</p>
<pre><code>[ScheduleMessageService.java]
public void start() {
    if (started.compareAndSet(false, true)) {
        this.timer = new Timer(&quot;ScheduleMessageTimerThread&quot;, true);
</code></pre><p>start方法的核心思想为</p>
<blockquote>
<p>对不同的延迟级别创建对应的定时任务，通过定时任务对持久化的消息队列的进度进行存储。</p>
</blockquote>
<pre><code>// 首先对delayLevelTable进行迭代，取出每一个级别及其对应的延时长度。
for (Map.Entry&lt;Integer, Long&gt; entry : this.delayLevelTable.entrySet()) {
    Integer level = entry.getKey();
    Long timeDelay = entry.getValue();
    Long offset = this.offsetTable.get(level);
    // 获取该级别对应的消费进度offset，如果不存在则设置为0
    if (null == offset) {
        offset = 0L;
    }

    // 如果延时不为空，则延迟1秒执行定时任务
    if (timeDelay != null) {
        this.timer.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME);
    }
}
</code></pre><p>这里简单总结一下，首先对delayLevelTable进行遍历，获取对应延迟级别level对应的消费进度，默认进度不存在，每个延迟级别对应的消费进度都从0开始。</p>
<p>创建定时任务开始进行调度，每个定时任务初始都延迟1秒开始进行调度。后续则使用对应的延迟级别进行调度。</p>
<blockquote>
<p>注意：延时级别与消费队列的关系为：消息队列id=延时级别-1，具体逻辑在queueId2DelayLevel方法中。</p>
</blockquote>
<pre><code>        this.timer.scheduleAtFixedRate(new TimerTask() {

            @Override
            public void run() {
                try {
                    if (started.get()) ScheduleMessageService.this.persist();
                } catch (Throwable e) {
                    log.error(&quot;scheduleAtFixedRate flush exception&quot;, e);
                }
            }
        }, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval());
    }
}
</code></pre><p>这段代码的核心逻辑为，执行定时任务，每隔10s进行一次消费进度的持久化操作。具体的持久化刷盘频率可以通过flushDelayOffsetInterval参数进行配置。</p>
<h3 id="定时任务实现：DeliverDelayedMessageTimerTask"><a href="#定时任务实现：DeliverDelayedMessageTimerTask" class="headerlink" title="定时任务实现：DeliverDelayedMessageTimerTask"></a>定时任务实现：DeliverDelayedMessageTimerTask</h3><p>上面的分析中我们得知，RocketMQ对定时消息的每一个延迟级别都设置了一个定时任务，这个定时任务识通过DeliverDelayedMessageTimerTask实现的。</p>
<p>DeliverDelayedMessageTimerTask继承了TimerTask，我们直接看它的run()方法实现。</p>
<pre><code>@Override
public void run() {
    try {
        if (isStarted()) {
            this.executeOnTimeup();
        }
    } catch (Exception e) {
        // XXX: warn and notify me
        log.error(&quot;ScheduleMessageService, executeOnTimeup exception&quot;, e);
        ScheduleMessageService.this.timer.schedule(new DeliverDelayedMessageTimerTask(
            this.delayLevel, this.offset), DELAY_FOR_A_PERIOD);
    }
}
</code></pre><p>可以看到，核心是executeOnTimeup()方法，当执行异常，延迟10s后继续执行调度。</p>
<p>我们进入executeOnTimeup()方法。</p>
<h4 id="executeOnTimeup"><a href="#executeOnTimeup" class="headerlink" title="executeOnTimeup()"></a>executeOnTimeup()</h4><p>首先根据topic=SCHEDULE_TOPIC_XXXX，延迟级别转换为队列id，查询到当前的消费队列。</p>
<pre><code>ConsumeQueue cq =
    ScheduleMessageService.this.defaultMessageStore.findConsumeQueue(SCHEDULE_TOPIC,
        delayLevel2QueueId(delayLevel));
</code></pre><p>根据当前的offset从消费队列中获取当前所有的有效消息，如果未能获取到则更新拉取进度，等待定时任务下次进行尝试。</p>
<pre><code>for (; i &lt; bufferCQ.getSize(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE) {
    long offsetPy = bufferCQ.getByteBuffer().getLong();
    int sizePy = bufferCQ.getByteBuffer().getInt();
    long tagsCode = bufferCQ.getByteBuffer().getLong();

    if (cq.isExtAddr(tagsCode)) {
        if (cq.getExt(tagsCode, cqExtUnit)) {
            tagsCode = cqExtUnit.getTagsCode();
        } else {
            //can&apos;t find ext content.So re compute tags code.
            log.error(&quot;[BUG] can&apos;t find consume queue extend file content!addr={}, offsetPy={}, sizePy={}&quot;,
                tagsCode, offsetPy, sizePy);
            long msgStoreTime = defaultMessageStore.getCommitLog().pickupStoreTimestamp(offsetPy, sizePy);
            tagsCode = computeDeliverTimestamp(delayLevel, msgStoreTime);
        }
    }
</code></pre><p>定时任务每次执行到这里都进行时间比较，计算延迟时间与当前时间的差值，如果延迟时间-当前时间&lt;=0说明该延迟消息应当被处理，使其能够被消费者消费。</p>
<pre><code>long now = System.currentTimeMillis();
long deliverTimestamp = this.correctDeliverTimestamp(now, tagsCode);

nextOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE);
</code></pre><p>根据消息偏移量及消息大小从commitLog中查询消息，如果查到，则开始执行正式的消息消费准备工作。</p>
<pre><code>if (countdown &lt;= 0) {
    MessageExt msgExt =
         ScheduleMessageService.
            this.defaultMessageStore.lookMessageByOffset(offsetPy, sizePy);
</code></pre><p>对消息执行重新存储操作，恢复原先的队列以及消息topic，再将消息重新持久化到commitLog中，此时的消息已经能够被消费者拉取到。</p>
<pre><code>if (msgExt != null) {
         try {
             MessageExtBrokerInner msgInner = this.messageTimeup(msgExt);

            PutMessageResult putMessageResult =
                ScheduleMessageService.this.writeMessageStore
                                    .putMessage(msgInner);
</code></pre><p>我们重点看一下messageTimeup(msgExt)方法是如何进行消息的恢复操作。</p>
<h4 id="messageTimeup-msgExt-恢复原消息主题及队列"><a href="#messageTimeup-msgExt-恢复原消息主题及队列" class="headerlink" title="messageTimeup(msgExt)恢复原消息主题及队列"></a>messageTimeup(msgExt)恢复原消息主题及队列</h4><pre><code>private MessageExtBrokerInner messageTimeup(MessageExt msgExt) {
        // 建立一个新的MessageExtBrokerInner实体
        MessageExtBrokerInner msgInner = new MessageExtBrokerInner();
        msgInner.setBody(msgExt.getBody());
        msgInner.setFlag(msgExt.getFlag());
        MessageAccessor.setProperties(msgInner, msgExt.getProperties());

        ...省略属性设置...

        msgInner.setWaitStoreMsgOK(false);
        // 清理消息延迟级别属性
        MessageAccessor.clearProperty(msgInner, MessageConst.PROPERTY_DELAY_TIME_LEVEL);

        // 恢复消息原主题
        msgInner.setTopic(msgInner.getProperty(MessageConst.PROPERTY_REAL_TOPIC));

        // 恢复消息原队列id
        String queueIdStr = msgInner.getProperty(MessageConst.PROPERTY_REAL_QUEUE_ID);
        int queueId = Integer.parseInt(queueIdStr);
        msgInner.setQueueId(queueId);

        return msgInner;
    }
</code></pre><p>经过上述操作，定时消息已经还原为普通消息。</p>
<p>我们继续回到 <strong>executeOnTimeup()</strong> 方法中，通过</p>
<pre><code>PutMessageResult putMessageResult = 
ScheduleMessageService.this.writeMessageStore.putMessage(msgInner);
</code></pre><p>将还原后的消息重新持久化到commitLog中。</p>
<pre><code>nextOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE);
ScheduleMessageService.this.timer.schedule(new DeliverDelayedMessageTimerTask(
        this.delayLevel, nextOffset), DELAY_FOR_A_WHILE);
ScheduleMessageService.this.updateOffset(this.delayLevel, nextOffset);
</code></pre><p>更新当前延迟队列的消息拉取进度，继续处理后续的消息。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>本文我们完整的对RocketMQ的定时消息实现方式进行了分析，我们总结一下它的完整流程：</p>
<ol>
<li>消息发送方发送消息，设置delayLevel。</li>
<li>如果delayLevel大于0，表明是一条延时消息，broker处理该消息，将消息的主题、队列id进行备份后，改变消息的主题为SCHEDULE_TOPIC_XXXX，队列id=延迟级别-1，将消息持久化。</li>
<li>通过定时任务ScheduleMessageService对定时消息进行处理，每隔1s从上次拉取偏移量取出所有的消息进行处理</li>
<li>从消费队列中解析出消息的物理偏移量，从而从commitLog中取出消息</li>
<li>根据消息的属性重建消息，恢复消息的topic、原队列id，将消息的延迟级别属性delayLevel清除掉，再次保存到commitLog中</li>
<li>将消息转发到原主题对应的消费队列中，此时消费者可以对该消息进行消费。</li>
</ol>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文我们单独对RocketMQ的定时消息进行源码解析。&lt;/p&gt;
&lt;p&gt;同事务消息类似，RocketMQ定时消息也是通过Topic替换，后台线程异步发送实现的。具体逻辑是通过org.apache.rocketmq.store.schedule.ScheduleMessageService实现的。&lt;/p&gt;
&lt;h2 id=&quot;定时消息原理概述&quot;&gt;&lt;a href=&quot;#定时消息原理概述&quot; class=&quot;headerlink&quot; title=&quot;定时消息原理概述&quot;&gt;&lt;/a&gt;定时消息原理概述&lt;/h2&gt;&lt;p&gt;在正式进行源码分析之前，我们先从概念上对定时消息做一个较为宏观的认知。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;RocketMQ支持指定级别的消息延迟，默认为1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h。&lt;/p&gt;
&lt;p&gt;RocketMQ消息重试以及定时消息均是通过定时任务实现的。重试消息以及定时消息在存入commitLog之前会判断重试次数，如果大于0，则会将消息的topic设置为SCHEDULE_TOPIC_XXXX。&lt;/p&gt;
&lt;p&gt;ScheduleMessageService在实例化之后会对SCHEDULE_TOPIC_XXXX主题下的消息进行定时调度，从而实现定时投递。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/categories/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
    
      <category term="跟我学RocketMQ" scheme="http://wuwenliang.net/tags/%E8%B7%9F%E6%88%91%E5%AD%A6RocketMQ/"/>
    
  </entry>
  
  <entry>
    <title>TCC-Transaction源码解析之事务补偿</title>
    <link href="http://wuwenliang.net/2019/09/09/TCC-Transaction%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E4%BA%8B%E5%8A%A1%E8%A1%A5%E5%81%BF/"/>
    <id>http://wuwenliang.net/2019/09/09/TCC-Transaction源码解析之事务补偿/</id>
    <published>2019-09-09T08:57:02.000Z</published>
    <updated>2019-09-09T09:29:20.758Z</updated>
    
    <content type="html"><![CDATA[<p>上文中，我们对TCC-Transaction的事务提交阶段主流程进行了详细的解析。上文遗留了一个问题：</p>
<blockquote>
<p>如果在事务执行过程中出现了down机、停电、网络异常等情况，事务一致性就无法得到保证，此时应该怎么做？</p>
</blockquote>
<p>这个问题在TCC-Transaction框架中是通过定时任务+状态机方式实现的，这种方式也是我们日常开发中经常使用的一种策略。本文，我们对事务恢复主逻辑进行分析，使TCC-Transaction源码解析形成一个闭环。</p>
<a id="more"></a>
<h2 id="RecoverScheduledJob"><a href="#RecoverScheduledJob" class="headerlink" title="RecoverScheduledJob"></a>RecoverScheduledJob</h2><p>事务补偿定时任务的核心逻辑由tcc-transaction-spring模块下的RecoverScheduledJob.java完成，对失败的confirm、cancel操作进行失败补偿操作。代码逻辑如下：</p>
<pre><code>public class RecoverScheduledJob {

    private TransactionRecovery transactionRecovery;

    private TransactionConfigurator transactionConfigurator;

    private Scheduler scheduler;

    // 通过init方法启动quartz定时任务
    public void init() {

        try {
            MethodInvokingJobDetailFactoryBean jobDetail = new MethodInvokingJobDetailFactoryBean();
            jobDetail.setTargetObject(transactionRecovery);
            jobDetail.setTargetMethod(&quot;startRecover&quot;);
            jobDetail.setName(&quot;transactionRecoveryJob&quot;);
            jobDetail.setConcurrent(false);
            jobDetail.afterPropertiesSet();

            CronTriggerFactoryBean cronTrigger = new CronTriggerFactoryBean();
            cronTrigger.setBeanName(&quot;transactionRecoveryCronTrigger&quot;);
            cronTrigger.setCronExpression(transactionConfigurator.getRecoverConfig().getCronExpression());
            cronTrigger.setJobDetail(jobDetail.getObject());
            cronTrigger.afterPropertiesSet();

            scheduler.scheduleJob(jobDetail.getObject(), cronTrigger.getObject());
            scheduler.start();

        } catch (Exception e) {
            throw new SystemException(e);
        }
    }
    ...省略getter setter...
}
</code></pre><p>通过quartz进行任务调度，通过RecoverConfig中的配置初始化定时任务，通过MethodInvokingJobDetailFactoryBean的targetObject与targetMethod指定了定时任务具体执行类及具体方法。</p>
<p>我们注意以下代码，它将任务的核心逻辑设置到jobDetail中</p>
<pre><code>jobDetail.setTargetObject(transactionRecovery);
jobDetail.setTargetMethod(&quot;startRecover&quot;);
</code></pre><p>最终通过quartz的Scheduler对任务发起调度，这里通过cron表达式触发器进行调度。</p>
<h2 id="TransactionRecovery"><a href="#TransactionRecovery" class="headerlink" title="TransactionRecovery"></a>TransactionRecovery</h2><p>TransactionRecovery是TCC-Transaction框架中事务补偿的核心实现</p>
<pre><code>public class TransactionRecovery {

    ......

    private TransactionConfigurator transactionConfigurator;
</code></pre><p>通过startRecover开启事务补偿重试任务。</p>
<pre><code>public void startRecover() {
    // 获取待补偿的任务列表
    List&lt;Transaction&gt; transactions = loadErrorTransactions();
    // 对待补偿的任务列表执行补偿操作
    recoverErrorTransactions(transactions);
}
</code></pre><p>首先通过loadErrorTransactions()获取待补偿的任务列表：</p>
<pre><code>private List&lt;Transaction&gt; loadErrorTransactions() {

    long currentTimeInMillis = Calendar.getInstance().getTimeInMillis();
    // 获取配置的具体事务持久化策略，如：基于数据库、zk、redis等
    TransactionRepository transactionRepository = transactionConfigurator.getTransactionRepository();

    // 获取重试策略，包含：cron表达式，最大重试次数等
    RecoverConfig recoverConfig = transactionConfigurator.getRecoverConfig();

    // 获取在RecoverDuration间隔之前未完成的transaction列表，查询方式依具体的持久化策略而定
    return transactionRepository
        .findAllUnmodifiedSince(
            new Date(currentTimeInMillis 
                - recoverConfig.getRecoverDuration() * 1000));
}
</code></pre><p>接着看一下recoverErrorTransactions方法逻辑，对待补偿的任务列表进行补偿操作。</p>
<pre><code>    private void recoverErrorTransactions(List&lt;Transaction&gt; transactions) {
        // 对需要进行重试的事务列表进行迭代
        for (Transaction transaction : transactions) {

            // 如果重试次数超过配置的最大重试次数，则打印异常日志；跳过不再重试
            if (transaction.getRetriedCount() &gt; 
                transactionConfigurator.getRecoverConfig().getMaxRetryCount()) {
                ...省略异常日志...
                continue;
            }

            // 如果是分支事务，并且超过最长超时时间则忽略不再重试
            if (transaction.getTransactionType().equals(TransactionType.BRANCH)
            &amp;&amp; (transaction.getCreateTime().getTime() 
                  transactionConfigurator.getRecoverConfig().getMaxRetryCount()
                  *transactionConfigurator.getRecoverConfig().getRecoverDuration() 
                  * 1000
                  &gt; System.currentTimeMillis())) {
                continue;
            }

            try {
                // 增加重试次数
                transaction.addRetriedCount();
                // 如果当前事务状态为CONFIRMING
                if (transaction.getStatus().equals(TransactionStatus.CONFIRMING)) {
                    // 设置事务状态为CONFIRMING
                    transaction.changeStatus(TransactionStatus.CONFIRMING);
                    // 修改当前事务状态
                    transactionConfigurator.getTransactionRepository().update(transaction);
                    // 提交事务
                    transaction.commit();
                    // 删除事务记录（删除失败则不作处理）
                    transactionConfigurator.getTransactionRepository().delete(transaction);

                // 如果事务状态为CANCELLING或者事务为根事务（根事务没提交）
                } else if (transaction.getStatus().equals(TransactionStatus.CANCELLING)
                        || transaction.getTransactionType().equals(TransactionType.ROOT)) {

                    // 设置事务状态为CANCELLING
                    transaction.changeStatus(TransactionStatus.CANCELLING);
                    // 修改当前事务状态
                    transactionConfigurator.getTransactionRepository().update(transaction);
                    // 回滚事务
                    transaction.rollback();
                    // 删除事务记录（删除失败则不作处理）
                    transactionConfigurator.getTransactionRepository().delete(transaction);
                }

            } catch (Throwable throwable) {

                ...省略异常日志...
            }
        }
    }

    public void setTransactionConfigurator(TransactionConfigurator transactionConfigurator) {
        this.transactionConfigurator = transactionConfigurator;
    }
}
</code></pre><p>上述注释已经很明确的指出了事务补偿job的核心逻辑，就不再赘述。我们总结一下：</p>
<p>对于trying阶段的异常事务，不会进行重试；而是会触发canceling操作；</p>
<p>对于confirming、canceling阶段的异常事务，定时进行重试补偿，尽最大努力去尝试提交事务，如果达到了最大重试次数还是处理失败则不再处理。这种极端的情况需要人工进行介入。</p>
<p>TCC-Transaction提供的后台server模块允许我们对事务进行手工补偿操作，这极大的提高了框架的可靠性以及易用性。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文我们主要对TCC-Transaction的事务补偿阶段逻辑进行了分析。TCC-Transaction框架通过定时补偿，对异常事务进行处理，保证了分布式事务的最终一致性。</p>
<p>通过这一过程，我们能够看出，TCC-Transaction框架本质上也是柔性事务的一种解决方案，如果仔细分析，它也是满足BASE原则的。</p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上文中，我们对TCC-Transaction的事务提交阶段主流程进行了详细的解析。上文遗留了一个问题：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果在事务执行过程中出现了down机、停电、网络异常等情况，事务一致性就无法得到保证，此时应该怎么做？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这个问题在TCC-Transaction框架中是通过定时任务+状态机方式实现的，这种方式也是我们日常开发中经常使用的一种策略。本文，我们对事务恢复主逻辑进行分析，使TCC-Transaction源码解析形成一个闭环。&lt;/p&gt;
    
    </summary>
    
      <category term="TCC-Transaction源码解析" scheme="http://wuwenliang.net/categories/TCC-Transaction%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
    
      <category term="TCC-Transaction源码解析" scheme="http://wuwenliang.net/tags/TCC-Transaction%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>TCC-Transaction源码解析之事务执行</title>
    <link href="http://wuwenliang.net/2019/09/09/TCC-Transaction%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B9%8B%E4%BA%8B%E5%8A%A1%E6%89%A7%E8%A1%8C/"/>
    <id>http://wuwenliang.net/2019/09/09/TCC-Transaction源码解析之事务执行/</id>
    <published>2019-09-09T05:54:24.000Z</published>
    <updated>2019-09-09T08:55:02.348Z</updated>
    
    <content type="html"><![CDATA[<p>TCC分布式事务解决方案在开源界的主要实现为Byte-TCC、TCC-Transaction等。其中笔者了解较多并且业界使用率较高的为TCC-Transaction这一实现。</p>
<p>本文，我将带领读者对TCC-Transaction这一分布式事务框架进行一次源码解析，提高自己的阅读源码的能力，也希望能够对读者深入了解TCC-Transaction有所帮助。</p>
<h2 id="源码下载"><a href="#源码下载" class="headerlink" title="源码下载"></a>源码下载</h2><p>源码地址为 <a href="https://github.com/changmingxie/tcc-transaction" target="_blank" rel="external">https://github.com/changmingxie/tcc-transaction</a>，我们关注最新版本1.2.x。</p>
<p>源码下载后导入IDEA中，项目目录结构如下图：</p>
<p><img src="/2019/09/09/TCC-Transaction源码解析之事务执行/tcc0.png" alt="代码结构"></p>
<a id="more"></a>
<p>模块及其对应职责说明如下：</p>
<pre><code>tcc-transaction
    |-transaction-tcc-api                   框架API定义，公共类/核心实体定义/枚举/工具类等
    |-transaction-tcc-core                  框架核心逻辑
    |-transaction-tcc-dubbo                 框架整合Dubbo实现
    |-transaction-tcc-spring                框架Spring整合，包含获取数据库连接/切面获取等
    |-transaction-tcc-server                后台管理页面，对事务进行手工重试等
    |-transaction-tcc-unit-test             单元测试
    |-transaction-tcc-tutorial-sample       样例工程
        |-tcc-transaction-dubbo-sample
        |-tcc-transaction-http-sample
        |-tcc-transaction-sample-domain
        |-tcc-transaction-server-sample
</code></pre><p>项目核心模块为 <strong>tcc-transaction-core</strong>，它实现了TCC核心业务逻辑，也是本次源码解析的重点对象。</p>
<p>我们从Dubbo使用样例入手进行分析，关于如何使用TCC-Transaction的更多说明，请参照官方文档: <a href="https://github.com/changmingxie/tcc-transaction/wiki/%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%971.2.x" target="_blank" rel="external">使用指南1.2.x</a></p>
<h2 id="从一个简单的样例入手"><a href="#从一个简单的样例入手" class="headerlink" title="从一个简单的样例入手"></a>从一个简单的样例入手</h2><blockquote>
<p>我们从一个调用案例入手开始进行分析，样例路径为org.mengyun.tcctransaction.sample.dubbo.order.service.PaymentServiceImpl。</p>
</blockquote>
<pre><code>@Compensable(confirmMethod = &quot;confirmMakePayment&quot;, cancelMethod = &quot;cancelMakePayment&quot;, 
asyncConfirm = false, 
delayCancelExceptions = {SocketTimeoutException.class, com.alibaba.dubbo.remoting.TimeoutException.class})
public void makePayment(@UniqueIdentity String orderNo, Order order,
                 BigDecimal redPacketPayAmount, BigDecimal capitalPayAmount) {
    System.out.println(&quot;order try make payment called.time seq:&quot; + 
                DateFormatUtils.format(Calendar.getInstance(), &quot;yyyy-MM-dd HH:mm:ss&quot;));

    //check if the order status is DRAFT, if no, means that another call makePayment 
                for the same order happened, ignore this call makePayment.
    if (order.getStatus().equals(&quot;DRAFT&quot;)) {
        order.pay(redPacketPayAmount, capitalPayAmount);
        try {
            orderRepository.updateOrder(order);
        } catch (OptimisticLockingFailureException e) {
            //ignore the concurrently update order exception, ensure idempotency.
        }
    }

    String result = capitalTradeOrderService.record(buildCapitalTradeOrderDto(order));
    String result2 = redPacketTradeOrderService.record(buildRedPacketTradeOrderDto(order));
}
...省略confirmMakePayment实现...
...省略cancelMakePayment实现...
</code></pre><p>这段代码为模拟支付扣款操作，可以看到在方法上添加了@Compensable注解，它是TCC-Transaction框架的核心注解，作用为：<strong>开启tcc事务支持</strong>，注解可以设置一下参数</p>
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">propagation</td>
<td style="text-align:left">事务传播属性，REQUIRED（必须存在事务，不存在则进行创建），SUPPORTS（如果有事务则在事务内运行），MANDATORY（必须存在事务），REQUIRES_NEW（不管是否存在是否都创建新的事务）</td>
</tr>
<tr>
<td style="text-align:left">confirmMethod</td>
<td style="text-align:left">confirm阶段方法实现</td>
</tr>
<tr>
<td style="text-align:left">cancelMethod</td>
<td style="text-align:left">cancel阶段方法实现</td>
</tr>
<tr>
<td style="text-align:left">transactionContextEditor</td>
<td style="text-align:left">设置transactionContextEditor</td>
</tr>
<tr>
<td style="text-align:left">asyncConfirm</td>
<td style="text-align:left">是否使用异步confirm</td>
</tr>
<tr>
<td style="text-align:left">asyncCancel</td>
<td style="text-align:left">是否使用异步cancel</td>
</tr>
</tbody>
</table>
<h2 id="解析注解-Compensable"><a href="#解析注解-Compensable" class="headerlink" title="解析注解@Compensable"></a>解析注解@Compensable</h2><p>看到了@Compensable注解以及对应的confirm、cancle方法，处于技术敏感，我们可以猜测在框架中一定存在切面逻辑对@Compensable进行拦截并处理；在切面逻辑中一定有对confirm、cancel方法的调用。从这个猜想出发，我们通过阅读相关代码去验证自己的猜想。</p>
<blockquote>
<p>我们进入tcc-transaction-core模块的代码目录，目录结构如下：</p>
</blockquote>
<pre><code>org.mengyun.tcctransaction
    |-common
    |-context
    |-interceptor           TCC事务拦截器
    |-recover               TCC事务补偿
    |-repository            事务存储
    |-serializer
    |-support
    |-utils
</code></pre><p>我们主要关注interceptor目录，该目录下的interceptor实现了对注解@Compensable的解析以及对事务的代理逻辑。</p>
<h3 id="CompensableTransactionAspect"><a href="#CompensableTransactionAspect" class="headerlink" title="CompensableTransactionAspect"></a>CompensableTransactionAspect</h3><blockquote>
<p>CompensableTransactionAspect切面主要实现了对@Compensable的解析以及对事务的代理。</p>
</blockquote>
<pre><code>@Aspect
public abstract class CompensableTransactionAspect {

    private CompensableTransactionInterceptor compensableTransactionInterceptor;

    public void setCompensableTransactionInterceptor(
                CompensableTransactionInterceptor compensableTransactionInterceptor) {
        this.compensableTransactionInterceptor = compensableTransactionInterceptor;
    }

    @Pointcut(&quot;@annotation(org.mengyun.tcctransaction.api.Compensable)&quot;)
    public void compensableService() {

    }

    @Around(&quot;compensableService()&quot;)
    public Object interceptCompensableMethod(ProceedingJoinPoint pjp) throws Throwable {

        return compensableTransactionInterceptor.interceptCompensableMethod(pjp);
    }

    public abstract int getOrder();
}
</code></pre><p>CompensableTransactionAspect的实现类为ConfigurableTransactionAspect.java, 加载顺序order= Ordered.HIGHEST_PRECEDENCE（-2147483648）。</p>
<p>该切面对标注了@Compensable的方法进行拦截，通过@Around为业务方法添加环绕增强。可以看到具体的增强方法实现为CompensableTransactionInterceptor.interceptCompensableMethod(pjp);</p>
<h4 id="CompensableTransactionInterceptor-interceptCompensableMethod-pjp"><a href="#CompensableTransactionInterceptor-interceptCompensableMethod-pjp" class="headerlink" title="CompensableTransactionInterceptor.interceptCompensableMethod(pjp);"></a>CompensableTransactionInterceptor.interceptCompensableMethod(pjp);</h4><p>接着上述的分析，我们看一下CompensableTransactionInterceptor.interceptCompensableMethod(pjp)的逻辑。</p>
<pre><code>[CompensableTransactionInterceptor.java]
public Object interceptCompensableMethod(ProceedingJoinPoint pjp) throws Throwable {

    // 初始化一个TCC方法执行上下文
    CompensableMethodContext compensableMethodContext = new CompensableMethodContext(pjp);
    // 校验事务支持是否开启
    boolean isTransactionActive = transactionManager.isTransactionActive();
    // 校验事务隔离级别
    if (!TransactionUtils.isLegalTransactionContext(
                isTransactionActive, compensableMethodContext)) {
        throw new SystemException
            (&quot;no active compensable transaction while propagation is mandatory for method &quot; 
                    + compensableMethodContext.getMethod().getName());
    }
    // 根据事务方法类型判断执行哪个逻辑
    switch (compensableMethodContext.getMethodRole(isTransactionActive)) {
        case ROOT:
            return rootMethodProceed(compensableMethodContext);
        case PROVIDER:
            return providerMethodProceed(compensableMethodContext);
        default:
            return pjp.proceed();
    }
}
</code></pre><p>我们主要关注switch代码段</p>
<pre><code>switch (compensableMethodContext.getMethodRole(isTransactionActive)) {
    case ROOT:
        //处理主事务切面，即：本次事务的入口方法
        return rootMethodProceed(compensableMethodContext);
    case PROVIDER:
        //处理提供者事务切面
        return providerMethodProceed(compensableMethodContext);
    default:
        //消费者事务直接执行，会对应执行远端提供者事务切面
        return pjp.proceed();
}
</code></pre><p>当事务方法为ROOT方法（即分布式事务的主方法）时，执行rootMethodProceed(compensableMethodContext);方法为PROVIDER（提供者）方法时，执行providerMethodProceed(compensableMethodContext)。默认为消费者事务，则直接执行。</p>
<p>我们以此看一下这几种事务切面的执行逻辑。</p>
<h4 id="rootMethodProceed-compensableMethodContext"><a href="#rootMethodProceed-compensableMethodContext" class="headerlink" title="rootMethodProceed(compensableMethodContext)"></a>rootMethodProceed(compensableMethodContext)</h4><p>对于事务的Root方法，执行rootMethodProceed逻辑，代码逻辑：</p>
<pre><code>private Object rootMethodProceed(CompensableMethodContext compensableMethodContext) 
    throws Throwable {

    Object returnValue = null;

    Transaction transaction = null;

    boolean asyncConfirm = compensableMethodContext.getAnnotation().asyncConfirm();

    boolean asyncCancel = compensableMethodContext.getAnnotation().asyncCancel();

    Set&lt;Class&lt;? extends Exception&gt;&gt; allDelayCancelExceptions = new HashSet&lt;Class&lt;? extends Exception&gt;&gt;();
    allDelayCancelExceptions.addAll(this.delayCancelExceptions);
    allDelayCancelExceptions.addAll(Arrays.asList(compensableMethodContext.getAnnotation().delayCancelExceptions()));

    try {
        // 创建事务， 将主事务的信息写入db或者zk或者redis中去，事务信息写入具体方式可配置
        transaction = transactionManager.begin(compensableMethodContext.getUniqueIdentity());

        try {
            // 执行完成之后会马上进到另外一个切面中去
            returnValue = compensableMethodContext.proceed();
        } catch (Throwable tryingException) {
            // 如果try失败，则进行回滚
            if (!isDelayCancelException(tryingException, allDelayCancelExceptions)) {

                logger.warn(String.format(&quot;compensable transaction trying failed. transaction content:%s&quot;, JSON.toJSONString(transaction)), tryingException);
                // 回滚事务
                transactionManager.rollback(asyncCancel);
            }

            throw tryingException;
        }
        // 提交事务
        transactionManager.commit(asyncConfirm);

    } finally {
        // 最终如果执行成功，则删除之前的事务记录；如果执行失败则不作任何处理，等待job进行补偿操作
        transactionManager.cleanAfterCompletion(transaction);
    }
    return returnValue;
}
</code></pre><p>注意关注这段代码</p>
<pre><code>// 执行完成之后会马上进到另外一个切面中去
returnValue = compensableMethodContext.proceed();
</code></pre><p>当所有的切面都执行完成之后才会执行后续的逻辑，也就是真正执行业务方法。</p>
<p>该方法为一个典型的模板方法，对事务通过begin、commit、rollback进行了抽象。</p>
<p>我们进入三个方法详细的分析。</p>
<h5 id="begin"><a href="#begin" class="headerlink" title="begin()"></a>begin()</h5><p>首先进入begin方法</p>
<pre><code>[TransactionManager.java]
public Transaction begin(Object uniqueIdentify) {
    // 0
    Transaction transaction = new Transaction(uniqueIdentify,TransactionType.ROOT);
    // 1
    transactionRepository.create(transaction);
    // 2
    registerTransaction(transaction);
    return transaction;
}
</code></pre><p>0.首先声明并初始化一个分布式事务对象Transaction，标记为ROOT事务，事务初始状态为TRYING。这里采用了经典的状态机策略</p>
<pre><code>public Transaction(TransactionType transactionType) {
    this.xid = new TransactionXid();
    // 事务初始状态设置成TRYING
    this.status = TransactionStatus.TRYING;
    this.transactionType = transactionType;
}   
</code></pre><p>1.将事务信息存储到数据源中，数据源可以是数据库、redis、zk等，可配置；TransactionRepository是具体的持久化策略的抽象</p>
<p>2.注册事务，在TransactionManager中，通过双向队列（Deque<transaction>）实现事务栈功能，用来处理嵌套事务。通过对Deque<transaction>声明为为ThreadLocal，所以对每个线程而言，事务栈都都是独立的</transaction></transaction></p>
<blockquote>
<p>private static final ThreadLocal<deque<transaction>&gt; CURRENT = new ThreadLocal<deque<transaction>&gt;();</deque<transaction></deque<transaction></p>
</blockquote>
<h5 id="commit"><a href="#commit" class="headerlink" title="commit()"></a>commit()</h5><p>接着看一下commit()方法</p>
<pre><code>[TransactionManager.java]
public void commit(boolean asyncCommit) {

    // 从ThreadLocal中获取当前事务
    final Transaction transaction = getCurrentTransaction();
    // 设置事务状态为CONFIRMING
    transaction.changeStatus(TransactionStatus.CONFIRMING);
    // 更新存储中的事务信息
    transactionRepository.update(transaction);

    // 如果异步commit属性为true
    if (asyncCommit) {
        try {
            Long statTime = System.currentTimeMillis();
            // 通过本地线程池异步进行事务提交
            executorService.submit(new Runnable() {
                @Override
                public void run() {
                    commitTransaction(transaction);
                }
            });
            logger.debug(&quot;async submit cost time:&quot; + (System.currentTimeMillis() - statTime));
        } catch (Throwable commitException) {
            logger.warn(&quot;compensable transaction async submit confirm failed, recovery job will try to confirm later.&quot;, commitException);
            throw new ConfirmingException(commitException);
        }
    } else {
        // 否则同步进行事务提交
        commitTransaction(transaction);
    }
}
</code></pre><p>commit(boolean asyncCommit)方法执行事务的提交过程，具体提交逻辑在commitTransaction(transaction)中完成。</p>
<pre><code>[TransactionManager.java]
private void commitTransaction(Transaction transaction) {
    try {
        // 提交事务
        transaction.commit();
        // 删除本次提交的本地事务记录，如果commit异常，不会把数据库内事务记录删除，
        // 通过job重试进行补偿
        transactionRepository.delete(transaction);
    } catch (Throwable commitException) {
        logger.warn(&quot;compensable transaction confirm failed, recovery job will try to confirm later.&quot;, commitException);
        throw new ConfirmingException(commitException);
    }
}

[Transaction.java]
public void commit() {
    // 对每一个分支执行提交操作
    for (Participant participant : participants) {
        participant.commit();
    }
}
</code></pre><p>可以看到，在事务提交完成之后，对本地持久化的事务记录进行了物理删除，具体删除方式取决于持久化机制。感兴趣的同学可以自行查看 <strong>org.mengyun.tcctransaction.repository</strong> 目录下的实现。</p>
<h5 id="rollback"><a href="#rollback" class="headerlink" title="rollback()"></a>rollback()</h5><p>我们看一下方法rollback()是如何实现事务回滚逻辑的</p>
<pre><code>[TransactionManager.java]
public void rollback(boolean asyncRollback) {

    // 从ThreadLocal中获取当前事务    
    final Transaction transaction = getCurrentTransaction();
    transaction.changeStatus(TransactionStatus.CANCELLING);
    // 更新事务状态为CANCELLING
    transactionRepository.update(transaction);
    // 如果异步rollback属性为true
    if (asyncRollback) {
        try {
            executorService.submit(new Runnable() {
                @Override
                public void run() {
                    // 通过线程池执行回滚逻辑
                    rollbackTransaction(transaction);
                }
            });
        } catch (Throwable rollbackException) {
            logger.warn(&quot;compensable transaction async rollback failed, recovery job will try to rollback later.&quot;, rollbackException);
            throw new CancellingException(rollbackException);
        }
    } else {
        // 异步rollback设置为false，同步执行回滚
        rollbackTransaction(transaction);
    }
}
</code></pre><p>和commit方法类似，在rollback(boolean asyncRollback)执行事务的回滚操作，具体的操作在rollbackTransaction(transaction)中执行：</p>
<pre><code>private void rollbackTransaction(Transaction transaction) {
    try {
        // 事务回滚
        transaction.rollback();
        // 删除本次回滚的本地事务记录，如果rollback异常，不会把数据库内事务记录删除，
        // 通过job重试进行补偿
        transactionRepository.delete(transaction);
    } catch (Throwable rollbackException) {
        logger.warn(&quot;compensable transaction rollback failed, recovery job will try to rollback later.&quot;, rollbackException);
        throw new CancellingException(rollbackException);
    }
}
</code></pre><h5 id="cleanAfterCompletion-transaction"><a href="#cleanAfterCompletion-transaction" class="headerlink" title="cleanAfterCompletion(transaction)"></a>cleanAfterCompletion(transaction)</h5><p>无论是否提交/回滚，最终都会执行cleanAfterCompletion(transaction)方法进行现场清理操作。</p>
<pre><code>public void cleanAfterCompletion(Transaction transaction) {
    if (isTransactionActive() &amp;&amp; transaction != null) {
        // 从ThreadLocal中获取当前事务
        Transaction currentTransaction = getCurrentTransaction();‘
        // 弹出当前事务
        if (currentTransaction == transaction) {
            CURRENT.get().pop();
            if (CURRENT.get().size() == 0) {
                CURRENT.remove();
            }
        } else {
            throw new SystemException(&quot;Illegal transaction when clean after completion&quot;);
        }
    }
}
</code></pre><p>事务执行结束，从栈中弹出当前结束的事务。</p>
<h4 id="providerMethodProceed-compensableMethodContext"><a href="#providerMethodProceed-compensableMethodContext" class="headerlink" title="providerMethodProceed(compensableMethodContext)"></a>providerMethodProceed(compensableMethodContext)</h4><p>看完rootMethodProceed根事务切面逻辑，再来看提供者切面事务逻辑就好理解多了，方法逻辑如下：</p>
<pre><code>private Object providerMethodProceed(CompensableMethodContext compensableMethodContext) throws Throwable {

    // 获取异步回滚、异步提交标识
    Transaction transaction = null;
    boolean asyncConfirm = compensableMethodContext.getAnnotation().asyncConfirm();
    boolean asyncCancel = compensableMethodContext.getAnnotation().asyncCancel();

    try {
        // 判断当前事务状态
        switch (TransactionStatus.valueOf(compensableMethodContext.getTransactionContext().getStatus())) {
            // 如果事务状态为TRYING
            case TRYING:
                //  通过使用transactionContext创建分支事务
                transaction = transactionManager.propagationNewBegin(compensableMethodContext.getTransactionContext());
                // 执行被切方法逻辑
                return compensableMethodContext.proceed();

            // 如果事务状态为CONFIRMING
            case CONFIRMING:
                try {
                    // 对事务状态进行更新
                    transaction = transactionManager.propagationExistBegin(compensableMethodContext.getTransactionContext());
                    // 提交事务，不执行切面方法
                    transactionManager.commit(asyncConfirm);
                } catch (NoExistedTransactionException excepton) {
                    //the transaction has been commit,ignore it.
                }
                break;

            // 如果事务状态为CANCELLING
            case CANCELLING:
                try {
                    // 更新事务状态
                    transaction = transactionManager.propagationExistBegin(compensableMethodContext.getTransactionContext());
                    // 执行事务回滚，不执行切面方法
                    transactionManager.rollback(asyncCancel);
                } catch (NoExistedTransactionException exception) {
                    //the transaction has been rollback,ignore it.
                }
                break;
        }

    } finally {
        // 对现场进行清理
        transactionManager.cleanAfterCompletion(transaction);
    }

    Method method = compensableMethodContext.getMethod();
    // 处理原始类型返回值，返回原始类型的默认值，因为不能返回null
    return ReflectionUtils.getNullValue(method.getReturnType());
}

public Transaction propagationExistBegin(TransactionContext transactionContext) throws NoExistedTransactionException {

    // 根据事务id从事务持久化组件中查询到本事务
    Transaction transaction = transactionRepository.findByXid(transactionContext.getXid());
    // 不为空
    if (transaction != null) {
        // 对事务状态进行更新，根据传参不同，执行TRYING-&gt;CONFIRMING或者TRYING-&gt;CANCELING等操作
        transaction.changeStatus(TransactionStatus.valueOf(transactionContext.getStatus()));
        // 对事务栈进行操作，执行嵌套事务入栈
        registerTransaction(transaction);
        return transaction;
    } else {
        throw new NoExistedTransactionException();
    }
}
</code></pre><blockquote>
<p>这里进行小结，可以看到在provider类型的方法切面，对于远程的Participant，如果transaction的status为trying，则通过transactionManager.propagationNewBegin创建分支事务并执行被切方法逻辑；</p>
<p>如果是status为confirming或canceling，则会调用对应的confirm或cancel配置的方法，跳过被切方法</p>
<p>对于普通类型方法直接调用，normal类型的方法是封装了对远程dubbo接口方法调用逻辑的本地proxy方法，所以直接执行即可</p>
</blockquote>
<h3 id="ResourceCoordinatorAspect"><a href="#ResourceCoordinatorAspect" class="headerlink" title="ResourceCoordinatorAspect"></a>ResourceCoordinatorAspect</h3><blockquote>
<p>ResourceCoordinatorAspect切面主要是为了执行资源协调，它的实现为ConfigurableCoordinatorAspect</p>
</blockquote>
<pre><code>[ResourceCoordinatorAspect.java]
@Aspect
public abstract class ResourceCoordinatorAspect {

    private ResourceCoordinatorInterceptor resourceCoordinatorInterceptor;

    @Pointcut(&quot;@annotation(org.mengyun.tcctransaction.api.Compensable)&quot;)
    public void transactionContextCall() {
    }

    @Around(&quot;transactionContextCall()&quot;)
    public Object interceptTransactionContextMethod(ProceedingJoinPoint pjp) throws Throwable {
        return resourceCoordinatorInterceptor.interceptTransactionContextMethod(pjp);
    }

    public void setResourceCoordinatorInterceptor(ResourceCoordinatorInterceptor resourceCoordinatorInterceptor) {
        this.resourceCoordinatorInterceptor = resourceCoordinatorInterceptor;
    }

    public abstract int getOrder();
}

[ConfigurableCoordinatorAspect.java]
@Aspect
public class ConfigurableCoordinatorAspect extends ResourceCoordinatorAspect implements Ordered {

    private TransactionConfigurator transactionConfigurator;

    public void init() {
        ResourceCoordinatorInterceptor resourceCoordinatorInterceptor = new ResourceCoordinatorInterceptor();
        resourceCoordinatorInterceptor.setTransactionManager(transactionConfigurator.getTransactionManager());
        this.setResourceCoordinatorInterceptor(resourceCoordinatorInterceptor);
    }

    @Override
    public int getOrder() {
        return Ordered.HIGHEST_PRECEDENCE + 1;
    }

    public void setTransactionConfigurator(TransactionConfigurator transactionConfigurator) {
        this.transactionConfigurator = transactionConfigurator;
    }
}
</code></pre><p>ConfigurableCoordinatorAspect的职责为设置事务的参与者；在一个事务内，每个被@Compensable注解的方法都是事务参与者。</p>
<p>可以看到该切面的优先级为 Ordered.HIGHEST_PRECEDENCE + 1，order的数值大于CompensableTransactionAspect。由于 <strong>@Order中的值越小，优先级越高</strong>，因此切面ResourceCoordinatorAspect的优先级小于CompensableTransactionAspect。</p>
<p>从代码可以看出，设置事务参与者逻辑是通过ResourceCoordinatorInterceptor.interceptTransactionContextMethod方法执行的。</p>
<pre><code>[ResourceCoordinatorInterceptor.java]
public Object interceptTransactionContextMethod(ProceedingJoinPoint pjp) throws Throwable {

    // 从当前ThreadLocal中获取事务
    Transaction transaction = transactionManager.getCurrentTransaction();
    if (transaction != null) {
        switch (transaction.getStatus()) {
            case TRYING:
                // 只需要在TRYING阶段将参与者的信息提取出来设置到transaction中
                enlistParticipant(pjp);
                break;
            case CONFIRMING:
                break;
            case CANCELLING:
                break;
        }
    }
    // 执行目标方法
    return pjp.proceed(pjp.getArgs());
}
</code></pre><p>我们可以得知，在trying阶段，框架会把所有事务参与者加入到当前事务中去。</p>
<p>对于Root方法，先创建主事务，事务参与者包括Root方法对应的本地参与者及Normal方法对应的远程参与者；</p>
<p>对于Provider方法，首先通过主事务上下文创建分支事务，事务参与者包括Provider方法对应的本地参与者以及它所包含的Normal方法对应的远程参与者。而远程参与者又可以开启新的分支事务。</p>
<p>我们可以合理的猜想，如果事务嵌套的层级很多，一定会存在性能问题。</p>
<h4 id="enlistParticipant-pjp"><a href="#enlistParticipant-pjp" class="headerlink" title="enlistParticipant(pjp)"></a>enlistParticipant(pjp)</h4><p>我们详细看一下enlistParticipant(pjp)是如何生成的事务参与者对象。</p>
<pre><code>private void enlistParticipant(ProceedingJoinPoint pjp) throws IllegalAccessException, InstantiationException {

    // 首先获取@Compensable信息
    Method method = CompensableMethodUtils.getCompensableMethod(pjp);
    if (method == null) {
        // @Compensable标注的方法为空则抛出异常
        throw new RuntimeException(String.format(&quot;join point not found method, point is : %s&quot;, pjp.getSignature().getName()));
    }
    Compensable compensable = method.getAnnotation(Compensable.class);

    // 回去confirm和cancle方法名
    String confirmMethodName = compensable.confirmMethod();
    String cancelMethodName = compensable.cancelMethod();
    // 获取当前事务以及全局事务id
    Transaction transaction = transactionManager.getCurrentTransaction();
    TransactionXid xid = new TransactionXid(transaction.getXid().getGlobalTransactionId());

    // 设置事务上下文到Editor中
    // Editor用来统一提取事务上下文，如果是dubbo则对应设置dubbo的rpc上下文
    // 此处的上下文设置之后就会调用try逻辑
    if (FactoryBuilder.factoryOf(compensable.transactionContextEditor()).getInstance().get(pjp.getTarget(), method, pjp.getArgs()) == null) {
        FactoryBuilder.factoryOf(compensable.transactionContextEditor()).getInstance().set(new TransactionContext(xid, TransactionStatus.TRYING.getId()), pjp.getTarget(), ((MethodSignature) pjp.getSignature()).getMethod(), pjp.getArgs());
    }

    // 通过目标类名，方法名，参数类型获取目标类
    Class targetClass = ReflectionUtils.getDeclaringType(pjp.getTarget().getClass(), method.getName(), method.getParameterTypes());

    // confirm逻辑调用上下文
    InvocationContext confirmInvocation = new InvocationContext(targetClass,
            confirmMethodName,
            method.getParameterTypes(), pjp.getArgs());

    //cancel逻辑调用上下文
    InvocationContext cancelInvocation = new InvocationContext(targetClass,
            cancelMethodName,
            method.getParameterTypes(), pjp.getArgs());

    // 此处较为关键，confirm和cancle具有相同地位，都被抽象成InvocationContext
    Participant participant =
            new Participant(
                    xid,
                    confirmInvocation,
                    cancelInvocation,
                    compensable.transactionContextEditor());
    // 将participant设置到transaction中，并同步到持久化存储中
    transactionManager.enlistParticipant(participant);
}

[TransactionManager.java]
public void enlistParticipant(Participant participant) {
    Transaction transaction = this.getCurrentTransaction();
    transaction.enlistParticipant(participant);
    transactionRepository.update(transaction);
}

[Transaction.java]
public void enlistParticipant(Participant participant) {
    participants.add(participant);
}
</code></pre><blockquote>
<p>从上述的代码逻辑中，我们可以得到结论，CompensableTransactionAspect开启事务，ResourceCoordinatorAspect对注解@Compensable进行解析，将confirm与cancel的具体逻辑设置到事务管理器中。</p>
<p>当上述两个切面都执行完成之后，开始执行try中的方法。如果try成功则执行commit否则执行rollback。</p>
<p>每个分支事务最终被封装到Transaction的participants中，每个分布式事务都有一个自己的 <strong>ThreadLocal<deque＜transaction＞></deque＜transaction＞></strong></p>
</blockquote>
<p>我们再次回顾commit的逻辑，查看Transaction.commit()方法</p>
<pre><code>[Transaction.java]
public void commit() {
    // 对每一个分支执行提交操作
    for (Participant participant : participants) {
        participant.commit();
    }
}
</code></pre><p>participant就是切面ResourceCoordinatorAspect 添加的。我们再看一下participant.commit()的逻辑：</p>
<pre><code>[Transaction.java]
public void commit() {
    terminator.invoke(new TransactionContext(xid, TransactionStatus.CONFIRMING.getId()), confirmInvocationContext, transactionContextEditorClass);
}
</code></pre><p>可以看到最终事务提交是通过invoke反射实现的，我们进入invoke逻辑</p>
<pre><code>public Object invoke(TransactionContext transactionContext, 
                    InvocationContext invocationContext, 
                    Class&lt;? extends TransactionContextEditor&gt; transactionContextEditorClass) {
    // 如果事务执行上下文方法名不为空
    if (StringUtils.isNotEmpty(invocationContext.getMethodName())) {
        try {
            Object target = FactoryBuilder.factoryOf(invocationContext.getTargetClass()).getInstance();

            Method method = null;

            method = target.getClass().getMethod(invocationContext.getMethodName(), invocationContext.getParameterTypes());
            // 实例化原事务执行者的代理对象
            FactoryBuilder.factoryOf(transactionContextEditorClass).getInstance().set(transactionContext, target, method, invocationContext.getArgs());
            // 反射执行
            return method.invoke(target, invocationContext.getArgs());

        } catch (Exception e) {
            throw new SystemException(e);
        }
    }
    return null;
}
</code></pre><p>最终通过method.invoke(target, invocationContext.getArgs())方法完成了真实的事务提交操作。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>到此我们对TCC-TRANSACTION的事务提交主流程进行了完整的分析。</p>
<p>通过分析我们可以知道TCC-TRANSACTION的核心逻辑是通过两个切面CompensableTransactionAspect、ResourceCoordinatorAspect 实现的。通过对事务进行包装与代理，实现了类二阶段的分布式事务解决方案。</p>
<p>实际上，TCC-TRANSACTION还有一个重要的补偿逻辑我们还没有分析，它是基于定时调度实现的。</p>
<p>限于本文的篇幅，就不再继续展开。我将单独用一篇文章来对TCC-TRANSACTION的补偿过程进行分析，我们下文再会。</p>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TCC分布式事务解决方案在开源界的主要实现为Byte-TCC、TCC-Transaction等。其中笔者了解较多并且业界使用率较高的为TCC-Transaction这一实现。&lt;/p&gt;
&lt;p&gt;本文，我将带领读者对TCC-Transaction这一分布式事务框架进行一次源码解析，提高自己的阅读源码的能力，也希望能够对读者深入了解TCC-Transaction有所帮助。&lt;/p&gt;
&lt;h2 id=&quot;源码下载&quot;&gt;&lt;a href=&quot;#源码下载&quot; class=&quot;headerlink&quot; title=&quot;源码下载&quot;&gt;&lt;/a&gt;源码下载&lt;/h2&gt;&lt;p&gt;源码地址为 &lt;a href=&quot;https://github.com/changmingxie/tcc-transaction&quot;&gt;https://github.com/changmingxie/tcc-transaction&lt;/a&gt;，我们关注最新版本1.2.x。&lt;/p&gt;
&lt;p&gt;源码下载后导入IDEA中，项目目录结构如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2019/09/09/TCC-Transaction源码解析之事务执行/tcc0.png&quot; alt=&quot;代码结构&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="TCC-Transaction源码解析" scheme="http://wuwenliang.net/categories/TCC-Transaction%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
    
      <category term="TCC-Transaction源码解析" scheme="http://wuwenliang.net/tags/TCC-Transaction%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>研磨Spring事件机制</title>
    <link href="http://wuwenliang.net/2019/09/08/%E7%A0%94%E7%A3%A8Spring%E4%BA%8B%E4%BB%B6%E6%9C%BA%E5%88%B6/"/>
    <id>http://wuwenliang.net/2019/09/08/研磨Spring事件机制/</id>
    <published>2019-09-08T08:41:09.000Z</published>
    <updated>2019-09-08T09:29:05.227Z</updated>
    
    <content type="html"><![CDATA[<p>Spring提供了应用程序事件特性为开发者提供了事件发布和接收事件的能力，它基于观察者模式实现，对于提升应用逻辑的松耦合很有意义。</p>
<p>本文就如何使用Spring事件机制进行较为详细的总结。</p>
<h2 id="Spring事件概述"><a href="#Spring事件概述" class="headerlink" title="Spring事件概述"></a>Spring事件概述</h2><ul>
<li>每一个Spring事件都需要继承ApplicationEvent的类，ApplicationEvent又继承自java.util.EventObject</li>
<li>Spring中的任何bean都能够通过实现ApplicationListener<t>接口来监听事件；这里的泛型T就是上面提到的继承了ApplicationEvent的类</t></li>
<li>SpringContext会对实现ApplicationListener接口的任意bean进行自动注册</li>
<li>发布事件是通过ApplicationEventPublisher.publishEvent方法实现的。应用中一般通过ApplicationContext进行事件发布，ApplicationContext实现了ApplicationEventPublisher接口</li>
<li>另一种发布事件的方式是bean实现ApplicationContextAware接口，拿到ApplicationContext实例从而实现事件发布</li>
</ul>
<a id="more"></a>
<h2 id="Spring事件机制实战"><a href="#Spring事件机制实战" class="headerlink" title="Spring事件机制实战"></a>Spring事件机制实战</h2><p>我们通过一个简单的案例对Spring事件机制使用进行总结。</p>
<h3 id="定义事件"><a href="#定义事件" class="headerlink" title="定义事件"></a>定义事件</h3><blockquote>
<p>首先需要对事件进行定义，事件可以理解为消息队列中的消息，即：当满足某个条件时候需要发布的一个实体，该实体中包含了我们需要事件监听器处理的内容。<br>比如：当数据库中的内容发生更新，我们需要同步对缓存中的数据进行更新，那么我们就可以在数据库更新成功之后发布一个缓存更新事件，将更新后的数据实体通过publishEvent发布出去，<br>在另一个监听器类中获取到该缓存更新事件对缓存进行更新即可。这是一个事件机制的典型应用。</p>
</blockquote>
<p>一个事件是一个javabean，它需要继承ApplicationEvent。</p>
<pre><code>public class DemoEvent extends ApplicationEvent {

    private String key;
    private String value;

    public DemoEvent(Object source) {
        super(source);
    }

    public String getKey() {
        return key;
    }

    public DemoEvent setKey(String key) {
        this.key = key;
        return this;
    }

    public String getValue() {
        return value;
    }

    public DemoEvent setValue(String value) {
        this.value = value;
        return this;
    }

    @Override
    public String toString() {
        return &quot;DemoEvent{&quot; +
                &quot;key=&apos;&quot; + key + &apos;\&apos;&apos; +
                &quot;, value=&apos;&quot; + value + &apos;\&apos;&apos; +
                &apos;}&apos;;
    }
}
</code></pre><p>上述这个类继承了ApplicationEvent，key、value是它的属性，在实际应用中，属性可以使任意的应用数据。</p>
<blockquote>
<p>需要注意的是，ApplicationEvent具有一个接受指向事件源的引用的构造函数，在DemoEvent的构造函数中，需要通过super(source);传入事件源的引用。</p>
</blockquote>
<h3 id="定义事件监听器"><a href="#定义事件监听器" class="headerlink" title="定义事件监听器"></a>定义事件监听器</h3><p>ApplicationListener接口定义了方法onApplicationEvent，当触发了一个事件的时候，Spring框架会回调onApplicationEvent方法。</p>
<p>通过实现ApplicationListener接口，应用事件监听器需要对接收到的事件类进行处理。</p>
<pre><code>@Component
public class DemoEventListener implements ApplicationListener&lt;DemoEvent&gt; {

    private static final Map&lt;String, String&gt; CONTAINER = new ConcurrentHashMap&lt;&gt;();

    @Override
    public void onApplicationEvent(DemoEvent demoEvent) {
        CONTAINER.put(demoEvent.getKey(), demoEvent.getValue());
        System.out.println(&quot;[DemoEventListener]接受事件--&quot; + demoEvent.toString());
        System.out.println(CONTAINER.toString());
    }
}
</code></pre><p>这里的逻辑是DemoEventListener实现了强类型的ApplicationListener接口，对我们发布的DemoEvent事件进行处理。</p>
<p>具体的处理逻辑为将DemoEvent的key、value属性添加到CONTAINER这个Map中，模拟缓存更新。</p>
<blockquote>
<p>另外一种创建事件监听器的方式为可以使用注解<strong>@EventListener</strong>：原理就是通过扫描这个注解，创建监听器并添加到ApplicationContext</p>
</blockquote>
<h3 id="发布事件"><a href="#发布事件" class="headerlink" title="发布事件"></a>发布事件</h3><p>通过接口ApplicationEventPublisher的publishEvent方法我们能够完成发布事件的目的，在Spring框架中AbstractApplicationContext实现了ApplicationEventPublisher接口。</p>
<p>因此我们能够在应用中通过获取ApplicationContext实例使用事件发布能力。</p>
<blockquote>
<p>也可以通过实现ApplicationEventPublisherAware接口来发布事件</p>
</blockquote>
<pre><code>@Component
public class DemoEventPublisher implements CommandLineRunner {

    ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1);

    private static final AtomicInteger ATOMIC_INTEGER = new AtomicInteger(0);

    @Autowired
    ApplicationContext context;

    @Override
    public void run(String... args) throws Exception {
        executorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                // 设置事件源
                DemoEvent demoEvent = new DemoEvent(this);
                demoEvent.setKey(String.valueOf(ATOMIC_INTEGER.getAndAdd(1))).setValue(&quot;snowalker&quot;);
                context.publishEvent(demoEvent);
                System.out.println(&quot;[DemoEventPublisher]发布事件：&quot; + demoEvent.toString());
            }
        }, 0, 3000, TimeUnit.MILLISECONDS);
    }
}
</code></pre><p>我们通过context.publishEvent(demoEvent);发布了缓存更新事件，每3秒发布一次。另一种写法为</p>
<pre><code>@Component
public class DemoEventPublisher2 implements CommandLineRunner, ApplicationEventPublisherAware {

    ScheduledExecutorService executorService = Executors.newScheduledThreadPool(1);

    private static final AtomicInteger ATOMIC_INTEGER = new AtomicInteger(0);

    ApplicationEventPublisher applicationEventPublisher;

    @Override
    public void run(String... args) throws Exception {
        executorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                // 设置事件源
                DemoEvent demoEvent = new DemoEvent(this);
                demoEvent.setKey(String.valueOf(ATOMIC_INTEGER.getAndAdd(1))).setValue(&quot;snowalker&quot;);
                System.out.println(&quot;[DemoEventPublisher]发布事件：&quot; + demoEvent.toString());
                applicationEventPublisher.publishEvent(demoEvent);
            }
        }, 0, 3000, TimeUnit.MILLISECONDS);
    }

    @Override
    public void setApplicationEventPublisher(ApplicationEventPublisher applicationEventPublisher) {
        this.applicationEventPublisher = applicationEventPublisher;
    }
}
</code></pre><p>这两种写法效果一致，最终都是通过applicationEventPublisher进行了事件的发布。</p>
<h3 id="案例运行"><a href="#案例运行" class="headerlink" title="案例运行"></a>案例运行</h3><pre><code>[DemoEventPublisher]发布事件：DemoEvent{key=&apos;0&apos;, value=&apos;snowalker&apos;}
[DemoEventListener]接受事件--DemoEvent{key=&apos;0&apos;, value=&apos;snowalker&apos;}
{0=snowalker}
[DemoEventPublisher]发布事件：DemoEvent{key=&apos;1&apos;, value=&apos;snowalker&apos;}
[DemoEventListener]接受事件--DemoEvent{key=&apos;1&apos;, value=&apos;snowalker&apos;}
{0=snowalker, 1=snowalker}
[DemoEventPublisher]发布事件：DemoEvent{key=&apos;2&apos;, value=&apos;snowalker&apos;}
[DemoEventListener]接受事件--DemoEvent{key=&apos;2&apos;, value=&apos;snowalker&apos;}
{0=snowalker, 1=snowalker, 2=snowalker}
......
</code></pre><p>可以看到，通过事件机制，我们实现了进程内的事件通信，这种方式能够很好的对业务进行解耦合，更加灵活的进行业务处理。</p>
<h4 id="事务绑定事件"><a href="#事务绑定事件" class="headerlink" title="事务绑定事件"></a>事务绑定事件</h4><p>另外需要注意的是，某些场景下，我们需要在事务提交之后再发布事件，这里就涉及到了事务绑定事件能力。</p>
<p>具体的方式为使用 <strong>@TransactionalEventListener注解</strong> 或者 <strong>TransactionSynchronizationManager</strong> 类来解决此类问题，也就是：事务成功提交之后，再发布事件。</p>
<p>当然我们也可以在事件提交之后将结果返回给调用方然后发布事件，但是这种方式不够优雅，因此还是建议使用事务绑定事件的方式进行基于事务的事件发布。</p>
<h3 id="异步事件支持"><a href="#异步事件支持" class="headerlink" title="异步事件支持"></a>异步事件支持</h3><p>上述的方式为同步事件，我们可以通过配置开启异步事件支持。</p>
<p>通过使用@Async注解事件监听器开启异步支持，需要要开启对异步注解的支持.</p>
<ul>
<li>java配置通过@EnableAsync开启.</li>
<li>如果是xml配置文件则需要配置: <task:annotation-driven></task:annotation-driven></li>
</ul>
<p><hr><br>版权声明：<br><br>原创不易，洗文可耻。除非注明，本博文章均为原创，转载请以链接形式标明本文地址。<br></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spring提供了应用程序事件特性为开发者提供了事件发布和接收事件的能力，它基于观察者模式实现，对于提升应用逻辑的松耦合很有意义。&lt;/p&gt;
&lt;p&gt;本文就如何使用Spring事件机制进行较为详细的总结。&lt;/p&gt;
&lt;h2 id=&quot;Spring事件概述&quot;&gt;&lt;a href=&quot;#Spring事件概述&quot; class=&quot;headerlink&quot; title=&quot;Spring事件概述&quot;&gt;&lt;/a&gt;Spring事件概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;每一个Spring事件都需要继承ApplicationEvent的类，ApplicationEvent又继承自java.util.EventObject&lt;/li&gt;
&lt;li&gt;Spring中的任何bean都能够通过实现ApplicationListener&lt;T&gt;接口来监听事件；这里的泛型T就是上面提到的继承了ApplicationEvent的类&lt;/T&gt;&lt;/li&gt;
&lt;li&gt;SpringContext会对实现ApplicationListener接口的任意bean进行自动注册&lt;/li&gt;
&lt;li&gt;发布事件是通过ApplicationEventPublisher.publishEvent方法实现的。应用中一般通过ApplicationContext进行事件发布，ApplicationContext实现了ApplicationEventPublisher接口&lt;/li&gt;
&lt;li&gt;另一种发布事件的方式是bean实现ApplicationContextAware接口，拿到ApplicationContext实例从而实现事件发布&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="spring-boot" scheme="http://wuwenliang.net/categories/spring-boot/"/>
    
    
      <category term="spring-boot" scheme="http://wuwenliang.net/tags/spring-boot/"/>
    
  </entry>
  
</feed>
